{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "df = pd.read_csv('flair-vgg16-data.csv', names=['_id', 'message', 'image_concept', 'published', 'disabled'])\n",
    "df['available'] = 0\n",
    "\n",
    "all_images_path = 'data/all_images'\n",
    "for i, row in df.iterrows():\n",
    "    if os.path.isfile(os.path.join(all_images_path, row['_id'] + '.jpg')):\n",
    "        df.at[i, 'available']= 1    \n",
    "        \n",
    "df_published = df.loc[df.query('available == 1 and published == 1').index]\n",
    "df_published['label'] = '__label__published'\n",
    "df_published['text'] = df_published['image_concept'] + ' ' + df_published['message']\n",
    "df_published = df_published.loc[df_published['text'].notnull()]\n",
    "published_count = len(df_published)\n",
    "\n",
    "\n",
    "df_disabled = df.loc[df.query('available == 1 and disabled == 1').index]\n",
    "df_disabled['label'] = '__label__disabled'\n",
    "df_disabled['text'] = df_disabled['image_concept'] + ' ' + df_disabled['message']\n",
    "df_disabled = df_disabled.loc[df_disabled['text'].notnull()]\n",
    "df_disabled = df_disabled[:published_count]\n",
    "\n",
    "\n",
    "\n",
    "df_all = pd.concat([df_published, df_disabled], ignore_index=True)\n",
    "\n",
    "\n",
    "df_all = df_all.reset_index(drop=True)\n",
    "\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[['label', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all.to_csv('docker/local_test/data.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_docker = pd.read_csv('docker/local_test/data.csv', names=['_id', 'message', 'image_concept', 'published', 'disabled', 'available', 'label', 'text'])\n",
    "# df_docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, validation_df = train_test_split(df_all, test_size=0.4, random_state=42)\n",
    "validation_df, test_df = train_test_split(validation_df, test_size=0.4, random_state=42)\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "validation_df = validation_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "train_csv = 'flair_classification_data/train.csv'\n",
    "dev_csv = 'flair_classification_data/dev.csv'\n",
    "test_csv = 'flair_classification_data/test.csv'\n",
    "\n",
    "train_df[['label', 'text']].to_csv(train_csv, sep='\\t', index=False, header=False)\n",
    "validation_df[['label', 'text']].to_csv(dev_csv, sep='\\t', index=False, header=False)\n",
    "test_df[['label', 'text']].to_csv(test_csv, sep='\\t', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df2 = pd.read_csv('flair_classification_data/train.csv', sep='\\t', names=['label', 'text'])\n",
    "train_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from flair.data_fetcher import NLPTaskDataFetcher\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentRNNEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus = NLPTaskDataFetcher.load_classification_corpus(\n",
    "    Path('flair_classification_data'),\n",
    "    test_file='test.csv',\n",
    "    dev_file='dev.csv',\n",
    "    train_file='train.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "label_dict = corpus.make_label_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus.make_vocab_dictionary().get_items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_embeddings = DocumentRNNEmbeddings([\n",
    "        WordEmbeddings('twitter'),\n",
    "    #     FlairEmbeddings('news-forward'),\n",
    "    #     FlairEmbeddings('news-backward')\n",
    "    ], \n",
    "    hidden_size=128,\n",
    "    reproject_words=True,\n",
    "    reproject_words_dimension=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import Sentence\n",
    "\n",
    "sentence1 = Sentence('The grass is green . And the sky is blue .')\n",
    "\n",
    "# embed the sentence with our document embedding\n",
    "document_embeddings.embed(sentence1)\n",
    "\n",
    "# now check out the embedded sentence.\n",
    "print(sentence1.get_embedding().shape)\n",
    "\n",
    "sentence2 = Sentence(\"\"\"It accounts for virtually all discussion in the media, enjoying priority over such topics as the 2020 US presidential election or the UK finally leaving the EU for good in less than 9 months. People are flooding social media with COVID information, which can only mean one thing: data. Fresh data waiting to be analysed. And analyse it we will.\"\"\")\n",
    "\n",
    "# embed the sentence with our document embedding\n",
    "document_embeddings.embed(sentence2)\n",
    "\n",
    "# now check out the embedded sentence.\n",
    "print(sentence2.get_embedding().shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classifier = TextClassifier(\n",
    "    document_embeddings, \n",
    "    label_dictionary=['published', 'disabled'],\n",
    "    multi_label=True\n",
    ")\n",
    "\n",
    "print(classifier)\n",
    "\n",
    "trainer = ModelTrainer(classifier, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.train('./', max_epochs=20, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier([Sentence(\"this is a sentence\")])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "from flair.training_utils import store_embeddings\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "torch.from_numpy(train_df[:10][['published', 'disabled']].values).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_batches(df, target_names, batch_size=16):\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        x = []\n",
    "        y = []\n",
    "        for row in df[i:i+batch_size].iterrow():\n",
    "            image_concept = '' if pd.isna(row['image_concept']) else row['image_concept']\n",
    "            message = '' if pd.isna(row['message']) else row['message']                        \n",
    "            \n",
    "            # shuffle image concepts\n",
    "            words = image_concept.split()\n",
    "            random.shuffle(words)\n",
    "            image_concept = ' '.join(words)\n",
    "            \n",
    "            # join message and image_concept together\n",
    "            txt = ' '.join([message, image_concept])                    \n",
    "            x.append(Sentence(txt))                        \n",
    "            y.append([row[t] for t in target_names])\n",
    "        \n",
    "        yield x, torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "\n",
    "def train_model(model, epochs, lr, train_df, val_df, target_names, checkpoint_file, early_stopping=5):        \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    best_loss = np.inf\n",
    "    no_improvement = 0\n",
    "\n",
    "    if train_on_gpu:\n",
    "        model = model.cuda()\n",
    "        \n",
    "    for epoch in range(epochs):        \n",
    "        total_train_loss = 0\n",
    "        total_val_loss = 0\n",
    "        train_loss = 0\n",
    "        val_loss = 0\n",
    "        \n",
    "        # Train\n",
    "        model.train()        \n",
    "        for i, (sentences, labels) in enumerate(get_batches(train_df, target_names)):         \n",
    "            if train_on_gpu:\n",
    "                labels = labels.cuda()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            out = model(sentences)            \n",
    "            \n",
    "            loss = criterion(out, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            store_embeddings(sentences, 'cpu')\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Batch {i}, train loss {loss.item()/labels.size(0)}\")\n",
    "            \n",
    "            \n",
    "        train_loss = total_train_loss/len(train_df)\n",
    "        print(f\"> Epoch {epoch}, train loss {train_loss}\")\n",
    "        \n",
    "        # Eval\n",
    "        model.eval()\n",
    "        for sentences, labels in get_batches(val_df):\n",
    "            if train_on_gpu:\n",
    "                labels = labels.cuda()\n",
    "            \n",
    "            out = model(sentences)\n",
    "            loss = criterion(out, labels)\n",
    "            total_val_loss += loss.item()\n",
    "            \n",
    "            store_embeddings(sentences, 'cpu')\n",
    "            \n",
    "            \n",
    "        val_loss = total_val_loss / len(val_df)\n",
    "        \n",
    "        print(f\"> Epoch {epoch}, val loss {val_loss}\")\n",
    "        \n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            no_improvement = 0\n",
    "            torch.save(model.state_dict(), checkpoint_file)\n",
    "            print(\"Saved model.\")\n",
    "        else:\n",
    "            no_improvement += 1\n",
    "            print(\"No improvement.\")\n",
    "            if no_improvement >= early_stopping:\n",
    "                print(f\"Early Stopping\")\n",
    "                break\n",
    "            \n",
    "                                              \n",
    "checkpoint_file = 'flair_text_model_2.pt'      \n",
    "lr = 0.005\n",
    "epochs = 5            \n",
    "\n",
    "target_names = ['published', 'disabled']\n",
    "train_model(classifier, epochs, lr, train_df, validation_df, target_names, checkpoint_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, test_df):\n",
    "    if train_on_gpu:\n",
    "        model = model.cuda()\n",
    "        \n",
    "    model.eval()\n",
    "    num_correct = 0\n",
    "    total = 0\n",
    "    for i, (sentences, labels) in enumerate(get_batches(test_df)):\n",
    "        if train_on_gpu:\n",
    "            labels = labels.cuda()\n",
    "            \n",
    "        out = model(sentences)\n",
    "        _, pred = torch.max(out, 1)\n",
    "                \n",
    "        correct = (labels == pred)\n",
    "        correct = correct.cpu().numpy() if train_on_gpu else correct.numpy()\n",
    "        \n",
    "        num_correct += np.sum(correct)\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        store_embeddings(sentences, 'cpu')\n",
    "        \n",
    "\n",
    "    print(f\"{num_correct}/{total} correct. Accuracy: {num_correct*100/total} %\")\n",
    "    \n",
    "    \n",
    "eval_model(classifier, test_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:efficientnet]",
   "language": "python",
   "name": "conda-env-efficientnet-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
