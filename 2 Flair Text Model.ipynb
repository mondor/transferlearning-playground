{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>message</th>\n",
       "      <th>image_concept</th>\n",
       "      <th>published</th>\n",
       "      <th>disabled</th>\n",
       "      <th>available</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5e5836fee917e8d9a8a7b277</td>\n",
       "      <td>endless blues greatbarrierreef australia whits...</td>\n",
       "      <td>seascape water shoal sea turquoise sun tropica...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>__label__published</td>\n",
       "      <td>seascape water shoal sea turquoise sun tropica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5e58343ded065ad79e312f3d</td>\n",
       "      <td>hamiltonisland</td>\n",
       "      <td>tree travel vacation seashore water hotel isla...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>__label__published</td>\n",
       "      <td>tree travel vacation seashore water hotel isla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5e57dc939e88b6be2ac42800</td>\n",
       "      <td>we are going coconuts for hamiltonisland here ...</td>\n",
       "      <td>relaxation beach sea vacation sand recreation ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>__label__published</td>\n",
       "      <td>relaxation beach sea vacation sand recreation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5e55dca437fa5927dcdf02f3</td>\n",
       "      <td>en route to gbr embrace the elevation in luxur...</td>\n",
       "      <td>nature travel diving water sea underwater ocea...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>__label__published</td>\n",
       "      <td>nature travel diving water sea underwater ocea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5e55d69eb9e5b725cd7ba02f</td>\n",
       "      <td>golf course views hamiltonislandgolfcourse whi...</td>\n",
       "      <td>outdoors landscape beach sky nature rural nope...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>__label__published</td>\n",
       "      <td>outdoors landscape beach sky nature rural nope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>5e4e3124497f22be9069f067</td>\n",
       "      <td>golf trips with the boys are always wicked and...</td>\n",
       "      <td>sky water seashore sea travel winter ship land...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>__label__disabled</td>\n",
       "      <td>sky water seashore sea travel winter ship land...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>5e4e3124ffb21abead202386</td>\n",
       "      <td>golf trips with the boys are always wicked and...</td>\n",
       "      <td>travel golf ocean grass water sand nature sea ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>__label__disabled</td>\n",
       "      <td>travel golf ocean grass water sand nature sea ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>5e4e3124ffb21abead202385</td>\n",
       "      <td>throwback to that time i was warm and tanned q...</td>\n",
       "      <td>watercraft water people noperson recreation se...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>__label__disabled</td>\n",
       "      <td>watercraft water people noperson recreation se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>5e4e3123164d73be9b8cd43e</td>\n",
       "      <td>golf trips with the boys are always wicked and...</td>\n",
       "      <td>adult people class girl grouptogether portrait...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>__label__disabled</td>\n",
       "      <td>adult people class girl grouptogether portrait...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>5e4e3123164d73be9b8cd43f</td>\n",
       "      <td>golf trips with the boys are always wicked and...</td>\n",
       "      <td>travel golf sea nature landscape rural noperso...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>__label__disabled</td>\n",
       "      <td>travel golf sea nature landscape rural noperso...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1440 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           _id  \\\n",
       "0     5e5836fee917e8d9a8a7b277   \n",
       "1     5e58343ded065ad79e312f3d   \n",
       "2     5e57dc939e88b6be2ac42800   \n",
       "3     5e55dca437fa5927dcdf02f3   \n",
       "4     5e55d69eb9e5b725cd7ba02f   \n",
       "...                        ...   \n",
       "1435  5e4e3124497f22be9069f067   \n",
       "1436  5e4e3124ffb21abead202386   \n",
       "1437  5e4e3124ffb21abead202385   \n",
       "1438  5e4e3123164d73be9b8cd43e   \n",
       "1439  5e4e3123164d73be9b8cd43f   \n",
       "\n",
       "                                                message  \\\n",
       "0     endless blues greatbarrierreef australia whits...   \n",
       "1                                        hamiltonisland   \n",
       "2     we are going coconuts for hamiltonisland here ...   \n",
       "3     en route to gbr embrace the elevation in luxur...   \n",
       "4     golf course views hamiltonislandgolfcourse whi...   \n",
       "...                                                 ...   \n",
       "1435  golf trips with the boys are always wicked and...   \n",
       "1436  golf trips with the boys are always wicked and...   \n",
       "1437  throwback to that time i was warm and tanned q...   \n",
       "1438  golf trips with the boys are always wicked and...   \n",
       "1439  golf trips with the boys are always wicked and...   \n",
       "\n",
       "                                          image_concept  published  disabled  \\\n",
       "0     seascape water shoal sea turquoise sun tropica...          1         0   \n",
       "1     tree travel vacation seashore water hotel isla...          1         0   \n",
       "2     relaxation beach sea vacation sand recreation ...          1         0   \n",
       "3     nature travel diving water sea underwater ocea...          1         0   \n",
       "4     outdoors landscape beach sky nature rural nope...          1         0   \n",
       "...                                                 ...        ...       ...   \n",
       "1435  sky water seashore sea travel winter ship land...          0         1   \n",
       "1436  travel golf ocean grass water sand nature sea ...          0         1   \n",
       "1437  watercraft water people noperson recreation se...          0         1   \n",
       "1438  adult people class girl grouptogether portrait...          0         1   \n",
       "1439  travel golf sea nature landscape rural noperso...          0         1   \n",
       "\n",
       "      available               label  \\\n",
       "0             1  __label__published   \n",
       "1             1  __label__published   \n",
       "2             1  __label__published   \n",
       "3             1  __label__published   \n",
       "4             1  __label__published   \n",
       "...         ...                 ...   \n",
       "1435          1   __label__disabled   \n",
       "1436          1   __label__disabled   \n",
       "1437          1   __label__disabled   \n",
       "1438          1   __label__disabled   \n",
       "1439          1   __label__disabled   \n",
       "\n",
       "                                                   text  \n",
       "0     seascape water shoal sea turquoise sun tropica...  \n",
       "1     tree travel vacation seashore water hotel isla...  \n",
       "2     relaxation beach sea vacation sand recreation ...  \n",
       "3     nature travel diving water sea underwater ocea...  \n",
       "4     outdoors landscape beach sky nature rural nope...  \n",
       "...                                                 ...  \n",
       "1435  sky water seashore sea travel winter ship land...  \n",
       "1436  travel golf ocean grass water sand nature sea ...  \n",
       "1437  watercraft water people noperson recreation se...  \n",
       "1438  adult people class girl grouptogether portrait...  \n",
       "1439  travel golf sea nature landscape rural noperso...  \n",
       "\n",
       "[1440 rows x 8 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "df = pd.read_csv('flair-vgg16-data.csv', names=['_id', 'message', 'image_concept', 'published', 'disabled'])\n",
    "df['available'] = 0\n",
    "\n",
    "all_images_path = 'data/all_images'\n",
    "for i, row in df.iterrows():\n",
    "    if os.path.isfile(os.path.join(all_images_path, row['_id'] + '.jpg')):\n",
    "        df.at[i, 'available']= 1    \n",
    "        \n",
    "df_published = df.loc[df.query('available == 1 and published == 1').index]\n",
    "df_published['label'] = '__label__published'\n",
    "df_published['text'] = df_published['image_concept'] + ' ' + df_published['message']\n",
    "df_published = df_published.loc[df_published['text'].notnull()]\n",
    "published_count = len(df_published)\n",
    "\n",
    "\n",
    "df_disabled = df.loc[df.query('available == 1 and disabled == 1').index]\n",
    "df_disabled['label'] = '__label__disabled'\n",
    "df_disabled['text'] = df_disabled['image_concept'] + ' ' + df_disabled['message']\n",
    "df_disabled = df_disabled.loc[df_disabled['text'].notnull()]\n",
    "df_disabled = df_disabled[:published_count]\n",
    "\n",
    "\n",
    "\n",
    "df_all = pd.concat([df_published, df_disabled], ignore_index=True)\n",
    "\n",
    "\n",
    "df_all = df_all.reset_index(drop=True)\n",
    "\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>__label__published</td>\n",
       "      <td>seascape water shoal sea turquoise sun tropica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__label__published</td>\n",
       "      <td>tree travel vacation seashore water hotel isla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>__label__published</td>\n",
       "      <td>relaxation beach sea vacation sand recreation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>__label__published</td>\n",
       "      <td>nature travel diving water sea underwater ocea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>__label__published</td>\n",
       "      <td>outdoors landscape beach sky nature rural nope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>__label__disabled</td>\n",
       "      <td>sky water seashore sea travel winter ship land...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>__label__disabled</td>\n",
       "      <td>travel golf ocean grass water sand nature sea ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>__label__disabled</td>\n",
       "      <td>watercraft water people noperson recreation se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>__label__disabled</td>\n",
       "      <td>adult people class girl grouptogether portrait...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>__label__disabled</td>\n",
       "      <td>travel golf sea nature landscape rural noperso...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1440 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   label                                               text\n",
       "0     __label__published  seascape water shoal sea turquoise sun tropica...\n",
       "1     __label__published  tree travel vacation seashore water hotel isla...\n",
       "2     __label__published  relaxation beach sea vacation sand recreation ...\n",
       "3     __label__published  nature travel diving water sea underwater ocea...\n",
       "4     __label__published  outdoors landscape beach sky nature rural nope...\n",
       "...                  ...                                                ...\n",
       "1435   __label__disabled  sky water seashore sea travel winter ship land...\n",
       "1436   __label__disabled  travel golf ocean grass water sand nature sea ...\n",
       "1437   __label__disabled  watercraft water people noperson recreation se...\n",
       "1438   __label__disabled  adult people class girl grouptogether portrait...\n",
       "1439   __label__disabled  travel golf sea nature landscape rural noperso...\n",
       "\n",
       "[1440 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all[['label', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, validation_df = train_test_split(df_all, test_size=0.4, random_state=42)\n",
    "validation_df, test_df = train_test_split(validation_df, test_size=0.4, random_state=42)\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "validation_df = validation_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "train_csv = 'flair_classification_data/train.csv'\n",
    "dev_csv = 'flair_classification_data/dev.csv'\n",
    "test_csv = 'flair_classification_data/test.csv'\n",
    "\n",
    "train_df[['label', 'text']].to_csv(train_csv, sep='\\t', index=False, header=False)\n",
    "validation_df[['label', 'text']].to_csv(dev_csv, sep='\\t', index=False, header=False)\n",
    "test_df[['label', 'text']].to_csv(test_csv, sep='\\t', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>__label__published</td>\n",
       "      <td>outdoors nature land water ocean sea shoreline...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__label__disabled</td>\n",
       "      <td>exotic hotel noperson beach palm swimming vaca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>__label__disabled</td>\n",
       "      <td>zipup noperson science conceptual fashion insu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>__label__disabled</td>\n",
       "      <td>sky water vacation sea desktop pattern nature ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>__label__disabled</td>\n",
       "      <td>portrait people beautiful sunglasses woman man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>__label__disabled</td>\n",
       "      <td>couple woman boat people water sea luxury summ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>__label__disabled</td>\n",
       "      <td>sunglasses watersports man fun paddle oar wate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>__label__disabled</td>\n",
       "      <td>sand water leisure beach sea tropical vacation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>__label__disabled</td>\n",
       "      <td>vacation beach resort sun palm swimmingpool wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>__label__disabled</td>\n",
       "      <td>people bird pet parrot love cute family nature...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>864 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  label                                               text\n",
       "0    __label__published  outdoors nature land water ocean sea shoreline...\n",
       "1     __label__disabled  exotic hotel noperson beach palm swimming vaca...\n",
       "2     __label__disabled  zipup noperson science conceptual fashion insu...\n",
       "3     __label__disabled  sky water vacation sea desktop pattern nature ...\n",
       "4     __label__disabled  portrait people beautiful sunglasses woman man...\n",
       "..                  ...                                                ...\n",
       "859   __label__disabled  couple woman boat people water sea luxury summ...\n",
       "860   __label__disabled  sunglasses watersports man fun paddle oar wate...\n",
       "861   __label__disabled  sand water leisure beach sea tropical vacation...\n",
       "862   __label__disabled  vacation beach resort sun palm swimmingpool wa...\n",
       "863   __label__disabled  people bird pet parrot love cute family nature...\n",
       "\n",
       "[864 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df2 = pd.read_csv('flair_classification_data/train.csv', sep='\\t', names=['label', 'text'])\n",
    "train_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from flair.data_fetcher import NLPTaskDataFetcher\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentRNNEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-06 12:51:04,492 Reading data from flair_classification_data\n",
      "2020-04-06 12:51:04,493 Train: flair_classification_data/train.csv\n",
      "2020-04-06 12:51:04,494 Dev: flair_classification_data/dev.csv\n",
      "2020-04-06 12:51:04,495 Test: flair_classification_data/test.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/efficientnet/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated function (or staticmethod) load_classification_corpus. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  \"\"\"\n",
      "/home/ec2-user/anaconda3/envs/efficientnet/lib/python3.6/site-packages/flair/data_fetcher.py:452: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  train_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n",
      "/home/ec2-user/anaconda3/envs/efficientnet/lib/python3.6/site-packages/flair/data_fetcher.py:457: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  test_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n",
      "/home/ec2-user/anaconda3/envs/efficientnet/lib/python3.6/site-packages/flair/data_fetcher.py:464: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  dev_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n"
     ]
    }
   ],
   "source": [
    "corpus = NLPTaskDataFetcher.load_classification_corpus(\n",
    "    Path('flair_classification_data'),\n",
    "    test_file='test.csv',\n",
    "    dev_file='dev.csv',\n",
    "    train_file='train.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-06 14:03:19,483 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 864/864 [00:00<00:00, 167989.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-06 14:03:19,492 [b'published', b'disabled']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "label_dict = corpus.make_label_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>',\n",
       " 'hamiltonisland',\n",
       " 'the',\n",
       " 'water',\n",
       " 'beach',\n",
       " 'island',\n",
       " 'travel',\n",
       " 'sea',\n",
       " 'ocean',\n",
       " 'summer',\n",
       " 'nature',\n",
       " 'outdoors',\n",
       " 'to',\n",
       " 'australia',\n",
       " 'and',\n",
       " 'vacation',\n",
       " 'a',\n",
       " 'tropical',\n",
       " 'in',\n",
       " 'of',\n",
       " 'sky',\n",
       " 'noperson',\n",
       " 'person',\n",
       " 'people',\n",
       " 'human',\n",
       " 'whitsundays',\n",
       " 'hamilton',\n",
       " 'seashore',\n",
       " 'queensland',\n",
       " 'vehicle',\n",
       " 'tree',\n",
       " 'sun',\n",
       " 'landscape',\n",
       " 'リ',\n",
       " 'boat',\n",
       " 'ト',\n",
       " 'recreation',\n",
       " 'ン',\n",
       " 'sunset',\n",
       " 'on',\n",
       " 'land',\n",
       " 'sand',\n",
       " 'we',\n",
       " 'leisure',\n",
       " 'coast',\n",
       " 'i',\n",
       " 'shoreline',\n",
       " 'is',\n",
       " 'transportation',\n",
       " 'for',\n",
       " 'this',\n",
       " 'you',\n",
       " 'it',\n",
       " 'plant',\n",
       " 'with',\n",
       " 'woman',\n",
       " 'た',\n",
       " 'beautiful',\n",
       " 'relaxation',\n",
       " 'watercraft',\n",
       " 'resort',\n",
       " 's',\n",
       " 'greatbarrierreef',\n",
       " 'one',\n",
       " 'い',\n",
       " 'fun',\n",
       " 'our',\n",
       " 'girl',\n",
       " 'love',\n",
       " 'の',\n",
       " 'ス',\n",
       " 'portrait',\n",
       " 'hotel',\n",
       " 'day',\n",
       " 'ア',\n",
       " 'at',\n",
       " 'man',\n",
       " 'seascape',\n",
       " 'reef',\n",
       " 'vessel',\n",
       " 'lake',\n",
       " 'building',\n",
       " 'ラ',\n",
       " 'scenery',\n",
       " 'exotic',\n",
       " 'な',\n",
       " 'promontory',\n",
       " 'っ',\n",
       " 'に',\n",
       " 'yacht',\n",
       " '行',\n",
       " 'turquoise',\n",
       " 'family',\n",
       " 'オ',\n",
       " 'bay',\n",
       " 'so',\n",
       " 'clothing',\n",
       " 'whitehavenbeach',\n",
       " 'apparel',\n",
       " 'て',\n",
       " 'で',\n",
       " 'し',\n",
       " 'be',\n",
       " 'も',\n",
       " 't',\n",
       " 'luxury',\n",
       " 'my',\n",
       " 'adventure',\n",
       " 'marina',\n",
       " 'animal',\n",
       " 'sunglasses',\n",
       " 'paradise',\n",
       " 'two',\n",
       " 'fairweather',\n",
       " 'palm',\n",
       " 'was',\n",
       " 'が',\n",
       " 'ル',\n",
       " 'idyllic',\n",
       " 'swimming',\n",
       " 'your',\n",
       " 'are',\n",
       " 'from',\n",
       " 'me',\n",
       " 'whitsundayislands',\n",
       " '아',\n",
       " 'travelgram',\n",
       " 'that',\n",
       " 'る',\n",
       " 'sailboat',\n",
       " '주',\n",
       " 'chair',\n",
       " '일',\n",
       " '호',\n",
       " 'ワ',\n",
       " 'holiday',\n",
       " 'ホ',\n",
       " 'qualia',\n",
       " 'dawn',\n",
       " 'food',\n",
       " 'furniture',\n",
       " 'vegetation',\n",
       " 'travelphotography',\n",
       " 'adult',\n",
       " 'an',\n",
       " 'wood',\n",
       " 'wildlife',\n",
       " 'ビ',\n",
       " 'palmtree',\n",
       " 'whitehaven',\n",
       " '해',\n",
       " 'と',\n",
       " 'ら',\n",
       " '2020',\n",
       " '旅',\n",
       " 'す',\n",
       " 'ハ',\n",
       " 'coral',\n",
       " 'arecaceae',\n",
       " 'all',\n",
       " 'great',\n",
       " 'pier',\n",
       " 'wildoatsxi',\n",
       " 'indoors',\n",
       " 'photography',\n",
       " '이',\n",
       " '島',\n",
       " 'by',\n",
       " 'but',\n",
       " 'り',\n",
       " 'table',\n",
       " 'dusk',\n",
       " 'フ',\n",
       " 'can',\n",
       " 'more',\n",
       " 'bird',\n",
       " 'have',\n",
       " 'amazing',\n",
       " 'は',\n",
       " 'golf',\n",
       " 'architecture',\n",
       " 'best',\n",
       " 'seeaustralia',\n",
       " 'race',\n",
       " '밀',\n",
       " '턴',\n",
       " '랜',\n",
       " '드',\n",
       " 'か',\n",
       " 'チ',\n",
       " 'what',\n",
       " 'sunrise',\n",
       " 'like',\n",
       " 'thisisqueensland',\n",
       " 'wedding',\n",
       " 'up',\n",
       " '리',\n",
       " 'ッ',\n",
       " 'waterfront',\n",
       " 'fish',\n",
       " 'light',\n",
       " 'back',\n",
       " 'pool',\n",
       " 'view',\n",
       " 'イ',\n",
       " 'leisureactivities',\n",
       " 'dugoutpool',\n",
       " 'swimmingpool',\n",
       " 'no',\n",
       " 'islandlife',\n",
       " 'sport',\n",
       " 'lovewhitsundays',\n",
       " 'will',\n",
       " '여',\n",
       " '행',\n",
       " 'underwater',\n",
       " 'weekendaway',\n",
       " 'couple',\n",
       " 'lagoon',\n",
       " 'place',\n",
       " 'lifestyle',\n",
       " '海',\n",
       " 'き',\n",
       " 'fashion',\n",
       " 'as',\n",
       " 'most',\n",
       " 'ミ',\n",
       " 'バ',\n",
       " 'birthday',\n",
       " 'gateway',\n",
       " 'business',\n",
       " 'how',\n",
       " '다',\n",
       " 'little',\n",
       " 'dock',\n",
       " 'port',\n",
       " 'or',\n",
       " 'time',\n",
       " 'us',\n",
       " 'city',\n",
       " 'mountain',\n",
       " 'get',\n",
       " 'レ',\n",
       " 'ま',\n",
       " 'transportationsystem',\n",
       " 'child',\n",
       " 'shorts',\n",
       " 'diving',\n",
       " 'sydney',\n",
       " 'here',\n",
       " '7',\n",
       " 'birthdaytrip',\n",
       " 'there',\n",
       " 'カ',\n",
       " 'travelling',\n",
       " 've',\n",
       " 'enjoyment',\n",
       " 'wild',\n",
       " 'ig',\n",
       " 'だ',\n",
       " 'today',\n",
       " 'happy',\n",
       " 'just',\n",
       " 'bikini',\n",
       " 'photo',\n",
       " 'sunlight',\n",
       " 'new',\n",
       " 'welldeserved',\n",
       " 'if',\n",
       " '비',\n",
       " 'グ',\n",
       " 'cute',\n",
       " 'home',\n",
       " 'れ',\n",
       " 'redsky',\n",
       " 'not',\n",
       " 'desktop',\n",
       " 'got',\n",
       " 'snorkeling',\n",
       " 'morning',\n",
       " '나',\n",
       " 'drink',\n",
       " 'qualiaresort',\n",
       " 'countryside',\n",
       " 'grass',\n",
       " 'coconut',\n",
       " '好',\n",
       " 'color',\n",
       " 'life',\n",
       " 'く',\n",
       " 'airliebeach',\n",
       " 'trip',\n",
       " '치',\n",
       " '外',\n",
       " 'シ',\n",
       " 'reflection',\n",
       " 'over',\n",
       " 'barrier',\n",
       " '1',\n",
       " 'australiagram',\n",
       " 'road',\n",
       " '가',\n",
       " 'perfect',\n",
       " 'タ',\n",
       " 'airplane',\n",
       " 'out',\n",
       " 'ん',\n",
       " 'room',\n",
       " 'house',\n",
       " '日',\n",
       " 'キ',\n",
       " 'evening',\n",
       " 'seat',\n",
       " 'interiordesign',\n",
       " 'do',\n",
       " '安',\n",
       " 'aircraft',\n",
       " 'け',\n",
       " 'awesome',\n",
       " 'days',\n",
       " 'bio',\n",
       " 'young',\n",
       " 'car',\n",
       " 'smile',\n",
       " 'world',\n",
       " 'harbor',\n",
       " 'garden',\n",
       " 'zoo',\n",
       " 'whiteheavenbeach',\n",
       " 'views',\n",
       " 'has',\n",
       " 'week',\n",
       " '2',\n",
       " 'ク',\n",
       " 'restaurant',\n",
       " 'take',\n",
       " 'bride',\n",
       " 'look',\n",
       " 'rock',\n",
       " '3',\n",
       " 'leaf',\n",
       " 'surf',\n",
       " 'good',\n",
       " 'see',\n",
       " 'や',\n",
       " 'ュ',\n",
       " 'these',\n",
       " 'club',\n",
       " 'sports',\n",
       " 'weather',\n",
       " 'wanderlust',\n",
       " 'female',\n",
       " '사',\n",
       " 'window',\n",
       " 'celebratingmybirthdayinstyle',\n",
       " 'tropicalparadise',\n",
       " 'ever',\n",
       " 'make',\n",
       " 'ど',\n",
       " 'koala',\n",
       " 'visitqueensland',\n",
       " 'crew',\n",
       " 'action',\n",
       " '5',\n",
       " 'because',\n",
       " 'seen',\n",
       " 'again',\n",
       " '지',\n",
       " 'picoftheday',\n",
       " 'hill',\n",
       " 'daylight',\n",
       " 'park',\n",
       " 'こ',\n",
       " 'togetherness',\n",
       " 'cockatoo',\n",
       " 'first',\n",
       " 'event',\n",
       " 'heartreef',\n",
       " 'helicopter',\n",
       " '美',\n",
       " 'thanks',\n",
       " 'competition',\n",
       " 'romance',\n",
       " 'mammal',\n",
       " 'sailing',\n",
       " 'holidayherethisyear',\n",
       " 'link',\n",
       " 'sexy',\n",
       " 'model',\n",
       " '워',\n",
       " 'ブ',\n",
       " '기',\n",
       " 'been',\n",
       " 'weekend',\n",
       " 'weddingideas',\n",
       " 'who',\n",
       " '一',\n",
       " 'face',\n",
       " 'ニ',\n",
       " 'experience',\n",
       " 'photooftheday',\n",
       " 'peninsula',\n",
       " 'bronzing',\n",
       " 'had',\n",
       " 'now',\n",
       " 'cloud',\n",
       " 'watersports',\n",
       " 'laserworlds',\n",
       " 'glass',\n",
       " 'year',\n",
       " 'f',\n",
       " 'military',\n",
       " 'captures',\n",
       " 'incredible',\n",
       " 'わ',\n",
       " 'stunning',\n",
       " 'only',\n",
       " 'than',\n",
       " 'when',\n",
       " 'urban',\n",
       " 'river',\n",
       " 'such',\n",
       " 'accessory',\n",
       " 'accessories',\n",
       " 'friends',\n",
       " '見',\n",
       " 'health',\n",
       " 'wear',\n",
       " 'really',\n",
       " 'some',\n",
       " 'much',\n",
       " 'were',\n",
       " 'live',\n",
       " 'sail',\n",
       " '18',\n",
       " '홀',\n",
       " '是',\n",
       " 'r',\n",
       " 'silhouette',\n",
       " 'visitaustralia',\n",
       " '10',\n",
       " 'being',\n",
       " 'poolside',\n",
       " 'group',\n",
       " 'hiweddings',\n",
       " 'flare',\n",
       " 'com',\n",
       " 'around',\n",
       " 'design',\n",
       " 'coralreef',\n",
       " 'whitsunday',\n",
       " 'd',\n",
       " 'catseyebeach',\n",
       " 'cruisewhitsundays',\n",
       " 'patio',\n",
       " 'field',\n",
       " 'wet',\n",
       " 'ship',\n",
       " 'art',\n",
       " 'm',\n",
       " 'ド',\n",
       " 'go',\n",
       " 'y',\n",
       " 'flight',\n",
       " '的',\n",
       " 'avpartners',\n",
       " 'made',\n",
       " 'dinner',\n",
       " '신',\n",
       " '혼',\n",
       " 'symbol',\n",
       " 'start',\n",
       " 'sealife',\n",
       " 'where',\n",
       " 'og',\n",
       " 'aerialview',\n",
       " 'parrot',\n",
       " 'あ',\n",
       " 'instagood',\n",
       " 'would',\n",
       " '繋',\n",
       " 'last',\n",
       " 'they',\n",
       " 'つ',\n",
       " 'next',\n",
       " 'qld',\n",
       " 'ョ',\n",
       " 'う',\n",
       " 'rolexsydneyhobart',\n",
       " 'holidays',\n",
       " '클',\n",
       " '럽',\n",
       " 'happiness',\n",
       " 'places',\n",
       " 'every',\n",
       " 'years',\n",
       " 'flying',\n",
       " 'don',\n",
       " 'real',\n",
       " 'de',\n",
       " 'always',\n",
       " 'many',\n",
       " '바',\n",
       " 'scenic',\n",
       " 'offroad',\n",
       " 'buggy',\n",
       " 'よ',\n",
       " 'oceanview',\n",
       " 'ヘ',\n",
       " 'delicious',\n",
       " 'instadaily',\n",
       " 'off',\n",
       " 'weddingdecor',\n",
       " '人',\n",
       " 'コ',\n",
       " 'blue',\n",
       " 'bar',\n",
       " 'meal',\n",
       " 'her',\n",
       " '에',\n",
       " 'special',\n",
       " 'shotz',\n",
       " 'boy',\n",
       " 'turtle',\n",
       " 'heart',\n",
       " 'into',\n",
       " 'swim',\n",
       " 'times',\n",
       " 'never',\n",
       " 'forex',\n",
       " 'さ',\n",
       " 'flower',\n",
       " 'wine',\n",
       " 'canon',\n",
       " 'beachlife',\n",
       " 'honeymoon',\n",
       " 'exploreaustralia',\n",
       " '데',\n",
       " 'へ',\n",
       " '本',\n",
       " 'australiaworkingholiday',\n",
       " 'baby',\n",
       " '어',\n",
       " 'rural',\n",
       " 'hamiltonislandgolfclub',\n",
       " 'pretty',\n",
       " 'bridge',\n",
       " 'え',\n",
       " 'inside',\n",
       " 'scuba',\n",
       " 'ベ',\n",
       " 'cocktails',\n",
       " 'golfcourse',\n",
       " 'another',\n",
       " 'instatravel',\n",
       " 'サ',\n",
       " 'を',\n",
       " '장',\n",
       " 'them',\n",
       " 'texture',\n",
       " 'come',\n",
       " 'fitness',\n",
       " 'ち',\n",
       " 'デ',\n",
       " 'top',\n",
       " 'sit',\n",
       " 'workandtravel',\n",
       " 'azuresky',\n",
       " 'beaches',\n",
       " 'exploringaustralia',\n",
       " 'workingholiday',\n",
       " 'white',\n",
       " '하',\n",
       " '고',\n",
       " 'panoramic',\n",
       " 'need',\n",
       " 'seaplane',\n",
       " 'composure',\n",
       " '4',\n",
       " 'weddingstyling',\n",
       " 'better',\n",
       " 'ぎ',\n",
       " 'line',\n",
       " 'localthriller',\n",
       " 'virtuoso',\n",
       " 'deep',\n",
       " 'au',\n",
       " 'tourism',\n",
       " '50',\n",
       " 'sitting',\n",
       " 'onetreehill',\n",
       " 'hamiltonislandmarina',\n",
       " '出',\n",
       " 'sofa',\n",
       " 'their',\n",
       " 'gopro',\n",
       " 'she',\n",
       " 'am',\n",
       " 'congratulations',\n",
       " 'boys',\n",
       " '도',\n",
       " '스',\n",
       " 'reefviewhotel',\n",
       " 'traditional',\n",
       " 'explore',\n",
       " 'dm',\n",
       " 'lisafogartylive',\n",
       " 'tour',\n",
       " 'close',\n",
       " 'illustration',\n",
       " 're',\n",
       " 'bucketlist',\n",
       " 'regatta',\n",
       " 'spot',\n",
       " 'goaussailors',\n",
       " 'tokyotogether',\n",
       " 'whitebeach',\n",
       " 'crystalclearwater',\n",
       " 'wheel',\n",
       " 'wonderful',\n",
       " 'australian',\n",
       " 'discoverqueensland',\n",
       " '간',\n",
       " '만',\n",
       " '무',\n",
       " 'team',\n",
       " 'which',\n",
       " 'looking',\n",
       " 'railing',\n",
       " 'beauty',\n",
       " 'few',\n",
       " 'corals',\n",
       " 'away',\n",
       " 'cruise',\n",
       " 'www',\n",
       " '最',\n",
       " '綺',\n",
       " '麗',\n",
       " 'about',\n",
       " 'contemporary',\n",
       " 'lunch',\n",
       " '서',\n",
       " '우',\n",
       " '80',\n",
       " 'freedom',\n",
       " 'put',\n",
       " 'photos',\n",
       " 'even',\n",
       " 'trading',\n",
       " 'machine',\n",
       " 'tbt',\n",
       " '12',\n",
       " 'way',\n",
       " 'text',\n",
       " 'laser',\n",
       " 'zhikaustralia',\n",
       " '6',\n",
       " 'swing',\n",
       " 'sure',\n",
       " 'own',\n",
       " 'grouptogether',\n",
       " '方',\n",
       " '在',\n",
       " '非',\n",
       " 'thank',\n",
       " '은',\n",
       " '라',\n",
       " 'inspiration',\n",
       " 'marine',\n",
       " 'marryme',\n",
       " 'weddings',\n",
       " 'image',\n",
       " 'hiking',\n",
       " 'hat',\n",
       " 'truly',\n",
       " 'book',\n",
       " 'snorkel',\n",
       " 'homedecor',\n",
       " 'goggles',\n",
       " 'fishing',\n",
       " '食',\n",
       " 'ジ',\n",
       " 'ズ',\n",
       " '大',\n",
       " '会',\n",
       " 'ゴ',\n",
       " '礁',\n",
       " 'hievents',\n",
       " 'big',\n",
       " 'swimwear',\n",
       " 'while',\n",
       " 'naturephotography',\n",
       " '2018',\n",
       " 'dark',\n",
       " 'airport',\n",
       " 'buggies',\n",
       " 'cocktail',\n",
       " '랑',\n",
       " 'hamiltonislandweddings',\n",
       " 'sydneytohobart',\n",
       " 'images',\n",
       " 'joy',\n",
       " 'skin',\n",
       " 'e',\n",
       " 'air',\n",
       " 'party',\n",
       " 'finish',\n",
       " 'finedining',\n",
       " '보',\n",
       " '니',\n",
       " 'abstract',\n",
       " '8',\n",
       " 'turtles',\n",
       " 'feeling',\n",
       " 'until',\n",
       " 'wow',\n",
       " 'find',\n",
       " 'yes',\n",
       " 'cliff',\n",
       " 'worlds',\n",
       " 'sailmelbourne',\n",
       " 'lasersailing',\n",
       " 'ilca',\n",
       " 'laserclass',\n",
       " 'sandringhamyachtclub',\n",
       " 'cairns',\n",
       " 'mm',\n",
       " 'too',\n",
       " 'beachvibes',\n",
       " 'lifejacket',\n",
       " 'vest',\n",
       " 'environment',\n",
       " 'news',\n",
       " 'sunsets',\n",
       " '間',\n",
       " '当',\n",
       " 'whitesunday',\n",
       " 'breakfast',\n",
       " 'getaway',\n",
       " 'cold',\n",
       " 'course',\n",
       " 'relax',\n",
       " 'umbrella',\n",
       " 'swimmer',\n",
       " '휴',\n",
       " 'robe',\n",
       " 'gown',\n",
       " 'weddingday',\n",
       " 'drinks',\n",
       " 'bench',\n",
       " 'wing',\n",
       " 'part',\n",
       " 'invertebrate',\n",
       " 'capture',\n",
       " 'ロ',\n",
       " 'ガ',\n",
       " 'still',\n",
       " 'beautifuldestinations',\n",
       " 'villa',\n",
       " 'cumulus',\n",
       " 'travelblogger',\n",
       " 'マ',\n",
       " 'ナ',\n",
       " '気',\n",
       " 'ポ',\n",
       " 'traveltheworld',\n",
       " 'gorgeous',\n",
       " 'audiovisual',\n",
       " 'also',\n",
       " 'street',\n",
       " '天',\n",
       " 'll',\n",
       " '인',\n",
       " 'chocolate',\n",
       " 'hike',\n",
       " 'after',\n",
       " 'friendship',\n",
       " 'learn',\n",
       " 'dining',\n",
       " 'those',\n",
       " 'jetski',\n",
       " 'son',\n",
       " 'hello',\n",
       " 'tent',\n",
       " '트',\n",
       " 'und',\n",
       " 'why',\n",
       " 'said',\n",
       " 'paper',\n",
       " 'huge',\n",
       " 'rolex',\n",
       " 'tenerife',\n",
       " 'seaturtle',\n",
       " 'championship',\n",
       " 'melbourne',\n",
       " 'fast',\n",
       " 'radial',\n",
       " 'glassitems',\n",
       " 'celebration',\n",
       " 'anniversary',\n",
       " 'retro',\n",
       " 'full',\n",
       " 'old',\n",
       " '2019',\n",
       " 'oats',\n",
       " 'going',\n",
       " 'excited',\n",
       " 'oz',\n",
       " 'travelaustralia',\n",
       " '킹',\n",
       " 'perfection',\n",
       " 'bush',\n",
       " 'automobile',\n",
       " '変',\n",
       " 'facialexpression',\n",
       " 'toy',\n",
       " '한',\n",
       " 'h2o',\n",
       " 'alcohol',\n",
       " 'porch',\n",
       " 'having',\n",
       " 'down',\n",
       " 'luxurytravel',\n",
       " 'weddinginspo',\n",
       " 'groom',\n",
       " 'queenslandbride',\n",
       " 'weddingplanner',\n",
       " 'nightlife',\n",
       " 'sailinglife',\n",
       " 'path',\n",
       " 'parakeet',\n",
       " '世',\n",
       " '界',\n",
       " 'catseye',\n",
       " 'shoe',\n",
       " 'footwear',\n",
       " 'boats',\n",
       " 'hillinlet',\n",
       " 'balcony',\n",
       " 'l',\n",
       " 'favorite',\n",
       " 'each',\n",
       " 'hand',\n",
       " 'personal',\n",
       " 'discoveraustralia',\n",
       " 'receive',\n",
       " '遊',\n",
       " '飛',\n",
       " 'housing',\n",
       " 'doesn',\n",
       " 'celebrate',\n",
       " 'islands',\n",
       " 'diver',\n",
       " 'definitely',\n",
       " 'grassland',\n",
       " 'slope',\n",
       " 'un',\n",
       " 'fur',\n",
       " 'marsupial',\n",
       " 'flora',\n",
       " 'プ',\n",
       " 'palmtrees',\n",
       " '目',\n",
       " '中',\n",
       " '何',\n",
       " '婚',\n",
       " '映',\n",
       " 'affection',\n",
       " 'ケ',\n",
       " 'ろ',\n",
       " 'space',\n",
       " '爾',\n",
       " 'work',\n",
       " 'lighting',\n",
       " 'avpartnerswhitsundays',\n",
       " 'eventstylist',\n",
       " 'weddingstylist',\n",
       " '容',\n",
       " '師',\n",
       " 'tourist',\n",
       " 'christmas',\n",
       " 'four',\n",
       " 'islandvibes',\n",
       " 'coffee',\n",
       " 'weddingplanning',\n",
       " 'th',\n",
       " 'favourite',\n",
       " 'lovely',\n",
       " '히',\n",
       " 'alaniwhitsundays',\n",
       " 'lost',\n",
       " '그',\n",
       " 'wait',\n",
       " 'same',\n",
       " 'champagne',\n",
       " '19',\n",
       " 'designing',\n",
       " 'internet',\n",
       " 'opportunity',\n",
       " 'football',\n",
       " 'reptile',\n",
       " 'お',\n",
       " 'excuses',\n",
       " 'regrets',\n",
       " 'oscar',\n",
       " 'wilde',\n",
       " 'remembering',\n",
       " 'aussiland',\n",
       " 'terraaustralis',\n",
       " 'eurekatower',\n",
       " 'australiaopen',\n",
       " 'vamosrafa',\n",
       " 'greatoceanroad',\n",
       " 'apostles',\n",
       " 'helicopterride',\n",
       " 'sydneyoperahouse',\n",
       " 'sydneybridgeclimb',\n",
       " 'citizenoftheworld',\n",
       " 'thor',\n",
       " 'dani',\n",
       " 'service',\n",
       " 'rowboat',\n",
       " 'aussies',\n",
       " 'body',\n",
       " 'dress',\n",
       " 'hobart',\n",
       " 'absolutely',\n",
       " '끝',\n",
       " '캐',\n",
       " '츠',\n",
       " 'austrip',\n",
       " 'austravel',\n",
       " 'swipe',\n",
       " 'exercise',\n",
       " 'ive',\n",
       " '時',\n",
       " '色',\n",
       " '마',\n",
       " 'takemeback',\n",
       " 'wallaby',\n",
       " 'nofilter',\n",
       " 'breathtaking',\n",
       " 'yachting',\n",
       " 'aquatic',\n",
       " 'awesomeearth',\n",
       " 'weddinginspiration',\n",
       " 'livingourbestlife',\n",
       " 'bright',\n",
       " 'ず',\n",
       " 'town',\n",
       " 'beer',\n",
       " 'available',\n",
       " 'aerial',\n",
       " 'natural',\n",
       " 'night',\n",
       " 'hamiltonislandbusinessevents',\n",
       " 'o',\n",
       " 'floor',\n",
       " 'keep',\n",
       " 'watch',\n",
       " 'buy',\n",
       " 'earth',\n",
       " 'ourplanetdaily',\n",
       " 'further',\n",
       " 'backyard',\n",
       " 'set',\n",
       " 'amongst',\n",
       " 'repost',\n",
       " '足',\n",
       " 'bondibeach',\n",
       " 'テ',\n",
       " 'any',\n",
       " '機',\n",
       " 'hard',\n",
       " 'hamiltonislandpoolbar',\n",
       " 'weisbar',\n",
       " 'pinacolada',\n",
       " 'he',\n",
       " 'beachclub',\n",
       " 'eye',\n",
       " 'hair',\n",
       " 'wilderness',\n",
       " 'modern',\n",
       " 'パ',\n",
       " 'メ',\n",
       " '移',\n",
       " 'ム',\n",
       " 'そ',\n",
       " '空',\n",
       " '有',\n",
       " '艾',\n",
       " '利',\n",
       " '記',\n",
       " 'taking',\n",
       " 'painting',\n",
       " 'hibusinessevents',\n",
       " 'something',\n",
       " 'through',\n",
       " 'catalanspelmon',\n",
       " 'without',\n",
       " ...]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.make_vocab_dictionary().get_items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_embeddings = DocumentRNNEmbeddings([\n",
    "        WordEmbeddings('twitter'),\n",
    "    #     FlairEmbeddings('news-forward'),\n",
    "    #     FlairEmbeddings('news-backward')\n",
    "    ], \n",
    "    hidden_size=128,\n",
    "    reproject_words=True,\n",
    "    reproject_words_dimension=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "from flair.embeddings import Sentence\n",
    "\n",
    "sentence1 = Sentence('The grass is green . And the sky is blue .')\n",
    "\n",
    "# embed the sentence with our document embedding\n",
    "document_embeddings.embed(sentence1)\n",
    "\n",
    "# now check out the embedded sentence.\n",
    "print(sentence1.get_embedding().shape)\n",
    "\n",
    "sentence2 = Sentence(\"\"\"It accounts for virtually all discussion in the media, enjoying priority over such topics as the 2020 US presidential election or the UK finally leaving the EU for good in less than 9 months. People are flooding social media with COVID information, which can only mean one thing: data. Fresh data waiting to be analysed. And analyse it we will.\"\"\")\n",
    "\n",
    "# embed the sentence with our document embedding\n",
    "document_embeddings.embed(sentence2)\n",
    "\n",
    "# now check out the embedded sentence.\n",
    "print(sentence2.get_embedding().shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-06 12:51:21,221 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 864/864 [00:00<00:00, 23923.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-06 12:51:21,261 [b'published', b'disabled']\n",
      "TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('twitter')\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=100, out_features=128, bias=True)\n",
      "    (rnn): GRU(128, 128, batch_first=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "classifier = TextClassifier(\n",
    "    document_embeddings, \n",
    "    label_dictionary=corpus.make_label_dictionary(),\n",
    "    multi_label=False\n",
    ")\n",
    "\n",
    "print(classifier)\n",
    "\n",
    "trainer = ModelTrainer(classifier, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-06 12:33:08,182 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-06 12:33:08,183 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('twitter')\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=100, out_features=128, bias=True)\n",
      "    (rnn): GRU(128, 128, batch_first=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2020-04-06 12:33:08,184 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-06 12:33:08,185 Corpus: \"Corpus: 864 train + 345 dev + 231 test sentences\"\n",
      "2020-04-06 12:33:08,186 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-06 12:33:08,187 Parameters:\n",
      "2020-04-06 12:33:08,188  - learning_rate: \"0.1\"\n",
      "2020-04-06 12:33:08,189  - mini_batch_size: \"32\"\n",
      "2020-04-06 12:33:08,190  - patience: \"3\"\n",
      "2020-04-06 12:33:08,191  - anneal_factor: \"0.5\"\n",
      "2020-04-06 12:33:08,192  - max_epochs: \"20\"\n",
      "2020-04-06 12:33:08,193  - shuffle: \"True\"\n",
      "2020-04-06 12:33:08,193  - train_with_dev: \"False\"\n",
      "2020-04-06 12:33:08,194  - batch_growth_annealing: \"False\"\n",
      "2020-04-06 12:33:08,195 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-06 12:33:08,195 Model training base path: \".\"\n",
      "2020-04-06 12:33:08,197 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-06 12:33:08,198 Device: cuda:0\n",
      "2020-04-06 12:33:08,199 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-06 12:33:08,200 Embeddings storage mode: cpu\n",
      "2020-04-06 12:33:08,202 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-06 12:33:08,392 epoch 1 - iter 2/27 - loss 0.69917914 - samples/sec: 338.94\n",
      "2020-04-06 12:33:08,622 epoch 1 - iter 4/27 - loss 0.72004214 - samples/sec: 292.00\n",
      "2020-04-06 12:33:08,831 epoch 1 - iter 6/27 - loss 0.71943898 - samples/sec: 318.11\n",
      "2020-04-06 12:33:09,047 epoch 1 - iter 8/27 - loss 0.70183486 - samples/sec: 307.63\n",
      "2020-04-06 12:33:09,259 epoch 1 - iter 10/27 - loss 0.70303022 - samples/sec: 313.78\n",
      "2020-04-06 12:33:09,514 epoch 1 - iter 12/27 - loss 0.70629503 - samples/sec: 258.83\n",
      "2020-04-06 12:33:09,789 epoch 1 - iter 14/27 - loss 0.71633598 - samples/sec: 239.46\n",
      "2020-04-06 12:33:10,044 epoch 1 - iter 16/27 - loss 0.72670688 - samples/sec: 259.67\n",
      "2020-04-06 12:33:10,264 epoch 1 - iter 18/27 - loss 0.71999855 - samples/sec: 301.21\n",
      "2020-04-06 12:33:10,492 epoch 1 - iter 20/27 - loss 0.71790887 - samples/sec: 291.89\n",
      "2020-04-06 12:33:10,716 epoch 1 - iter 22/27 - loss 0.71339566 - samples/sec: 295.57\n",
      "2020-04-06 12:33:10,895 epoch 1 - iter 24/27 - loss 0.70865790 - samples/sec: 373.88\n",
      "2020-04-06 12:33:11,076 epoch 1 - iter 26/27 - loss 0.70776766 - samples/sec: 370.46\n",
      "2020-04-06 12:33:11,170 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-06 12:33:11,171 EPOCH 1 done: loss 0.7116 - lr 0.1000\n",
      "2020-04-06 12:33:12,008 DEV : loss 0.6663678288459778 - score 0.6029\n",
      "2020-04-06 12:33:12,052 BAD EPOCHS (no improvement): 0\n",
      "2020-04-06 12:33:27,416 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-06 12:33:27,648 epoch 2 - iter 2/27 - loss 0.77432308 - samples/sec: 278.89\n",
      "2020-04-06 12:33:27,899 epoch 2 - iter 4/27 - loss 0.71528587 - samples/sec: 263.71\n",
      "2020-04-06 12:33:28,098 epoch 2 - iter 6/27 - loss 0.69840281 - samples/sec: 334.91\n",
      "2020-04-06 12:33:28,356 epoch 2 - iter 8/27 - loss 0.68252080 - samples/sec: 256.07\n",
      "2020-04-06 12:33:28,596 epoch 2 - iter 10/27 - loss 0.68860831 - samples/sec: 275.47\n",
      "2020-04-06 12:33:28,810 epoch 2 - iter 12/27 - loss 0.69014771 - samples/sec: 312.30\n",
      "2020-04-06 12:33:29,012 epoch 2 - iter 14/27 - loss 0.68608846 - samples/sec: 329.81\n",
      "2020-04-06 12:33:29,207 epoch 2 - iter 16/27 - loss 0.69832264 - samples/sec: 344.40\n",
      "2020-04-06 12:33:29,386 epoch 2 - iter 18/27 - loss 0.69824932 - samples/sec: 375.45\n",
      "2020-04-06 12:33:29,607 epoch 2 - iter 20/27 - loss 0.69929634 - samples/sec: 300.54\n",
      "2020-04-06 12:33:29,841 epoch 2 - iter 22/27 - loss 0.69764576 - samples/sec: 283.46\n",
      "2020-04-06 12:33:30,070 epoch 2 - iter 24/27 - loss 0.70195264 - samples/sec: 290.51\n",
      "2020-04-06 12:33:30,297 epoch 2 - iter 26/27 - loss 0.70198532 - samples/sec: 292.24\n",
      "2020-04-06 12:33:30,401 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-06 12:33:30,402 EPOCH 2 done: loss 0.7023 - lr 0.1000\n",
      "2020-04-06 12:33:31,236 DEV : loss 0.6800024509429932 - score 0.6348\n",
      "2020-04-06 12:33:31,280 BAD EPOCHS (no improvement): 0\n",
      "2020-04-06 12:33:46,464 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-06 12:33:46,691 epoch 3 - iter 2/27 - loss 0.69073948 - samples/sec: 283.40\n",
      "2020-04-06 12:33:46,906 epoch 3 - iter 4/27 - loss 0.68546012 - samples/sec: 310.18\n",
      "2020-04-06 12:33:47,160 epoch 3 - iter 6/27 - loss 0.66201043 - samples/sec: 259.77\n",
      "2020-04-06 12:33:47,363 epoch 3 - iter 8/27 - loss 0.66759053 - samples/sec: 331.32\n",
      "2020-04-06 12:33:47,572 epoch 3 - iter 10/27 - loss 0.66917751 - samples/sec: 316.90\n",
      "2020-04-06 12:33:47,769 epoch 3 - iter 12/27 - loss 0.66236541 - samples/sec: 339.91\n",
      "2020-04-06 12:33:47,976 epoch 3 - iter 14/27 - loss 0.66857566 - samples/sec: 321.39\n",
      "2020-04-06 12:33:48,167 epoch 3 - iter 16/27 - loss 0.67857907 - samples/sec: 351.94\n",
      "2020-04-06 12:33:48,391 epoch 3 - iter 18/27 - loss 0.68357621 - samples/sec: 297.29\n",
      "2020-04-06 12:33:48,629 epoch 3 - iter 20/27 - loss 0.68085614 - samples/sec: 277.93\n",
      "2020-04-06 12:33:48,869 epoch 3 - iter 22/27 - loss 0.68143136 - samples/sec: 275.21\n",
      "2020-04-06 12:33:49,110 epoch 3 - iter 24/27 - loss 0.68029858 - samples/sec: 275.77\n",
      "2020-04-06 12:33:49,327 epoch 3 - iter 26/27 - loss 0.67831519 - samples/sec: 306.04\n",
      "2020-04-06 12:33:49,453 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-06 12:33:49,454 EPOCH 3 done: loss 0.6828 - lr 0.1000\n",
      "2020-04-06 12:33:50,276 DEV : loss 0.667547345161438 - score 0.6145\n",
      "2020-04-06 12:33:50,320 BAD EPOCHS (no improvement): 1\n",
      "2020-04-06 12:33:50,321 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-06 12:33:50,516 epoch 4 - iter 2/27 - loss 0.69369909 - samples/sec: 332.27\n",
      "2020-04-06 12:33:50,718 epoch 4 - iter 4/27 - loss 0.67772144 - samples/sec: 328.59\n",
      "2020-04-06 12:33:50,941 epoch 4 - iter 6/27 - loss 0.68861796 - samples/sec: 297.64\n",
      "2020-04-06 12:33:51,192 epoch 4 - iter 8/27 - loss 0.67682268 - samples/sec: 263.07\n",
      "2020-04-06 12:33:51,439 epoch 4 - iter 10/27 - loss 0.67887433 - samples/sec: 267.75\n",
      "2020-04-06 12:33:51,683 epoch 4 - iter 12/27 - loss 0.67820923 - samples/sec: 272.15\n",
      "2020-04-06 12:33:51,864 epoch 4 - iter 14/27 - loss 0.66727990 - samples/sec: 370.19\n",
      "2020-04-06 12:33:52,045 epoch 4 - iter 16/27 - loss 0.66809595 - samples/sec: 371.19\n",
      "2020-04-06 12:33:52,266 epoch 4 - iter 18/27 - loss 0.67079005 - samples/sec: 301.31\n",
      "2020-04-06 12:33:52,506 epoch 4 - iter 20/27 - loss 0.67404150 - samples/sec: 276.63\n",
      "2020-04-06 12:33:52,714 epoch 4 - iter 22/27 - loss 0.67642261 - samples/sec: 321.94\n",
      "2020-04-06 12:33:52,928 epoch 4 - iter 24/27 - loss 0.67779735 - samples/sec: 311.26\n",
      "2020-04-06 12:33:53,155 epoch 4 - iter 26/27 - loss 0.68231617 - samples/sec: 291.23\n",
      "2020-04-06 12:33:53,264 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-06 12:33:53,265 EPOCH 4 done: loss 0.6863 - lr 0.1000\n",
      "2020-04-06 12:33:54,082 DEV : loss 0.6815690994262695 - score 0.5478\n",
      "2020-04-06 12:33:54,126 BAD EPOCHS (no improvement): 2\n",
      "2020-04-06 12:33:54,127 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-06 12:33:54,321 epoch 5 - iter 2/27 - loss 0.64334485 - samples/sec: 331.33\n",
      "2020-04-06 12:33:54,520 epoch 5 - iter 4/27 - loss 0.66364813 - samples/sec: 335.90\n",
      "2020-04-06 12:33:54,760 epoch 5 - iter 6/27 - loss 0.66396772 - samples/sec: 275.61\n",
      "2020-04-06 12:33:54,969 epoch 5 - iter 8/27 - loss 0.67268697 - samples/sec: 317.43\n",
      "2020-04-06 12:33:55,218 epoch 5 - iter 10/27 - loss 0.67805111 - samples/sec: 265.83\n",
      "2020-04-06 12:33:55,422 epoch 5 - iter 12/27 - loss 0.67275711 - samples/sec: 327.65\n",
      "2020-04-06 12:33:55,686 epoch 5 - iter 14/27 - loss 0.66785309 - samples/sec: 250.46\n",
      "2020-04-06 12:33:55,856 epoch 5 - iter 16/27 - loss 0.67033946 - samples/sec: 395.71\n",
      "2020-04-06 12:33:56,071 epoch 5 - iter 18/27 - loss 0.66519839 - samples/sec: 309.93\n",
      "2020-04-06 12:33:56,291 epoch 5 - iter 20/27 - loss 0.67047878 - samples/sec: 302.43\n",
      "2020-04-06 12:33:56,495 epoch 5 - iter 22/27 - loss 0.66634789 - samples/sec: 326.47\n",
      "2020-04-06 12:33:56,703 epoch 5 - iter 24/27 - loss 0.66875681 - samples/sec: 320.81\n",
      "2020-04-06 12:33:56,928 epoch 5 - iter 26/27 - loss 0.66884740 - samples/sec: 295.95\n",
      "2020-04-06 12:33:57,060 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-06 12:33:57,061 EPOCH 5 done: loss 0.6666 - lr 0.1000\n",
      "2020-04-06 12:33:57,875 DEV : loss 0.6609984636306763 - score 0.6638\n",
      "2020-04-06 12:33:57,917 BAD EPOCHS (no improvement): 0\n",
      "2020-04-06 12:34:13,197 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-06 12:34:13,426 epoch 6 - iter 2/27 - loss 0.62969372 - samples/sec: 281.11\n",
      "2020-04-06 12:34:13,657 epoch 6 - iter 4/27 - loss 0.64156912 - samples/sec: 287.09\n",
      "2020-04-06 12:34:13,838 epoch 6 - iter 6/27 - loss 0.64348637 - samples/sec: 369.92\n",
      "2020-04-06 12:34:14,078 epoch 6 - iter 8/27 - loss 0.65695767 - samples/sec: 275.39\n",
      "2020-04-06 12:34:14,279 epoch 6 - iter 10/27 - loss 0.65685831 - samples/sec: 331.68\n",
      "2020-04-06 12:34:14,514 epoch 6 - iter 12/27 - loss 0.65479491 - samples/sec: 281.75\n",
      "2020-04-06 12:34:14,703 epoch 6 - iter 14/27 - loss 0.66552642 - samples/sec: 353.82\n",
      "2020-04-06 12:34:14,917 epoch 6 - iter 16/27 - loss 0.66497304 - samples/sec: 310.55\n",
      "2020-04-06 12:34:15,105 epoch 6 - iter 18/27 - loss 0.66189238 - samples/sec: 356.47\n",
      "2020-04-06 12:34:15,340 epoch 6 - iter 20/27 - loss 0.66089380 - samples/sec: 282.64\n",
      "2020-04-06 12:34:15,553 epoch 6 - iter 22/27 - loss 0.66043369 - samples/sec: 312.04\n",
      "2020-04-06 12:34:15,787 epoch 6 - iter 24/27 - loss 0.66209911 - samples/sec: 284.10\n",
      "2020-04-06 12:34:15,965 epoch 6 - iter 26/27 - loss 0.66672328 - samples/sec: 377.63\n",
      "2020-04-06 12:34:16,111 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-06 12:34:16,112 EPOCH 6 done: loss 0.6654 - lr 0.1000\n",
      "2020-04-06 12:34:16,944 DEV : loss 0.6535009145736694 - score 0.6551\n",
      "2020-04-06 12:34:16,987 BAD EPOCHS (no improvement): 1\n",
      "2020-04-06 12:34:16,988 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-06 12:34:17,202 epoch 7 - iter 2/27 - loss 0.70445481 - samples/sec: 301.38\n",
      "2020-04-06 12:34:17,422 epoch 7 - iter 4/27 - loss 0.67361197 - samples/sec: 301.65\n",
      "2020-04-06 12:34:17,638 epoch 7 - iter 6/27 - loss 0.66694806 - samples/sec: 307.98\n",
      "2020-04-06 12:34:17,855 epoch 7 - iter 8/27 - loss 0.66509200 - samples/sec: 306.63\n",
      "2020-04-06 12:34:18,052 epoch 7 - iter 10/27 - loss 0.67551133 - samples/sec: 336.60\n",
      "2020-04-06 12:34:18,328 epoch 7 - iter 12/27 - loss 0.67251141 - samples/sec: 239.52\n",
      "2020-04-06 12:34:18,518 epoch 7 - iter 14/27 - loss 0.67604879 - samples/sec: 349.41\n",
      "2020-04-06 12:34:18,730 epoch 7 - iter 16/27 - loss 0.68010123 - samples/sec: 314.23\n",
      "2020-04-06 12:34:18,934 epoch 7 - iter 18/27 - loss 0.67888065 - samples/sec: 328.26\n",
      "2020-04-06 12:34:19,127 epoch 7 - iter 20/27 - loss 0.67632743 - samples/sec: 346.22\n",
      "2020-04-06 12:34:19,383 epoch 7 - iter 22/27 - loss 0.67790161 - samples/sec: 259.38\n",
      "2020-04-06 12:34:19,594 epoch 7 - iter 24/27 - loss 0.67502607 - samples/sec: 314.51\n",
      "2020-04-06 12:34:19,779 epoch 7 - iter 26/27 - loss 0.67149175 - samples/sec: 363.83\n",
      "2020-04-06 12:34:19,890 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-06 12:34:19,891 EPOCH 7 done: loss 0.6696 - lr 0.1000\n",
      "2020-04-06 12:34:20,717 DEV : loss 0.6652097105979919 - score 0.6725\n",
      "2020-04-06 12:34:20,759 BAD EPOCHS (no improvement): 0\n",
      "2020-04-06 12:34:35,890 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-06 12:34:36,095 epoch 8 - iter 2/27 - loss 0.68653402 - samples/sec: 316.18\n",
      "2020-04-06 12:34:36,308 epoch 8 - iter 4/27 - loss 0.67396188 - samples/sec: 313.05\n",
      "2020-04-06 12:34:36,559 epoch 8 - iter 6/27 - loss 0.67620548 - samples/sec: 263.11\n",
      "2020-04-06 12:34:36,764 epoch 8 - iter 8/27 - loss 0.65339977 - samples/sec: 324.97\n",
      "2020-04-06 12:34:36,989 epoch 8 - iter 10/27 - loss 0.65265661 - samples/sec: 295.42\n",
      "2020-04-06 12:34:37,204 epoch 8 - iter 12/27 - loss 0.63814583 - samples/sec: 308.20\n",
      "2020-04-06 12:34:37,408 epoch 8 - iter 14/27 - loss 0.64092640 - samples/sec: 327.27\n",
      "2020-04-06 12:34:37,659 epoch 8 - iter 16/27 - loss 0.64962558 - samples/sec: 262.71\n",
      "2020-04-06 12:34:37,876 epoch 8 - iter 18/27 - loss 0.65058987 - samples/sec: 307.03\n",
      "2020-04-06 12:34:38,088 epoch 8 - iter 20/27 - loss 0.65521656 - samples/sec: 313.41\n",
      "2020-04-06 12:34:38,298 epoch 8 - iter 22/27 - loss 0.65955428 - samples/sec: 318.81\n",
      "2020-04-06 12:34:38,539 epoch 8 - iter 24/27 - loss 0.66149067 - samples/sec: 275.26\n",
      "2020-04-06 12:34:38,751 epoch 8 - iter 26/27 - loss 0.66310852 - samples/sec: 314.45\n",
      "2020-04-06 12:34:38,868 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-06 12:34:38,869 EPOCH 8 done: loss 0.6612 - lr 0.1000\n",
      "2020-04-06 12:34:39,724 DEV : loss 0.6578933596611023 - score 0.6638\n",
      "2020-04-06 12:34:39,768 BAD EPOCHS (no improvement): 1\n",
      "2020-04-06 12:34:39,770 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-06 12:34:40,022 epoch 9 - iter 2/27 - loss 0.65870228 - samples/sec: 254.93\n",
      "2020-04-06 12:34:40,217 epoch 9 - iter 4/27 - loss 0.66593085 - samples/sec: 343.14\n",
      "2020-04-06 12:34:40,401 epoch 9 - iter 6/27 - loss 0.63813956 - samples/sec: 363.30\n",
      "2020-04-06 12:34:40,649 epoch 9 - iter 8/27 - loss 0.63895979 - samples/sec: 267.58\n",
      "2020-04-06 12:34:40,830 epoch 9 - iter 10/27 - loss 0.64909045 - samples/sec: 369.56\n",
      "2020-04-06 12:34:41,025 epoch 9 - iter 12/27 - loss 0.65376044 - samples/sec: 341.44\n",
      "2020-04-06 12:34:41,279 epoch 9 - iter 14/27 - loss 0.65330137 - samples/sec: 261.73\n",
      "2020-04-06 12:34:41,489 epoch 9 - iter 16/27 - loss 0.64888567 - samples/sec: 316.68\n",
      "2020-04-06 12:34:41,699 epoch 9 - iter 18/27 - loss 0.65316161 - samples/sec: 316.82\n",
      "2020-04-06 12:34:41,927 epoch 9 - iter 20/27 - loss 0.65592936 - samples/sec: 290.93\n",
      "2020-04-06 12:34:42,126 epoch 9 - iter 22/27 - loss 0.65748872 - samples/sec: 334.65\n",
      "2020-04-06 12:34:42,326 epoch 9 - iter 24/27 - loss 0.65908737 - samples/sec: 332.66\n",
      "2020-04-06 12:34:42,563 epoch 9 - iter 26/27 - loss 0.65745222 - samples/sec: 279.98\n",
      "2020-04-06 12:34:42,694 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-06 12:34:42,695 EPOCH 9 done: loss 0.6568 - lr 0.1000\n",
      "2020-04-06 12:34:43,537 DEV : loss 0.6520576477050781 - score 0.6696\n",
      "2020-04-06 12:34:43,581 BAD EPOCHS (no improvement): 2\n",
      "2020-04-06 12:34:43,582 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-06 12:34:43,801 epoch 10 - iter 2/27 - loss 0.69658911 - samples/sec: 294.03\n",
      "2020-04-06 12:34:44,036 epoch 10 - iter 4/27 - loss 0.62726694 - samples/sec: 281.69\n",
      "2020-04-06 12:34:44,216 epoch 10 - iter 6/27 - loss 0.63534017 - samples/sec: 373.64\n",
      "2020-04-06 12:34:44,465 epoch 10 - iter 8/27 - loss 0.62847609 - samples/sec: 264.95\n",
      "2020-04-06 12:34:44,715 epoch 10 - iter 10/27 - loss 0.63530669 - samples/sec: 264.65\n",
      "2020-04-06 12:34:44,899 epoch 10 - iter 12/27 - loss 0.63708122 - samples/sec: 364.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-06 12:34:45,118 epoch 10 - iter 14/27 - loss 0.64778780 - samples/sec: 303.12\n",
      "2020-04-06 12:34:45,304 epoch 10 - iter 16/27 - loss 0.64652052 - samples/sec: 361.39\n",
      "2020-04-06 12:34:45,501 epoch 10 - iter 18/27 - loss 0.64474945 - samples/sec: 336.85\n",
      "2020-04-06 12:34:45,712 epoch 10 - iter 20/27 - loss 0.64191903 - samples/sec: 316.46\n",
      "2020-04-06 12:34:45,934 epoch 10 - iter 22/27 - loss 0.64240784 - samples/sec: 300.67\n",
      "2020-04-06 12:34:46,187 epoch 10 - iter 24/27 - loss 0.64137957 - samples/sec: 261.14\n",
      "2020-04-06 12:34:46,443 epoch 10 - iter 26/27 - loss 0.64735124 - samples/sec: 257.45\n",
      "2020-04-06 12:34:46,559 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-06 12:34:46,560 EPOCH 10 done: loss 0.6490 - lr 0.1000\n",
      "2020-04-06 12:34:47,424 DEV : loss 0.6518052816390991 - score 0.6754\n",
      "2020-04-06 12:34:47,468 BAD EPOCHS (no improvement): 0\n",
      "2020-04-06 12:35:02,696 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-06 12:35:02,918 epoch 11 - iter 2/27 - loss 0.62047008 - samples/sec: 290.30\n",
      "2020-04-06 12:35:03,141 epoch 11 - iter 4/27 - loss 0.61898741 - samples/sec: 298.44\n",
      "2020-04-06 12:35:03,354 epoch 11 - iter 6/27 - loss 0.61590232 - samples/sec: 313.33\n",
      "2020-04-06 12:35:03,531 epoch 11 - iter 8/27 - loss 0.62535807 - samples/sec: 377.22\n",
      "2020-04-06 12:35:03,799 epoch 11 - iter 10/27 - loss 0.61596211 - samples/sec: 246.48\n",
      "2020-04-06 12:35:03,985 epoch 11 - iter 12/27 - loss 0.62243650 - samples/sec: 361.73\n",
      "2020-04-06 12:35:04,185 epoch 11 - iter 14/27 - loss 0.63764371 - samples/sec: 331.89\n",
      "2020-04-06 12:35:04,413 epoch 11 - iter 16/27 - loss 0.64058554 - samples/sec: 291.64\n",
      "2020-04-06 12:35:04,653 epoch 11 - iter 18/27 - loss 0.64153446 - samples/sec: 276.03\n",
      "2020-04-06 12:35:04,876 epoch 11 - iter 20/27 - loss 0.64020676 - samples/sec: 296.74\n",
      "2020-04-06 12:35:05,098 epoch 11 - iter 22/27 - loss 0.63717088 - samples/sec: 299.92\n",
      "2020-04-06 12:35:05,340 epoch 11 - iter 24/27 - loss 0.63128942 - samples/sec: 272.53\n",
      "2020-04-06 12:35:05,552 epoch 11 - iter 26/27 - loss 0.63220355 - samples/sec: 314.90\n",
      "2020-04-06 12:35:05,644 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-06 12:35:05,645 EPOCH 11 done: loss 0.6327 - lr 0.1000\n",
      "2020-04-06 12:35:06,488 DEV : loss 0.6563916802406311 - score 0.6319\n",
      "2020-04-06 12:35:06,532 BAD EPOCHS (no improvement): 1\n",
      "2020-04-06 12:35:06,533 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-06 12:35:06,756 epoch 12 - iter 2/27 - loss 0.55890128 - samples/sec: 289.23\n",
      "2020-04-06 12:35:06,951 epoch 12 - iter 4/27 - loss 0.58766900 - samples/sec: 342.55\n",
      "2020-04-06 12:35:07,146 epoch 12 - iter 6/27 - loss 0.61775935 - samples/sec: 341.26\n",
      "2020-04-06 12:35:07,342 epoch 12 - iter 8/27 - loss 0.62296600 - samples/sec: 341.53\n",
      "2020-04-06 12:35:07,532 epoch 12 - iter 10/27 - loss 0.62679439 - samples/sec: 353.76\n",
      "2020-04-06 12:35:07,702 epoch 12 - iter 12/27 - loss 0.62495578 - samples/sec: 394.48\n",
      "2020-04-06 12:35:07,912 epoch 12 - iter 14/27 - loss 0.62123679 - samples/sec: 317.56\n",
      "2020-04-06 12:35:08,159 epoch 12 - iter 16/27 - loss 0.62856696 - samples/sec: 268.48\n",
      "2020-04-06 12:35:08,382 epoch 12 - iter 18/27 - loss 0.63208055 - samples/sec: 298.92\n",
      "2020-04-06 12:35:08,629 epoch 12 - iter 20/27 - loss 0.63360739 - samples/sec: 268.65\n",
      "2020-04-06 12:35:08,891 epoch 12 - iter 22/27 - loss 0.63821634 - samples/sec: 252.72\n",
      "2020-04-06 12:35:09,101 epoch 12 - iter 24/27 - loss 0.63813288 - samples/sec: 318.15\n",
      "2020-04-06 12:35:09,324 epoch 12 - iter 26/27 - loss 0.64083275 - samples/sec: 297.96\n",
      "2020-04-06 12:35:09,458 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-06 12:35:09,460 EPOCH 12 done: loss 0.6398 - lr 0.1000\n",
      "2020-04-06 12:35:10,302 DEV : loss 0.64848393201828 - score 0.6986\n",
      "2020-04-06 12:35:10,346 BAD EPOCHS (no improvement): 0\n",
      "2020-04-06 12:35:25,557 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-06 12:35:25,759 epoch 13 - iter 2/27 - loss 0.62098637 - samples/sec: 319.91\n",
      "2020-04-06 12:35:25,984 epoch 13 - iter 4/27 - loss 0.64134152 - samples/sec: 320.41\n",
      "2020-04-06 12:35:26,214 epoch 13 - iter 6/27 - loss 0.63874962 - samples/sec: 288.53\n",
      "2020-04-06 12:35:26,417 epoch 13 - iter 8/27 - loss 0.65146078 - samples/sec: 327.70\n",
      "2020-04-06 12:35:26,627 epoch 13 - iter 10/27 - loss 0.65434904 - samples/sec: 318.40\n",
      "2020-04-06 12:35:26,889 epoch 13 - iter 12/27 - loss 0.64498860 - samples/sec: 252.25\n",
      "2020-04-06 12:35:27,108 epoch 13 - iter 14/27 - loss 0.64051676 - samples/sec: 303.20\n",
      "2020-04-06 12:35:27,302 epoch 13 - iter 16/27 - loss 0.65353341 - samples/sec: 344.63\n",
      "2020-04-06 12:35:27,531 epoch 13 - iter 18/27 - loss 0.64950570 - samples/sec: 289.63\n",
      "2020-04-06 12:35:27,728 epoch 13 - iter 20/27 - loss 0.64787061 - samples/sec: 338.35\n",
      "2020-04-06 12:35:27,975 epoch 13 - iter 22/27 - loss 0.64689760 - samples/sec: 267.84\n",
      "2020-04-06 12:35:28,167 epoch 13 - iter 24/27 - loss 0.64309368 - samples/sec: 349.95\n",
      "2020-04-06 12:35:28,386 epoch 13 - iter 26/27 - loss 0.64053165 - samples/sec: 302.14\n",
      "2020-04-06 12:35:28,515 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-06 12:35:28,516 EPOCH 13 done: loss 0.6390 - lr 0.1000\n",
      "2020-04-06 12:35:29,368 DEV : loss 0.635068953037262 - score 0.6725\n",
      "2020-04-06 12:35:29,411 BAD EPOCHS (no improvement): 1\n",
      "2020-04-06 12:35:29,412 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-06 12:35:29,633 epoch 14 - iter 2/27 - loss 0.61627108 - samples/sec: 291.16\n",
      "2020-04-06 12:35:29,854 epoch 14 - iter 4/27 - loss 0.61713670 - samples/sec: 302.16\n",
      "2020-04-06 12:35:30,071 epoch 14 - iter 6/27 - loss 0.61758882 - samples/sec: 307.50\n",
      "2020-04-06 12:35:30,306 epoch 14 - iter 8/27 - loss 0.62382770 - samples/sec: 281.72\n",
      "2020-04-06 12:35:30,523 epoch 14 - iter 10/27 - loss 0.63918539 - samples/sec: 305.96\n",
      "2020-04-06 12:35:30,728 epoch 14 - iter 12/27 - loss 0.64999060 - samples/sec: 325.16\n",
      "2020-04-06 12:35:30,974 epoch 14 - iter 14/27 - loss 0.64940909 - samples/sec: 269.56\n",
      "2020-04-06 12:35:31,166 epoch 14 - iter 16/27 - loss 0.64749259 - samples/sec: 348.20\n",
      "2020-04-06 12:35:31,378 epoch 14 - iter 18/27 - loss 0.64168233 - samples/sec: 312.94\n",
      "2020-04-06 12:35:31,596 epoch 14 - iter 20/27 - loss 0.63663063 - samples/sec: 304.67\n",
      "2020-04-06 12:35:31,815 epoch 14 - iter 22/27 - loss 0.63440844 - samples/sec: 303.24\n",
      "2020-04-06 12:35:32,060 epoch 14 - iter 24/27 - loss 0.63791769 - samples/sec: 269.88\n",
      "2020-04-06 12:35:32,277 epoch 14 - iter 26/27 - loss 0.63529849 - samples/sec: 305.94\n",
      "2020-04-06 12:35:32,376 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-06 12:35:32,376 EPOCH 14 done: loss 0.6335 - lr 0.1000\n",
      "2020-04-06 12:35:33,210 DEV : loss 0.6457505226135254 - score 0.6667\n",
      "2020-04-06 12:35:33,253 BAD EPOCHS (no improvement): 2\n",
      "2020-04-06 12:35:33,255 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-06 12:35:33,473 epoch 15 - iter 2/27 - loss 0.58477148 - samples/sec: 294.85\n",
      "2020-04-06 12:35:33,631 epoch 15 - iter 4/27 - loss 0.61270712 - samples/sec: 427.48\n",
      "2020-04-06 12:35:33,868 epoch 15 - iter 6/27 - loss 0.62300014 - samples/sec: 278.91\n",
      "2020-04-06 12:35:34,071 epoch 15 - iter 8/27 - loss 0.62488171 - samples/sec: 328.70\n",
      "2020-04-06 12:35:34,311 epoch 15 - iter 10/27 - loss 0.62658407 - samples/sec: 275.64\n",
      "2020-04-06 12:35:34,489 epoch 15 - iter 12/27 - loss 0.62374934 - samples/sec: 377.50\n",
      "2020-04-06 12:35:34,721 epoch 15 - iter 14/27 - loss 0.61584190 - samples/sec: 286.75\n",
      "2020-04-06 12:35:34,958 epoch 15 - iter 16/27 - loss 0.62410910 - samples/sec: 280.28\n",
      "2020-04-06 12:35:35,193 epoch 15 - iter 18/27 - loss 0.62317759 - samples/sec: 283.36\n",
      "2020-04-06 12:35:35,440 epoch 15 - iter 20/27 - loss 0.62235799 - samples/sec: 267.81\n",
      "2020-04-06 12:35:35,661 epoch 15 - iter 22/27 - loss 0.62318407 - samples/sec: 300.55\n",
      "2020-04-06 12:35:35,850 epoch 15 - iter 24/27 - loss 0.62062048 - samples/sec: 351.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-06 12:35:36,084 epoch 15 - iter 26/27 - loss 0.62285865 - samples/sec: 284.12\n",
      "2020-04-06 12:35:36,211 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-06 12:35:36,212 EPOCH 15 done: loss 0.6244 - lr 0.1000\n",
      "2020-04-06 12:35:37,074 DEV : loss 0.6487848162651062 - score 0.6232\n",
      "2020-04-06 12:35:37,120 BAD EPOCHS (no improvement): 3\n",
      "2020-04-06 12:35:37,121 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-06 12:35:37,338 epoch 16 - iter 2/27 - loss 0.62236482 - samples/sec: 297.75\n",
      "2020-04-06 12:35:37,537 epoch 16 - iter 4/27 - loss 0.61676866 - samples/sec: 335.52\n",
      "2020-04-06 12:35:37,755 epoch 16 - iter 6/27 - loss 0.65586740 - samples/sec: 304.20\n",
      "2020-04-06 12:35:37,974 epoch 16 - iter 8/27 - loss 0.65788950 - samples/sec: 303.05\n",
      "2020-04-06 12:35:38,144 epoch 16 - iter 10/27 - loss 0.65378333 - samples/sec: 394.20\n",
      "2020-04-06 12:35:38,386 epoch 16 - iter 12/27 - loss 0.64915132 - samples/sec: 272.84\n",
      "2020-04-06 12:35:38,628 epoch 16 - iter 14/27 - loss 0.64995037 - samples/sec: 275.02\n",
      "2020-04-06 12:35:38,836 epoch 16 - iter 16/27 - loss 0.64980919 - samples/sec: 320.91\n",
      "2020-04-06 12:35:39,067 epoch 16 - iter 18/27 - loss 0.64431218 - samples/sec: 288.16\n",
      "2020-04-06 12:35:39,324 epoch 16 - iter 20/27 - loss 0.64584539 - samples/sec: 257.94\n",
      "2020-04-06 12:35:39,560 epoch 16 - iter 22/27 - loss 0.65014927 - samples/sec: 280.03\n",
      "2020-04-06 12:35:39,774 epoch 16 - iter 24/27 - loss 0.65582466 - samples/sec: 311.11\n",
      "2020-04-06 12:35:39,990 epoch 16 - iter 26/27 - loss 0.65215531 - samples/sec: 307.57\n",
      "2020-04-06 12:35:40,105 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-06 12:35:40,106 EPOCH 16 done: loss 0.6459 - lr 0.1000\n",
      "2020-04-06 12:35:40,969 DEV : loss 0.6339343190193176 - score 0.6812\n",
      "Epoch    16: reducing learning rate of group 0 to 5.0000e-02.\n",
      "2020-04-06 12:35:41,013 BAD EPOCHS (no improvement): 4\n",
      "2020-04-06 12:35:41,014 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-06 12:35:41,260 epoch 17 - iter 2/27 - loss 0.73871705 - samples/sec: 262.72\n",
      "2020-04-06 12:35:41,469 epoch 17 - iter 4/27 - loss 0.66929662 - samples/sec: 318.26\n",
      "2020-04-06 12:35:41,638 epoch 17 - iter 6/27 - loss 0.65984063 - samples/sec: 397.21\n",
      "2020-04-06 12:35:41,889 epoch 17 - iter 8/27 - loss 0.63586926 - samples/sec: 264.12\n",
      "2020-04-06 12:35:42,109 epoch 17 - iter 10/27 - loss 0.63763840 - samples/sec: 301.12\n",
      "2020-04-06 12:35:42,359 epoch 17 - iter 12/27 - loss 0.63059348 - samples/sec: 264.90\n",
      "2020-04-06 12:35:42,570 epoch 17 - iter 14/27 - loss 0.62778698 - samples/sec: 314.51\n",
      "2020-04-06 12:35:42,823 epoch 17 - iter 16/27 - loss 0.63101992 - samples/sec: 261.35\n",
      "2020-04-06 12:35:43,015 epoch 17 - iter 18/27 - loss 0.63474166 - samples/sec: 347.90\n",
      "2020-04-06 12:35:43,231 epoch 17 - iter 20/27 - loss 0.63257761 - samples/sec: 307.59\n",
      "2020-04-06 12:35:43,421 epoch 17 - iter 22/27 - loss 0.62746604 - samples/sec: 352.30\n",
      "2020-04-06 12:35:43,665 epoch 17 - iter 24/27 - loss 0.62484136 - samples/sec: 270.42\n",
      "2020-04-06 12:35:43,884 epoch 17 - iter 26/27 - loss 0.62180233 - samples/sec: 303.41\n",
      "2020-04-06 12:35:43,989 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-06 12:35:43,990 EPOCH 17 done: loss 0.6180 - lr 0.0500\n",
      "2020-04-06 12:35:44,856 DEV : loss 0.6278334856033325 - score 0.6928\n",
      "2020-04-06 12:35:44,901 BAD EPOCHS (no improvement): 1\n",
      "2020-04-06 12:35:44,903 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-06 12:35:45,120 epoch 18 - iter 2/27 - loss 0.66499987 - samples/sec: 296.17\n",
      "2020-04-06 12:35:45,335 epoch 18 - iter 4/27 - loss 0.64112379 - samples/sec: 309.47\n",
      "2020-04-06 12:35:45,548 epoch 18 - iter 6/27 - loss 0.62968600 - samples/sec: 312.93\n",
      "2020-04-06 12:35:45,734 epoch 18 - iter 8/27 - loss 0.62604569 - samples/sec: 358.88\n",
      "2020-04-06 12:35:45,952 epoch 18 - iter 10/27 - loss 0.61590066 - samples/sec: 303.80\n",
      "2020-04-06 12:35:46,183 epoch 18 - iter 12/27 - loss 0.61478405 - samples/sec: 287.37\n",
      "2020-04-06 12:35:46,418 epoch 18 - iter 14/27 - loss 0.61511006 - samples/sec: 281.13\n",
      "2020-04-06 12:35:46,643 epoch 18 - iter 16/27 - loss 0.61053191 - samples/sec: 296.31\n",
      "2020-04-06 12:35:46,860 epoch 18 - iter 18/27 - loss 0.61848618 - samples/sec: 306.78\n",
      "2020-04-06 12:35:47,064 epoch 18 - iter 20/27 - loss 0.61725543 - samples/sec: 328.65\n",
      "2020-04-06 12:35:47,240 epoch 18 - iter 22/27 - loss 0.61840422 - samples/sec: 383.57\n",
      "2020-04-06 12:35:47,492 epoch 18 - iter 24/27 - loss 0.61910312 - samples/sec: 261.94\n",
      "2020-04-06 12:35:47,710 epoch 18 - iter 26/27 - loss 0.61811107 - samples/sec: 306.34\n",
      "2020-04-06 12:35:47,854 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-06 12:35:47,855 EPOCH 18 done: loss 0.6185 - lr 0.0500\n",
      "2020-04-06 12:35:48,679 DEV : loss 0.6229228973388672 - score 0.687\n",
      "2020-04-06 12:35:48,723 BAD EPOCHS (no improvement): 2\n",
      "2020-04-06 12:35:48,724 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-06 12:35:48,919 epoch 19 - iter 2/27 - loss 0.61598387 - samples/sec: 331.35\n",
      "2020-04-06 12:35:49,098 epoch 19 - iter 4/27 - loss 0.61298169 - samples/sec: 375.32\n",
      "2020-04-06 12:35:49,312 epoch 19 - iter 6/27 - loss 0.59045742 - samples/sec: 310.85\n",
      "2020-04-06 12:35:49,532 epoch 19 - iter 8/27 - loss 0.59090588 - samples/sec: 302.28\n",
      "2020-04-06 12:35:49,765 epoch 19 - iter 10/27 - loss 0.59916150 - samples/sec: 285.98\n",
      "2020-04-06 12:35:49,956 epoch 19 - iter 12/27 - loss 0.61201665 - samples/sec: 349.44\n",
      "2020-04-06 12:35:50,165 epoch 19 - iter 14/27 - loss 0.61772362 - samples/sec: 320.57\n",
      "2020-04-06 12:35:50,372 epoch 19 - iter 16/27 - loss 0.61698145 - samples/sec: 320.44\n",
      "2020-04-06 12:35:50,623 epoch 19 - iter 18/27 - loss 0.61826575 - samples/sec: 265.14\n",
      "2020-04-06 12:35:50,851 epoch 19 - iter 20/27 - loss 0.61605728 - samples/sec: 289.94\n",
      "2020-04-06 12:35:51,113 epoch 19 - iter 22/27 - loss 0.61634364 - samples/sec: 252.48\n",
      "2020-04-06 12:35:51,338 epoch 19 - iter 24/27 - loss 0.61119666 - samples/sec: 295.15\n",
      "2020-04-06 12:35:51,538 epoch 19 - iter 26/27 - loss 0.60945969 - samples/sec: 331.14\n",
      "2020-04-06 12:35:51,659 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-06 12:35:51,659 EPOCH 19 done: loss 0.6090 - lr 0.0500\n",
      "2020-04-06 12:35:52,477 DEV : loss 0.6274125576019287 - score 0.658\n",
      "2020-04-06 12:35:52,522 BAD EPOCHS (no improvement): 3\n",
      "2020-04-06 12:35:52,523 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-06 12:35:52,748 epoch 20 - iter 2/27 - loss 0.59346017 - samples/sec: 286.21\n",
      "2020-04-06 12:35:52,995 epoch 20 - iter 4/27 - loss 0.60058740 - samples/sec: 267.20\n",
      "2020-04-06 12:35:53,232 epoch 20 - iter 6/27 - loss 0.59717042 - samples/sec: 280.66\n",
      "2020-04-06 12:35:53,458 epoch 20 - iter 8/27 - loss 0.58528927 - samples/sec: 294.70\n",
      "2020-04-06 12:35:53,654 epoch 20 - iter 10/27 - loss 0.59776629 - samples/sec: 341.88\n",
      "2020-04-06 12:35:53,837 epoch 20 - iter 12/27 - loss 0.61229236 - samples/sec: 366.84\n",
      "2020-04-06 12:35:54,073 epoch 20 - iter 14/27 - loss 0.61133257 - samples/sec: 280.87\n",
      "2020-04-06 12:35:54,272 epoch 20 - iter 16/27 - loss 0.60961802 - samples/sec: 335.10\n",
      "2020-04-06 12:35:54,516 epoch 20 - iter 18/27 - loss 0.61986870 - samples/sec: 271.48\n",
      "2020-04-06 12:35:54,726 epoch 20 - iter 20/27 - loss 0.62075674 - samples/sec: 318.29\n",
      "2020-04-06 12:35:54,944 epoch 20 - iter 22/27 - loss 0.61537952 - samples/sec: 304.36\n",
      "2020-04-06 12:35:55,107 epoch 20 - iter 24/27 - loss 0.61769408 - samples/sec: 411.30\n",
      "2020-04-06 12:35:55,358 epoch 20 - iter 26/27 - loss 0.61662981 - samples/sec: 263.27\n",
      "2020-04-06 12:35:55,455 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-06 12:35:55,456 EPOCH 20 done: loss 0.6159 - lr 0.0500\n",
      "2020-04-06 12:35:56,276 DEV : loss 0.6126670837402344 - score 0.7159\n",
      "2020-04-06 12:35:56,319 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-06 12:36:26,396 ----------------------------------------------------------------------------------------------------\n",
      "2020-04-06 12:36:26,397 Testing using best model ...\n",
      "2020-04-06 12:36:26,398 loading file best-model.pt\n",
      "2020-04-06 12:36:30,398 0.6537\t0.6537\t0.6537\n",
      "2020-04-06 12:36:30,399 \n",
      "MICRO_AVG: acc 0.4855 - f1-score 0.6537\n",
      "MACRO_AVG: acc 0.4851 - f1-score 0.6531499999999999\n",
      "disabled   tp: 71 - fp: 47 - fn: 33 - tn: 80 - precision: 0.6017 - recall: 0.6827 - accuracy: 0.4702 - f1-score: 0.6396\n",
      "published  tp: 80 - fp: 33 - fn: 47 - tn: 71 - precision: 0.7080 - recall: 0.6299 - accuracy: 0.5000 - f1-score: 0.6667\n",
      "2020-04-06 12:36:30,399 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_score': 0.6537,\n",
       " 'dev_score_history': [0.6029,\n",
       "  0.6348,\n",
       "  0.6145,\n",
       "  0.5478,\n",
       "  0.6638,\n",
       "  0.6551,\n",
       "  0.6725,\n",
       "  0.6638,\n",
       "  0.6696,\n",
       "  0.6754,\n",
       "  0.6319,\n",
       "  0.6986,\n",
       "  0.6725,\n",
       "  0.6667,\n",
       "  0.6232,\n",
       "  0.6812,\n",
       "  0.6928,\n",
       "  0.687,\n",
       "  0.658,\n",
       "  0.7159],\n",
       " 'train_loss_history': [0.711567136976454,\n",
       "  0.7023470710825037,\n",
       "  0.6828084786732992,\n",
       "  0.6863388772364016,\n",
       "  0.6666182544496324,\n",
       "  0.6654118961758084,\n",
       "  0.6695940538688943,\n",
       "  0.6611989913163362,\n",
       "  0.6567812650292008,\n",
       "  0.6489906333110951,\n",
       "  0.6326575323387429,\n",
       "  0.6398089947523894,\n",
       "  0.6390212575594584,\n",
       "  0.6334975207293475,\n",
       "  0.6243826393727903,\n",
       "  0.6459086482171659,\n",
       "  0.6180060196805883,\n",
       "  0.6185051004091898,\n",
       "  0.6089737724374842,\n",
       "  0.6159107243573224],\n",
       " 'dev_loss_history': [tensor(0.6664, device='cuda:0'),\n",
       "  tensor(0.6800, device='cuda:0'),\n",
       "  tensor(0.6675, device='cuda:0'),\n",
       "  tensor(0.6816, device='cuda:0'),\n",
       "  tensor(0.6610, device='cuda:0'),\n",
       "  tensor(0.6535, device='cuda:0'),\n",
       "  tensor(0.6652, device='cuda:0'),\n",
       "  tensor(0.6579, device='cuda:0'),\n",
       "  tensor(0.6521, device='cuda:0'),\n",
       "  tensor(0.6518, device='cuda:0'),\n",
       "  tensor(0.6564, device='cuda:0'),\n",
       "  tensor(0.6485, device='cuda:0'),\n",
       "  tensor(0.6351, device='cuda:0'),\n",
       "  tensor(0.6458, device='cuda:0'),\n",
       "  tensor(0.6488, device='cuda:0'),\n",
       "  tensor(0.6339, device='cuda:0'),\n",
       "  tensor(0.6278, device='cuda:0'),\n",
       "  tensor(0.6229, device='cuda:0'),\n",
       "  tensor(0.6274, device='cuda:0'),\n",
       "  tensor(0.6127, device='cuda:0')]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train('./', max_epochs=20, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6935, -0.8708]], device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier([Sentence(\"this is a sentence\")])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "from flair.training_utils import store_embeddings\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "torch.from_numpy(train_df[:10][['published', 'disabled']].values).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0, train loss 0.06326187402009964\n",
      "Epoch 0, Batch 10, train loss 0.06166648492217064\n",
      "Epoch 0, Batch 20, train loss 0.0313153900206089\n",
      "Epoch 0, Batch 30, train loss 0.03767106682062149\n",
      "Epoch 0, Batch 40, train loss 0.04703482985496521\n",
      "Epoch 0, Batch 50, train loss 0.0449230782687664\n",
      "> Epoch 0, train loss 0.039944981759483064\n",
      "> Epoch 0, val loss 0.018333641817604288\n",
      "Saved model.\n",
      "Epoch 1, Batch 0, train loss 0.03845386579632759\n",
      "Epoch 1, Batch 10, train loss 0.020252227783203125\n",
      "Epoch 1, Batch 20, train loss 0.012711026705801487\n",
      "Epoch 1, Batch 30, train loss 0.019124727696180344\n",
      "Epoch 1, Batch 40, train loss 0.056158095598220825\n",
      "Epoch 1, Batch 50, train loss 0.024085968732833862\n",
      "> Epoch 1, train loss 0.024453740704942634\n",
      "> Epoch 1, val loss 0.014631336430708568\n",
      "Saved model.\n",
      "Epoch 2, Batch 0, train loss 0.020150186493992805\n",
      "Epoch 2, Batch 10, train loss 0.013921372592449188\n",
      "Epoch 2, Batch 20, train loss 0.023955384269356728\n",
      "Epoch 2, Batch 30, train loss 0.017117992043495178\n",
      "Epoch 2, Batch 40, train loss 0.015482325106859207\n",
      "Epoch 2, Batch 50, train loss 0.0137685751542449\n",
      "> Epoch 2, train loss 0.017077265699669993\n",
      "> Epoch 2, val loss 0.009112788855597593\n",
      "Saved model.\n",
      "Epoch 3, Batch 0, train loss 0.021692529320716858\n",
      "Epoch 3, Batch 10, train loss 0.006681547500193119\n",
      "Epoch 3, Batch 20, train loss 0.015180269256234169\n",
      "Epoch 3, Batch 30, train loss 0.016718605533242226\n",
      "Epoch 3, Batch 40, train loss 0.011698611080646515\n",
      "Epoch 3, Batch 50, train loss 0.006583078298717737\n",
      "> Epoch 3, train loss 0.018396218910951306\n",
      "> Epoch 3, val loss 0.008723901330992795\n",
      "Saved model.\n",
      "Epoch 4, Batch 0, train loss 0.02802760899066925\n",
      "Epoch 4, Batch 10, train loss 0.015200423076748848\n",
      "Epoch 4, Batch 20, train loss 0.004815779160708189\n",
      "Epoch 4, Batch 30, train loss 0.03193814679980278\n",
      "Epoch 4, Batch 40, train loss 0.00959867238998413\n",
      "Epoch 4, Batch 50, train loss 0.022029057145118713\n",
      "> Epoch 4, train loss 0.016089355085838447\n",
      "> Epoch 4, val loss 0.00996187984727431\n",
      "No improvement.\n"
     ]
    }
   ],
   "source": [
    "def get_batches(df, batch_size=16):\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    n_batches = len(df)//batch_size    \n",
    "    for i in range(0, len(df), batch_size):\n",
    "        x = []        \n",
    "        for txt in df[i:i+batch_size]['image_concept']:\n",
    "            words = txt.split()            \n",
    "            random.shuffle(words)\n",
    "            txt = ' '.join(words)\n",
    "            x.append(Sentence(txt))\n",
    "        \n",
    "        # disabled 0, published 1\n",
    "        y = [1 if is_published else 0 for is_published in df[i:i+batch_size]['published']]\n",
    "        yield x, torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "\n",
    "def train_model(model, epochs, lr, train_df, val_df, checkpoint_file, early_stopping=5):        \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    best_loss = np.inf\n",
    "    no_improvement = 0\n",
    "\n",
    "    if train_on_gpu:\n",
    "        model = model.cuda()\n",
    "        \n",
    "    for epoch in range(epochs):        \n",
    "        total_train_loss = 0\n",
    "        total_val_loss = 0\n",
    "        train_loss = 0\n",
    "        val_loss = 0\n",
    "        \n",
    "        # Train\n",
    "        model.train()        \n",
    "        for i, (sentences, labels) in enumerate(get_batches(train_df)):         \n",
    "            if train_on_gpu:\n",
    "                labels = labels.cuda()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            out = model(sentences)            \n",
    "            \n",
    "            loss = criterion(out, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            store_embeddings(sentences, 'cpu')\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Batch {i}, train loss {loss.item()/labels.size(0)}\")\n",
    "            \n",
    "            \n",
    "        train_loss = total_train_loss/len(train_df)\n",
    "        print(f\"> Epoch {epoch}, train loss {train_loss}\")\n",
    "        \n",
    "        # Eval\n",
    "        model.eval()\n",
    "        for sentences, labels in get_batches(val_df):\n",
    "            if train_on_gpu:\n",
    "                labels = labels.cuda()\n",
    "            \n",
    "            out = model(sentences)\n",
    "            loss = criterion(out, labels)\n",
    "            total_val_loss += loss.item()\n",
    "            \n",
    "            store_embeddings(sentences, 'cpu')\n",
    "            \n",
    "            \n",
    "        val_loss = total_val_loss / len(val_df)\n",
    "        \n",
    "        print(f\"> Epoch {epoch}, val loss {val_loss}\")\n",
    "        \n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            no_improvement = 0\n",
    "            torch.save(model.state_dict(), checkpoint_file)\n",
    "            print(\"Saved model.\")\n",
    "        else:\n",
    "            no_improvement += 1\n",
    "            print(\"No improvement.\")\n",
    "            if no_improvement >= early_stopping:\n",
    "                print(f\"Early Stopping\")\n",
    "                break\n",
    "            \n",
    "                                              \n",
    "checkpoint_file = 'flair_text_model_2.pt'      \n",
    "lr = 0.005\n",
    "epochs = 5            \n",
    "\n",
    "train_model(classifier, epochs, lr, train_df, validation_df, checkpoint_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216/231 correct. Accuracy: 93.50649350649351 %\n"
     ]
    }
   ],
   "source": [
    "def eval_model(model, test_df):\n",
    "    if train_on_gpu:\n",
    "        model = model.cuda()\n",
    "        \n",
    "    model.eval()\n",
    "    num_correct = 0\n",
    "    total = 0\n",
    "    for i, (sentences, labels) in enumerate(get_batches(test_df)):\n",
    "        if train_on_gpu:\n",
    "            labels = labels.cuda()\n",
    "            \n",
    "        out = model(sentences)\n",
    "        _, pred = torch.max(out, 1)\n",
    "                \n",
    "        correct = (labels == pred)\n",
    "        correct = correct.cpu().numpy() if train_on_gpu else correct.numpy()\n",
    "        \n",
    "        num_correct += np.sum(correct)\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        store_embeddings(sentences, 'cpu')\n",
    "        \n",
    "\n",
    "    print(f\"{num_correct}/{total} correct. Accuracy: {num_correct*100/total} %\")\n",
    "    \n",
    "    \n",
    "eval_model(classifier, test_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:efficientnet]",
   "language": "python",
   "name": "conda-env-efficientnet-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
