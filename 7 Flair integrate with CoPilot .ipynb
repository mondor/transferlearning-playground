{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3 = boto3.client('s3')\n",
    "s3.download_file('sagemaker-us-east-1-953585160895',\n",
    "                 'data/5eaec695e1970/preprocess_data.txt', \n",
    "                 'preprocess_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>message</th>\n",
       "      <th>image_concept</th>\n",
       "      <th>image</th>\n",
       "      <th>published</th>\n",
       "      <th>disabled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5e55dca437fa5927dcdf02f3</td>\n",
       "      <td>en route to gbr embrace the elevation in luxur...</td>\n",
       "      <td>nature travel diving water sea underwater ocea...</td>\n",
       "      <td>https://scontent.xx.fbcdn.net/v/t51.2885-15/81...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5e55d69eb9e5b725cd7ba02f</td>\n",
       "      <td>golf course views ‚õ≥ #hamiltonislandgolfcourse ...</td>\n",
       "      <td>outdoors landscape beach sky nature rural no p...</td>\n",
       "      <td>https://scontent.xx.fbcdn.net/v/t51.2885-15/87...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5e55ca8d0f1aeb23b862f240</td>\n",
       "      <td>hamo family vay kay #hamiltonisland</td>\n",
       "      <td>boat water sunglasses leisure recreation one s...</td>\n",
       "      <td>https://scontent.xx.fbcdn.net/v/t51.2885-15/87...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5e55a83c75d14b7257d1aceb</td>\n",
       "      <td>news: entries booming for hamilton island race...</td>\n",
       "      <td>audience sports fan marathon people crowd grou...</td>\n",
       "      <td>https://uploads-cdn.stackla.com/10/hamiltonisl...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5e55a3e657f15e175b8fa58f</td>\n",
       "      <td>my series on the great barrier reef and surrou...</td>\n",
       "      <td>vintage texture no person abstract desktop nat...</td>\n",
       "      <td>https://scontent.xx.fbcdn.net/v/t51.2885-15/87...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>5e47bb6b1096abb55d1873d3</td>\n",
       "      <td>what a weekend! this place is amazing üòç #hamil...</td>\n",
       "      <td>person human vehicle transportation golf cart</td>\n",
       "      <td>https://scontent.xx.fbcdn.net/v/t51.2885-15/85...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>5e47bb6b1096abb55d1873dc</td>\n",
       "      <td>what a weekend! this place is amazing üòç #hamil...</td>\n",
       "      <td>land nature outdoors shoreline water sea ocean...</td>\n",
       "      <td>https://scontent.xx.fbcdn.net/v/t51.2885-15/84...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>5e47bb691096abb55d1873cb</td>\n",
       "      <td>this time last year, chilling at the whitsunda...</td>\n",
       "      <td>land nature outdoors water sea ocean shoreline...</td>\n",
       "      <td>https://scontent.xx.fbcdn.net/v/t51.2885-15/84...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>5e47b9e37f9fcbb333a170ab</td>\n",
       "      <td>back when i was live living the hamilton islan...</td>\n",
       "      <td>human person clothing apparel food pork</td>\n",
       "      <td>https://scontent.xx.fbcdn.net/v/t51.2885-15/80...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>5e47b53aba065fb3327adb82</td>\n",
       "      <td>#hamiltonisland #qualia #pebblebeachqualia #ba...</td>\n",
       "      <td>burger food bun bread</td>\n",
       "      <td>https://scontent.xx.fbcdn.net/v/t51.2885-15/84...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           _id  \\\n",
       "0     5e55dca437fa5927dcdf02f3   \n",
       "1     5e55d69eb9e5b725cd7ba02f   \n",
       "2     5e55ca8d0f1aeb23b862f240   \n",
       "3     5e55a83c75d14b7257d1aceb   \n",
       "4     5e55a3e657f15e175b8fa58f   \n",
       "...                        ...   \n",
       "1995  5e47bb6b1096abb55d1873d3   \n",
       "1996  5e47bb6b1096abb55d1873dc   \n",
       "1997  5e47bb691096abb55d1873cb   \n",
       "1998  5e47b9e37f9fcbb333a170ab   \n",
       "1999  5e47b53aba065fb3327adb82   \n",
       "\n",
       "                                                message  \\\n",
       "0     en route to gbr embrace the elevation in luxur...   \n",
       "1     golf course views ‚õ≥ #hamiltonislandgolfcourse ...   \n",
       "2                   hamo family vay kay #hamiltonisland   \n",
       "3     news: entries booming for hamilton island race...   \n",
       "4     my series on the great barrier reef and surrou...   \n",
       "...                                                 ...   \n",
       "1995  what a weekend! this place is amazing üòç #hamil...   \n",
       "1996  what a weekend! this place is amazing üòç #hamil...   \n",
       "1997  this time last year, chilling at the whitsunda...   \n",
       "1998  back when i was live living the hamilton islan...   \n",
       "1999  #hamiltonisland #qualia #pebblebeachqualia #ba...   \n",
       "\n",
       "                                          image_concept  \\\n",
       "0     nature travel diving water sea underwater ocea...   \n",
       "1     outdoors landscape beach sky nature rural no p...   \n",
       "2     boat water sunglasses leisure recreation one s...   \n",
       "3     audience sports fan marathon people crowd grou...   \n",
       "4     vintage texture no person abstract desktop nat...   \n",
       "...                                                 ...   \n",
       "1995      person human vehicle transportation golf cart   \n",
       "1996  land nature outdoors shoreline water sea ocean...   \n",
       "1997  land nature outdoors water sea ocean shoreline...   \n",
       "1998            human person clothing apparel food pork   \n",
       "1999                              burger food bun bread   \n",
       "\n",
       "                                                  image  published  disabled  \n",
       "0     https://scontent.xx.fbcdn.net/v/t51.2885-15/81...          1         0  \n",
       "1     https://scontent.xx.fbcdn.net/v/t51.2885-15/87...          1         0  \n",
       "2     https://scontent.xx.fbcdn.net/v/t51.2885-15/87...          1         0  \n",
       "3     https://uploads-cdn.stackla.com/10/hamiltonisl...          1         0  \n",
       "4     https://scontent.xx.fbcdn.net/v/t51.2885-15/87...          1         0  \n",
       "...                                                 ...        ...       ...  \n",
       "1995  https://scontent.xx.fbcdn.net/v/t51.2885-15/85...          0         1  \n",
       "1996  https://scontent.xx.fbcdn.net/v/t51.2885-15/84...          0         1  \n",
       "1997  https://scontent.xx.fbcdn.net/v/t51.2885-15/84...          0         1  \n",
       "1998  https://scontent.xx.fbcdn.net/v/t51.2885-15/80...          0         1  \n",
       "1999  https://scontent.xx.fbcdn.net/v/t51.2885-15/84...          0         1  \n",
       "\n",
       "[2000 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "column_names = ['_id', 'message', 'image_concept', 'image', 'published', 'disabled']\n",
    "target_names = ['published', 'disabled']\n",
    "\n",
    "\n",
    "df = pd.read_csv('preprocess_data.txt', names=column_names, engine='python')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "published 500 disabled 1500\n"
     ]
    }
   ],
   "source": [
    "published_count = len(df.loc[df['published'] == 1])\n",
    "disabled_count = len(df.loc[df['disabled'] == 1])\n",
    "print(f\"published {published_count} disabled {disabled_count}\") \n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class: (published), train: 342, val: 156\n",
      "class: (disabled), train: 1050, val: 441\n",
      "TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('twitter')\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=100, out_features=128, bias=True)\n",
      "    (rnn): GRU(128, 128, batch_first=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (loss_function): BCEWithLogitsLoss()\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\n",
      "Epoch 0, batch 0, train loss 0.04097532480955124\n",
      "Epoch 0, batch 10, train loss 0.0422392338514328\n",
      "Epoch 0, batch 20, train loss 0.05442492663860321\n",
      "Epoch 0, batch 30, train loss 0.04002945125102997\n",
      "Epoch 0, batch 40, train loss 0.048217594623565674\n",
      "Epoch 0, batch 50, train loss 0.03839818015694618\n",
      "Epoch 0, batch 60, train loss 0.034306056797504425\n",
      "Epoch 0, batch 70, train loss 0.03502165898680687\n",
      "Epoch 0, batch 80, train loss 0.04372956603765488\n",
      "Epoch 0, batch 90, train loss 0.04020122066140175\n",
      "Epoch 0, batch 100, train loss 0.033038586378097534\n",
      "Epoch 0, batch 110, train loss 0.04072318971157074\n",
      "Epoch 0, batch 120, train loss 0.04036008194088936\n",
      "Epoch 0, train loss 0.06131038812642125, val loss 0.03699866720979138, accuracy 0.7093802345058626, f1 0.7093800306530073\n",
      "Save model at Epoch 0, train loss 0.06131038812642125, val loss 0.03699866720979138, accuracy 0.7093802345058626, f1 0.7093800306530073\n",
      "Epoch 1, batch 0, train loss 0.03160782530903816\n",
      "Epoch 1, batch 10, train loss 0.04628296196460724\n",
      "Epoch 1, batch 20, train loss 0.03552500158548355\n",
      "Epoch 1, batch 30, train loss 0.035152800381183624\n",
      "Epoch 1, batch 40, train loss 0.0371832549571991\n",
      "Epoch 1, batch 50, train loss 0.03586234897375107\n",
      "Epoch 1, batch 60, train loss 0.03541409224271774\n",
      "Epoch 1, batch 70, train loss 0.040703993290662766\n",
      "Epoch 1, batch 80, train loss 0.032704271376132965\n",
      "Epoch 1, batch 90, train loss 0.04641910642385483\n",
      "Epoch 1, batch 100, train loss 0.04400959610939026\n",
      "Epoch 1, batch 110, train loss 0.03621300309896469\n",
      "Epoch 1, batch 120, train loss 0.042540766298770905\n",
      "Epoch 1, train loss 0.058880465469141115, val loss 0.034913864267531354, accuracy 0.7278056951423786, f1 0.7277963393324994\n",
      "Save model at Epoch 1, train loss 0.058880465469141115, val loss 0.034913864267531354, accuracy 0.7278056951423786, f1 0.7277963393324994\n",
      "Epoch 2, batch 0, train loss 0.03542190417647362\n",
      "Epoch 2, batch 10, train loss 0.042468562722206116\n",
      "Epoch 2, batch 20, train loss 0.03309528902173042\n",
      "Epoch 2, batch 30, train loss 0.0428985133767128\n",
      "Epoch 2, batch 40, train loss 0.04544972628355026\n",
      "Epoch 2, batch 50, train loss 0.034083619713783264\n",
      "Epoch 2, batch 60, train loss 0.04586426541209221\n",
      "Epoch 2, batch 70, train loss 0.04227752983570099\n",
      "Epoch 2, batch 80, train loss 0.029575709253549576\n",
      "Epoch 2, batch 90, train loss 0.030846701934933662\n",
      "Epoch 2, batch 100, train loss 0.031174499541521072\n",
      "Epoch 2, batch 110, train loss 0.021982533857226372\n",
      "Epoch 2, batch 120, train loss 0.039646685123443604\n",
      "Epoch 2, train loss 0.05498041135483775, val loss 0.035861877540447805, accuracy 0.7035175879396985, f1 0.7035142604621147\n",
      "No improvement.\n",
      "Epoch 3, batch 0, train loss 0.03176919370889664\n",
      "Epoch 3, batch 10, train loss 0.03704889118671417\n",
      "Epoch 3, batch 20, train loss 0.030356332659721375\n",
      "Epoch 3, batch 30, train loss 0.041258275508880615\n",
      "Epoch 3, batch 40, train loss 0.03409835696220398\n",
      "Epoch 3, batch 50, train loss 0.02786373160779476\n",
      "Epoch 3, batch 60, train loss 0.036069490015506744\n",
      "Epoch 3, batch 70, train loss 0.03232992812991142\n",
      "Epoch 3, batch 80, train loss 0.03535741940140724\n",
      "Epoch 3, batch 90, train loss 0.04070416837930679\n",
      "Epoch 3, batch 100, train loss 0.03884519636631012\n",
      "Epoch 3, batch 110, train loss 0.02725798636674881\n",
      "Epoch 3, batch 120, train loss 0.021216606721282005\n",
      "Epoch 3, train loss 0.0528241963521845, val loss 0.038391740702504486, accuracy 0.6959798994974874, f1 0.6959694497775303\n",
      "No improvement.\n",
      "Epoch 4, batch 0, train loss 0.028657780960202217\n",
      "Epoch 4, batch 10, train loss 0.04219971224665642\n",
      "Epoch 4, batch 20, train loss 0.04718799144029617\n",
      "Epoch 4, batch 30, train loss 0.03652822971343994\n",
      "Epoch 4, batch 40, train loss 0.03566905856132507\n",
      "Epoch 4, batch 50, train loss 0.052914656698703766\n",
      "Epoch 4, batch 60, train loss 0.03708624094724655\n",
      "Epoch 4, batch 70, train loss 0.04049792140722275\n",
      "Epoch 4, batch 80, train loss 0.029903355985879898\n",
      "Epoch 4, batch 90, train loss 0.022835319861769676\n",
      "Epoch 4, batch 100, train loss 0.038593560457229614\n",
      "Epoch 4, batch 110, train loss 0.04571518301963806\n",
      "Epoch 4, batch 120, train loss 0.029607698321342468\n",
      "Epoch 4, train loss 0.05261861554336274, val loss 0.03300137540802884, accuracy 0.7345058626465661, f1 0.734505676417877\n",
      "Save model at Epoch 4, train loss 0.05261861554336274, val loss 0.03300137540802884, accuracy 0.7345058626465661, f1 0.734505676417877\n",
      "Epoch 5, batch 0, train loss 0.044058628380298615\n",
      "Epoch 5, batch 10, train loss 0.022624727338552475\n",
      "Epoch 5, batch 20, train loss 0.03660457581281662\n",
      "Epoch 5, batch 30, train loss 0.02970070019364357\n",
      "Epoch 5, batch 40, train loss 0.027745865285396576\n",
      "Epoch 5, batch 50, train loss 0.02410329133272171\n",
      "Epoch 5, batch 60, train loss 0.027180779725313187\n",
      "Epoch 5, batch 70, train loss 0.024401340633630753\n",
      "Epoch 5, batch 80, train loss 0.03476989269256592\n",
      "Epoch 5, batch 90, train loss 0.03222997114062309\n",
      "Epoch 5, batch 100, train loss 0.027504533529281616\n",
      "Epoch 5, batch 110, train loss 0.020982220768928528\n",
      "Epoch 5, batch 120, train loss 0.04354766383767128\n",
      "Epoch 5, train loss 0.05158385865647217, val loss 0.038482018551435106, accuracy 0.7110552763819096, f1 0.7110550737039987\n",
      "No improvement.\n",
      "Epoch 6, batch 0, train loss 0.030905798077583313\n",
      "Epoch 6, batch 10, train loss 0.030362941324710846\n",
      "Epoch 6, batch 20, train loss 0.046460673213005066\n",
      "Epoch 6, batch 30, train loss 0.05702544003725052\n",
      "Epoch 6, batch 40, train loss 0.03837190195918083\n",
      "Epoch 6, batch 50, train loss 0.03411400690674782\n",
      "Epoch 6, batch 60, train loss 0.03505715727806091\n",
      "Epoch 6, batch 70, train loss 0.02221021056175232\n",
      "Epoch 6, batch 80, train loss 0.029698602855205536\n",
      "Epoch 6, batch 90, train loss 0.031475041061639786\n",
      "Epoch 6, batch 100, train loss 0.029490847140550613\n",
      "Epoch 6, batch 110, train loss 0.029770908877253532\n",
      "Epoch 6, batch 120, train loss 0.03542028367519379\n",
      "Epoch 6, train loss 0.049560800141216006, val loss 0.03794657674866106, accuracy 0.7001675041876047, f1 0.7001372157823403\n",
      "No improvement.\n",
      "Epoch 7, batch 0, train loss 0.02969365566968918\n",
      "Epoch 7, batch 10, train loss 0.028010699898004532\n",
      "Epoch 7, batch 20, train loss 0.04422200843691826\n",
      "Epoch 7, batch 30, train loss 0.033442530781030655\n",
      "Epoch 7, batch 40, train loss 0.03647078573703766\n",
      "Epoch 7, batch 50, train loss 0.039802923798561096\n",
      "Epoch 7, batch 60, train loss 0.03258630633354187\n",
      "Epoch 7, batch 70, train loss 0.022121181711554527\n",
      "Epoch 7, batch 80, train loss 0.03987431153655052\n",
      "Epoch 7, batch 90, train loss 0.029715772718191147\n",
      "Epoch 7, batch 100, train loss 0.038822632282972336\n",
      "Epoch 7, batch 110, train loss 0.04043371230363846\n",
      "Epoch 7, batch 120, train loss 0.018465934321284294\n",
      "Epoch 7, train loss 0.04871379445981363, val loss 0.040921519399687675, accuracy 0.7035175879396985, f1 0.7035167560773046\n",
      "No improvement.\n",
      "Epoch 8, batch 0, train loss 0.02619737759232521\n",
      "Epoch 8, batch 10, train loss 0.022797398269176483\n",
      "Epoch 8, batch 20, train loss 0.038163281977176666\n",
      "Epoch 8, batch 30, train loss 0.04077839106321335\n",
      "Epoch 8, batch 40, train loss 0.03477032110095024\n",
      "Epoch 8, batch 50, train loss 0.033504731953144073\n",
      "Epoch 8, batch 60, train loss 0.023295007646083832\n",
      "Epoch 8, batch 70, train loss 0.03523015230894089\n",
      "Epoch 8, batch 80, train loss 0.035177066922187805\n",
      "Epoch 8, batch 90, train loss 0.03274071589112282\n",
      "Epoch 8, batch 100, train loss 0.03288941830396652\n",
      "Epoch 8, batch 110, train loss 0.023369882255792618\n",
      "Epoch 8, batch 120, train loss 0.025022858753800392\n",
      "Epoch 8, train loss 0.048086636282246686, val loss 0.0364297629151512, accuracy 0.7278056951423786, f1 0.7278009218503503\n",
      "No improvement.\n",
      "Epoch 9, batch 0, train loss 0.03660698980093002\n",
      "Epoch 9, batch 10, train loss 0.031050603836774826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, batch 20, train loss 0.03278768062591553\n",
      "Epoch 9, batch 30, train loss 0.015525519847869873\n",
      "Epoch 9, batch 40, train loss 0.033642061054706573\n",
      "Epoch 9, batch 50, train loss 0.021566977724432945\n",
      "Epoch 9, batch 60, train loss 0.03916717320680618\n",
      "Epoch 9, batch 70, train loss 0.05242852866649628\n",
      "Epoch 9, batch 80, train loss 0.03453497588634491\n",
      "Epoch 9, batch 90, train loss 0.035776812583208084\n",
      "Epoch 9, batch 100, train loss 0.023819224908947945\n",
      "Epoch 9, batch 110, train loss 0.024403797462582588\n",
      "Epoch 9, batch 120, train loss 0.03292492777109146\n",
      "Epoch 9, train loss 0.04635313132927678, val loss 0.03206972024049391, accuracy 0.7554438860971524, f1 0.7554377104377105\n",
      "Save model at Epoch 9, train loss 0.04635313132927678, val loss 0.03206972024049391, accuracy 0.7554438860971524, f1 0.7554377104377105\n",
      "success!\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision\n",
    "import torchvision.models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from flair.data_fetcher import NLPTaskDataFetcher\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentRNNEmbeddings, Sentence\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import random\n",
    "from flair.training_utils import store_embeddings\n",
    "\n",
    "\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "def oversample_df(df, classes):\n",
    "    classes_count = []\n",
    "    for c in classes:    \n",
    "        classes_count.append(len(df.loc[df[c] == 1]))\n",
    "    \n",
    "    max_count = max(classes_count)\n",
    "    resample_ratios = [round(max_count/c) for c in classes_count]\n",
    "            \n",
    "    resampled = []\n",
    "    for i in range(len(resample_ratios)):\n",
    "        c = classes[i]\n",
    "        ratio = resample_ratios[i]        \n",
    "        for r in range(ratio):            \n",
    "            resampled.append(df.loc[df[c] == 1])\n",
    "            \n",
    "    resampled_df = pd.concat(resampled, ignore_index=True)\n",
    "    resampled_df = resampled_df.sample(frac=1)\n",
    "    resampled_df = resampled_df.reset_index(drop=True)\n",
    "    return resampled_df\n",
    "\n",
    "\n",
    "def undersample_df(df, classes):\n",
    "    classes_count = []\n",
    "    for c in classes:    \n",
    "        classes_count.append(len(df.loc[df[c] == 1]))\n",
    "    \n",
    "    min_count = min(classes_count)\n",
    "    \n",
    "    resampled = []\n",
    "    for c in classes:\n",
    "        resampled.append(df[df[c] == 1][:min_count])\n",
    "        \n",
    "    resampled_df = pd.concat(resampled, ignore_index=True)\n",
    "    resampled_df = resampled_df.sample(frac=1)\n",
    "    resampled_df = resampled_df.reset_index(drop=True)\n",
    "    return resampled_df\n",
    "\n",
    "\n",
    "def get_batches(df, target_names, mode=None, batch_size=16):\n",
    "    if mode == 'oversample':\n",
    "        df = oversample_df(df, target_names)\n",
    "    elif mode == 'undersample':\n",
    "        df = undersample_df(df, target_names)\n",
    "        \n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        ids = []\n",
    "        x = []\n",
    "        y = []\n",
    "        for _, row in df[i:i+batch_size].iterrows():\n",
    "            \n",
    "            image_concept = '' if pd.isna(row['image_concept']) else row['image_concept']\n",
    "            message = '' if pd.isna(row['message']) else row['message']                        \n",
    "            \n",
    "            # shuffle image concepts\n",
    "            words = image_concept.split()\n",
    "            random.shuffle(words)\n",
    "            image_concept = ' '.join(words)\n",
    "            \n",
    "            # join message and image_concept together\n",
    "            txt = ' '.join([message, image_concept])                    \n",
    "            x.append(Sentence(txt))                        \n",
    "            y.append([row[t] for t in target_names])\n",
    "            ids.append(row['_id'])\n",
    "        \n",
    "        yield ids, x, torch.FloatTensor(y)\n",
    "\n",
    "        \n",
    "def train_model(model, epochs, lr, train_df, val_df, target_names, checkpoint_file, early_stopping=5):        \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    best_loss = np.inf\n",
    "    no_improvement = 0\n",
    "\n",
    "    if train_on_gpu:\n",
    "        model = model.cuda()\n",
    "        \n",
    "    for epoch in range(epochs):        \n",
    "        total_train_loss = 0\n",
    "        total_val_loss = 0\n",
    "        train_loss = 0\n",
    "        val_loss = 0\n",
    "        \n",
    "        # Train\n",
    "        model.train()        \n",
    "        for i, (ids, sentences, labels) in enumerate(get_batches(train_df, target_names, 'oversample')):\n",
    "            if train_on_gpu:\n",
    "                labels = labels.cuda()\n",
    "            \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            out = model(sentences)            \n",
    "                        \n",
    "            loss = criterion(out, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            store_embeddings(sentences, 'cpu')\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, batch {i}, train loss {loss.item()/labels.size(0)}\")\n",
    "            \n",
    "            \n",
    "        train_loss = total_train_loss/len(train_df)\n",
    "\n",
    "        \n",
    "        # Eval\n",
    "        model.eval()\n",
    "        all_pred = np.array([])\n",
    "        all_labels = np.array([])\n",
    "        for _, sentences, labels in get_batches(val_df, target_names):\n",
    "            if train_on_gpu:\n",
    "                labels = labels.cuda()\n",
    "            \n",
    "\n",
    "            out = model(sentences)\n",
    "            loss = criterion(out, labels)\n",
    "            total_val_loss += loss.item()\n",
    "            \n",
    "            store_embeddings(sentences, 'cpu')\n",
    "            \n",
    "            pred = torch.round(torch.sigmoid(out))\n",
    "            pred_np = pred.data.cpu().numpy() if train_on_gpu else pred.data.numpy()\n",
    "            labels_np = labels.data.cpu().numpy() if train_on_gpu else labels.data.numpy()\n",
    "            all_pred = np.concatenate([all_pred, pred_np.flatten()])\n",
    "            all_labels = np.concatenate([all_labels, labels_np.flatten()])\n",
    "\n",
    "            \n",
    "        val_loss = total_val_loss / len(val_df)\n",
    "        f1 = f1_score(all_labels, all_pred, average='weighted')\n",
    "        acc = accuracy_score(all_labels, all_pred)\n",
    "        \n",
    "        print(f\"Epoch {epoch}, train loss {train_loss}, val loss {val_loss}, accuracy {acc}, f1 {f1}\")\n",
    "        \n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            no_improvement = 0\n",
    "            torch.save(model.state_dict(), checkpoint_file)\n",
    "            print(f\"Save model at Epoch {epoch}, train loss {train_loss}, val loss {val_loss}, accuracy {acc}, f1 {f1}\")\n",
    "        else:\n",
    "            no_improvement += 1\n",
    "            print(\"No improvement.\")\n",
    "            if no_improvement >= early_stopping:\n",
    "                print(f\"Early Stopping at Epoch {epoch}\")\n",
    "                break\n",
    "                \n",
    "\n",
    "def get_dfs(data_dir, column_names):    \n",
    "    csv_file = os.path.join(data_dir, 'preprocess_data.txt')\n",
    "    df = pd.read_csv(csv_file, names=column_names, engine='python')    \n",
    "    df = df.loc[df['message'].notnull() & df['image_concept'].notnull()]            \n",
    "    train_df, validation_df = train_test_split(df, test_size=0.3, random_state=42)    \n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "    validation_df = validation_df.reset_index(drop=True)    \n",
    "    return train_df, validation_df\n",
    "\n",
    "\n",
    "train_df, validation_df = get_dfs(\".\", column_names)\n",
    "    \n",
    "\n",
    "for target in target_names:\n",
    "    train_target_count = len(train_df.loc[train_df[target] == 1])\n",
    "    val_target_count = len(validation_df.loc[validation_df[target] == 1])\n",
    "    print(f\"class: ({target}), train: {train_target_count}, val: {val_target_count}\")\n",
    "\n",
    "\n",
    "document_embeddings = DocumentRNNEmbeddings([\n",
    "        WordEmbeddings('twitter'),\n",
    "    ], \n",
    "    hidden_size=128,\n",
    "    reproject_words=True,\n",
    "    reproject_words_dimension=128\n",
    ")\n",
    "\n",
    "classifier = TextClassifier(\n",
    "    document_embeddings, \n",
    "    label_dictionary=target_names,\n",
    "    multi_label=True\n",
    ")\n",
    "\n",
    "print(classifier)\n",
    "    \n",
    "checkpoint_file = os.path.join('/tmp', 'model.pt')\n",
    "lr = 0.001       \n",
    "train_model(\n",
    "    classifier, \n",
    "    10, \n",
    "    lr, \n",
    "    train_df, \n",
    "    validation_df, \n",
    "    target_names, \n",
    "    checkpoint_file,\n",
    "    early_stopping=5\n",
    ")\n",
    "\n",
    "print(\"success!\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('twitter')\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=100, out_features=128, bias=True)\n",
      "    (rnn): GRU(128, 128, batch_first=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (loss_function): BCEWithLogitsLoss()\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = TextClassifier(\n",
    "    document_embeddings, \n",
    "    label_dictionary=target_names,\n",
    "    multi_label=True\n",
    ")\n",
    "\n",
    "print(model2)\n",
    "\n",
    "model2.load_state_dict(torch.load(checkpoint_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.8200431034482759, F1 0.8200419657453925\n"
     ]
    }
   ],
   "source": [
    "def eval_model(model, test_df, target_names):\n",
    "    if train_on_gpu:\n",
    "        model = model.cuda()\n",
    "        \n",
    "    model.eval()\n",
    "    all_pred = np.array([])\n",
    "    all_labels = np.array([])\n",
    "    for _, sentences, labels in get_batches(test_df, target_names):\n",
    "        if train_on_gpu:\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        out = model(sentences)\n",
    "\n",
    "        store_embeddings(sentences, 'cpu')\n",
    "\n",
    "        pred = torch.round(torch.sigmoid(out))\n",
    "        pred_np = pred.data.cpu().numpy() if train_on_gpu else pred.data.numpy()\n",
    "        labels_np = labels.data.cpu().numpy() if train_on_gpu else labels.data.numpy()\n",
    "        all_pred = np.concatenate([all_pred, pred_np.flatten()])\n",
    "        all_labels = np.concatenate([all_labels, labels_np.flatten()])\n",
    "\n",
    "\n",
    "    f1 = f1_score(all_labels, all_pred, average='weighted')\n",
    "    acc = accuracy_score(all_labels, all_pred)\n",
    "\n",
    "    print(f\"Accuracy {acc}, F1 {f1}\")\n",
    "    \n",
    "eval_model(model2, train_df, target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>message</th>\n",
       "      <th>image_concept</th>\n",
       "      <th>image</th>\n",
       "      <th>published</th>\n",
       "      <th>disabled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5e2eb28842964194c85a9e79</td>\n",
       "      <td>„É™„Éº„Éï„Éì„É•„Éº„Éõ„ÉÜ„É´„Åß„ÅØ16Èöé„ÅÆÈÉ®Â±ã„Åß„Å®„Å´„Åã„ÅèÁú∫„ÇÅ„ÅåËâØ„ÅÑÔºÅ „Éõ„ÉÜ„É´„Å´„ÅØ„Éó„Éº„É´„ÇÇ„ÅÇ„Å£„Å¶Ë≥ë„ÇÑ„Åã„Å†„Å£...</td>\n",
       "      <td>railing animal bird cockatoo parrot wildlife m...</td>\n",
       "      <td>https://scontent.xx.fbcdn.net/v/t51.2885-15/83...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5deed4333a42802f61b1e2b8</td>\n",
       "      <td>wild oats xi and ribco ready for the grinders ...</td>\n",
       "      <td>transportation watercraft vessel vehicle boat ...</td>\n",
       "      <td>https://scontent.xx.fbcdn.net/v/t51.2885-15/75...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5e341b6c76f8896f9132ad1a</td>\n",
       "      <td>you, me &amp; the sea (2/3) . . . #hamiltonisland ...</td>\n",
       "      <td>person human jet ski transportation vehicle ap...</td>\n",
       "      <td>https://scontent.xx.fbcdn.net/v/t51.2885-15/82...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5e133457e066b13fc80f0fa6</td>\n",
       "      <td>ÏΩîÏä§ ‚õ≥ ÌÅ¥ÎüΩÌïòÏö∞Ïä§ üçΩ Ï±åÎ¶∞ÏßÄ üí™ ‚†Ä Í≥®ÌîÑÎß§ÎãàÏïÑÎì§ÏóêÍ≤å Ïù¥ÎØ∏ ÏûÖÏÜåÎ¨∏ ÎÇú Ìï¥Î∞ÄÌÑ¥ÏïÑÏùºÎûúÎìú...</td>\n",
       "      <td>nature outdoors land water shoreline ocean sea...</td>\n",
       "      <td>https://scontent.xx.fbcdn.net/v/t51.2885-15/79...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>5e38c6b822dd29f0a15b98f1</td>\n",
       "      <td>cruising around hamilton island. üõ©üí• #onetreehi...</td>\n",
       "      <td>shoreline water nature outdoors land sea ocean...</td>\n",
       "      <td>https://scontent.xx.fbcdn.net/v/t51.2885-15/83...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1372</th>\n",
       "      <td>5e468192d47bfd56c5265b7f</td>\n",
       "      <td>sunset at one tree hill! #onetreehill #sunset ...</td>\n",
       "      <td>nature outdoors red sky dawn sky dusk sunset s...</td>\n",
       "      <td>https://scontent.xx.fbcdn.net/v/t51.2885-15/83...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1377</th>\n",
       "      <td>5e49b0960bfa51488b937823</td>\n",
       "      <td>sunny days and mondays.</td>\n",
       "      <td>land outdoors nature shoreline water ocean sea...</td>\n",
       "      <td>https://scontent-iad3-1.cdninstagram.com/v/t51...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1379</th>\n",
       "      <td>5e1d7c54cbd4d346ef3679ea</td>\n",
       "      <td>‚Äúthe office‚Äù</td>\n",
       "      <td>land outdoors nature water shoreline sea ocean...</td>\n",
       "      <td>https://scontent-lga3-1.cdninstagram.com/v/t51...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1381</th>\n",
       "      <td>5dfa92b197304f0168d622fe</td>\n",
       "      <td>i don‚Äôt know if i was holding her or she was h...</td>\n",
       "      <td>animal mammal wildlife bear koala giant panda</td>\n",
       "      <td>https://scontent.xx.fbcdn.net/v/t51.2885-15/78...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1382</th>\n",
       "      <td>5e47600353417098f7fc0d51</td>\n",
       "      <td>introducing the beach pavilion on hamilton isl...</td>\n",
       "      <td>furniture chair summer table dining table trop...</td>\n",
       "      <td>https://scontent.xx.fbcdn.net/v/t51.2885-15/84...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>342 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           _id  \\\n",
       "5     5e2eb28842964194c85a9e79   \n",
       "25    5deed4333a42802f61b1e2b8   \n",
       "27    5e341b6c76f8896f9132ad1a   \n",
       "29    5e133457e066b13fc80f0fa6   \n",
       "31    5e38c6b822dd29f0a15b98f1   \n",
       "...                        ...   \n",
       "1372  5e468192d47bfd56c5265b7f   \n",
       "1377  5e49b0960bfa51488b937823   \n",
       "1379  5e1d7c54cbd4d346ef3679ea   \n",
       "1381  5dfa92b197304f0168d622fe   \n",
       "1382  5e47600353417098f7fc0d51   \n",
       "\n",
       "                                                message  \\\n",
       "5     „É™„Éº„Éï„Éì„É•„Éº„Éõ„ÉÜ„É´„Åß„ÅØ16Èöé„ÅÆÈÉ®Â±ã„Åß„Å®„Å´„Åã„ÅèÁú∫„ÇÅ„ÅåËâØ„ÅÑÔºÅ „Éõ„ÉÜ„É´„Å´„ÅØ„Éó„Éº„É´„ÇÇ„ÅÇ„Å£„Å¶Ë≥ë„ÇÑ„Åã„Å†„Å£...   \n",
       "25    wild oats xi and ribco ready for the grinders ...   \n",
       "27    you, me & the sea (2/3) . . . #hamiltonisland ...   \n",
       "29    ÏΩîÏä§ ‚õ≥ ÌÅ¥ÎüΩÌïòÏö∞Ïä§ üçΩ Ï±åÎ¶∞ÏßÄ üí™ ‚†Ä Í≥®ÌîÑÎß§ÎãàÏïÑÎì§ÏóêÍ≤å Ïù¥ÎØ∏ ÏûÖÏÜåÎ¨∏ ÎÇú Ìï¥Î∞ÄÌÑ¥ÏïÑÏùºÎûúÎìú...   \n",
       "31    cruising around hamilton island. üõ©üí• #onetreehi...   \n",
       "...                                                 ...   \n",
       "1372  sunset at one tree hill! #onetreehill #sunset ...   \n",
       "1377                            sunny days and mondays.   \n",
       "1379                                       ‚Äúthe office‚Äù   \n",
       "1381  i don‚Äôt know if i was holding her or she was h...   \n",
       "1382  introducing the beach pavilion on hamilton isl...   \n",
       "\n",
       "                                          image_concept  \\\n",
       "5     railing animal bird cockatoo parrot wildlife m...   \n",
       "25    transportation watercraft vessel vehicle boat ...   \n",
       "27    person human jet ski transportation vehicle ap...   \n",
       "29    nature outdoors land water shoreline ocean sea...   \n",
       "31    shoreline water nature outdoors land sea ocean...   \n",
       "...                                                 ...   \n",
       "1372  nature outdoors red sky dawn sky dusk sunset s...   \n",
       "1377  land outdoors nature shoreline water ocean sea...   \n",
       "1379  land outdoors nature water shoreline sea ocean...   \n",
       "1381      animal mammal wildlife bear koala giant panda   \n",
       "1382  furniture chair summer table dining table trop...   \n",
       "\n",
       "                                                  image  published  disabled  \n",
       "5     https://scontent.xx.fbcdn.net/v/t51.2885-15/83...          1         0  \n",
       "25    https://scontent.xx.fbcdn.net/v/t51.2885-15/75...          1         0  \n",
       "27    https://scontent.xx.fbcdn.net/v/t51.2885-15/82...          1         0  \n",
       "29    https://scontent.xx.fbcdn.net/v/t51.2885-15/79...          1         0  \n",
       "31    https://scontent.xx.fbcdn.net/v/t51.2885-15/83...          1         0  \n",
       "...                                                 ...        ...       ...  \n",
       "1372  https://scontent.xx.fbcdn.net/v/t51.2885-15/83...          1         0  \n",
       "1377  https://scontent-iad3-1.cdninstagram.com/v/t51...          1         0  \n",
       "1379  https://scontent-lga3-1.cdninstagram.com/v/t51...          1         0  \n",
       "1381  https://scontent.xx.fbcdn.net/v/t51.2885-15/78...          1         0  \n",
       "1382  https://scontent.xx.fbcdn.net/v/t51.2885-15/84...          1         0  \n",
       "\n",
       "[342 rows x 6 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.loc[train_df['published'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>message</th>\n",
       "      <th>image_concept</th>\n",
       "      <th>image</th>\n",
       "      <th>published</th>\n",
       "      <th>disabled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5e490801abcf0016a173a576</td>\n",
       "      <td>whitehaven beach ü§çüêö - definitely one of our hi...</td>\n",
       "      <td>person human accessories accessory sunglasses ...</td>\n",
       "      <td>https://scontent.xx.fbcdn.net/v/t51.2885-15/84...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5e4bb53a41da64df1c418792</td>\n",
       "      <td>amazing dinner with even more amazing ladies a...</td>\n",
       "      <td>food meal dish seasoning bowl restaurant cafet...</td>\n",
       "      <td>https://scontent.xx.fbcdn.net/v/t51.2885-15/83...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5e4cf285843e6c3e41217c99</td>\n",
       "      <td>pulau hamilton adalah salah satu tujuan libura...</td>\n",
       "      <td>tropical no person paper water turquoise deskt...</td>\n",
       "      <td>https://scontent.xx.fbcdn.net/v/t51.2885-15/84...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5e4d0ccef9b64844676addff</td>\n",
       "      <td>‚Äª honeymoon---6days--part2--‚úà . . . . #ÊÄù„ÅÑÂá∫Ë®òÈå≤ #...</td>\n",
       "      <td>education child young portrait isolated leisur...</td>\n",
       "      <td>https://scontent.xx.fbcdn.net/v/t51.2885-15/85...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5e50e293cb93eb9052d3264a</td>\n",
       "      <td>just livin' doing that whitsundays thing!!üòäüòäü¶Ñü¶Ñ...</td>\n",
       "      <td>couple leisure ocean water vacation girl summe...</td>\n",
       "      <td>https://scontent.xx.fbcdn.net/v/t51.2885-15/85...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>5e4ebcf7ab423de8e751bee4</td>\n",
       "      <td>still can't find nemo, but found all of his co...</td>\n",
       "      <td>marine underwater deep coral fish scuba ocean ...</td>\n",
       "      <td>https://scontent.xx.fbcdn.net/v/t51.2885-15/87...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1388</th>\n",
       "      <td>5e4d15140f85f846761e7d2c</td>\n",
       "      <td>action shots of wednesday night twilight saili...</td>\n",
       "      <td>water sailboat sea yacht sail crew ship waterc...</td>\n",
       "      <td>https://scontent.xx.fbcdn.net/v/t51.2885-15/87...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1389</th>\n",
       "      <td>5e51f931c82fabe3073098cd</td>\n",
       "      <td>one of the most amazing places to me #fun #sum...</td>\n",
       "      <td>fashion foot man shoe footwear girl beach two ...</td>\n",
       "      <td>https://scontent.xx.fbcdn.net/v/t51.2885-15/83...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1390</th>\n",
       "      <td>5e4bb6c45d0cb5e11f457c9e</td>\n",
       "      <td>#australia #hamiltonisland #pioveec√©sole #bell...</td>\n",
       "      <td>land nature outdoors water shoreline ocean sea...</td>\n",
       "      <td>https://scontent.xx.fbcdn.net/v/t51.2885-15/84...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1391</th>\n",
       "      <td>5e4ee1581d88d4f9dd08eb12</td>\n",
       "      <td>mir reicht's - ich geh schaukeln?üèñ . . . canon...</td>\n",
       "      <td>travel beach child ocean sand swing seashore s...</td>\n",
       "      <td>https://scontent.xx.fbcdn.net/v/t51.2885-15/87...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1050 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           _id  \\\n",
       "0     5e490801abcf0016a173a576   \n",
       "1     5e4bb53a41da64df1c418792   \n",
       "2     5e4cf285843e6c3e41217c99   \n",
       "3     5e4d0ccef9b64844676addff   \n",
       "4     5e50e293cb93eb9052d3264a   \n",
       "...                        ...   \n",
       "1387  5e4ebcf7ab423de8e751bee4   \n",
       "1388  5e4d15140f85f846761e7d2c   \n",
       "1389  5e51f931c82fabe3073098cd   \n",
       "1390  5e4bb6c45d0cb5e11f457c9e   \n",
       "1391  5e4ee1581d88d4f9dd08eb12   \n",
       "\n",
       "                                                message  \\\n",
       "0     whitehaven beach ü§çüêö - definitely one of our hi...   \n",
       "1     amazing dinner with even more amazing ladies a...   \n",
       "2     pulau hamilton adalah salah satu tujuan libura...   \n",
       "3     ‚Äª honeymoon---6days--part2--‚úà . . . . #ÊÄù„ÅÑÂá∫Ë®òÈå≤ #...   \n",
       "4     just livin' doing that whitsundays thing!!üòäüòäü¶Ñü¶Ñ...   \n",
       "...                                                 ...   \n",
       "1387  still can't find nemo, but found all of his co...   \n",
       "1388  action shots of wednesday night twilight saili...   \n",
       "1389  one of the most amazing places to me #fun #sum...   \n",
       "1390  #australia #hamiltonisland #pioveec√©sole #bell...   \n",
       "1391  mir reicht's - ich geh schaukeln?üèñ . . . canon...   \n",
       "\n",
       "                                          image_concept  \\\n",
       "0     person human accessories accessory sunglasses ...   \n",
       "1     food meal dish seasoning bowl restaurant cafet...   \n",
       "2     tropical no person paper water turquoise deskt...   \n",
       "3     education child young portrait isolated leisur...   \n",
       "4     couple leisure ocean water vacation girl summe...   \n",
       "...                                                 ...   \n",
       "1387  marine underwater deep coral fish scuba ocean ...   \n",
       "1388  water sailboat sea yacht sail crew ship waterc...   \n",
       "1389  fashion foot man shoe footwear girl beach two ...   \n",
       "1390  land nature outdoors water shoreline ocean sea...   \n",
       "1391  travel beach child ocean sand swing seashore s...   \n",
       "\n",
       "                                                  image  published  disabled  \n",
       "0     https://scontent.xx.fbcdn.net/v/t51.2885-15/84...          0         1  \n",
       "1     https://scontent.xx.fbcdn.net/v/t51.2885-15/83...          0         1  \n",
       "2     https://scontent.xx.fbcdn.net/v/t51.2885-15/84...          0         1  \n",
       "3     https://scontent.xx.fbcdn.net/v/t51.2885-15/85...          0         1  \n",
       "4     https://scontent.xx.fbcdn.net/v/t51.2885-15/85...          0         1  \n",
       "...                                                 ...        ...       ...  \n",
       "1387  https://scontent.xx.fbcdn.net/v/t51.2885-15/87...          0         1  \n",
       "1388  https://scontent.xx.fbcdn.net/v/t51.2885-15/87...          0         1  \n",
       "1389  https://scontent.xx.fbcdn.net/v/t51.2885-15/83...          0         1  \n",
       "1390  https://scontent.xx.fbcdn.net/v/t51.2885-15/84...          0         1  \n",
       "1391  https://scontent.xx.fbcdn.net/v/t51.2885-15/87...          0         1  \n",
       "\n",
       "[1050 rows x 6 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.loc[train_df['disabled'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5e490801abcf0016a173a576',\n",
       " 'whitehaven beach \\U0001f90düêö - definitely one of our highlights in australia #tb #whitsundays #australia #queensland #qld #travel #visitaustralia #photography #sea #pacific #nationalpark #hamiltonisland #whitehaven #whitehavenbeach #hamiltonislandair #helicopter',\n",
       " 'person human accessories accessory sunglasses nature land outdoors electronics window cockpit glasses',\n",
       " 'https://scontent.xx.fbcdn.net/v/t51.2885-15/84490140_567379633988670_6664859505015865217_n.jpg?_nc_cat=110&_nc_ohc=6M2SDnKcNUQAX8EM1hW&_nc_ht=scontent.xx&oh=b7d3790f673706cf9ecf2916b07c0bb2&oe=5EBCA4CE',\n",
       " 0,\n",
       " 1]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.iloc[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5e468192d47bfd56c5265b7f', 'sunset at one tree hill! #onetreehill #sunset #hamiltonisland', 'nature outdoors red sky dawn sky dusk sunset sunlight sunrise plant tree abies fir light flare', 'https://scontent.xx.fbcdn.net/v/t51.2885-15/83597253_187296752513549_6081794946749982366_n.jpg?_nc_cat=100&_nc_ohc=DOLnGMq13DIAX-AjFn9&_nc_ht=scontent.xx&oh=26c1c71142328ccd84b0955aa22b3a6a&oe=5ECD421B', 1, 0]\n",
      "['0.7', '0.31']\n"
     ]
    }
   ],
   "source": [
    "def predict(model, input_data):\n",
    "    # message and image concepts\n",
    "    text = input_data[1] + ' ' + input_data[2]\n",
    "    sentences = [Sentence(text)]\n",
    "    out = model([Sentence(text)])\n",
    "    store_embeddings(sentences, 'cpu')\n",
    "\n",
    "    pred = torch.round(torch.sigmoid(out) * 100) / 100\n",
    "    pred_np = pred.data.cpu().numpy() if train_on_gpu else pred.data.numpy()\n",
    "\n",
    "    out = []\n",
    "    for i, p in enumerate(pred_np[0]):\n",
    "        out.append(str(p))\n",
    "\n",
    "    return out\n",
    "\n",
    "sample = train_df.iloc[1372].tolist()\n",
    "print(sample)\n",
    "print(predict(model2, sample))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:efficientnet]",
   "language": "python",
   "name": "conda-env-efficientnet-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
