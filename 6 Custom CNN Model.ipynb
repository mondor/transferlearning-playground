{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable as V\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms as trn\n",
    "from torch.nn import functional as F\n",
    "import os\n",
    "from PIL import Image    \n",
    "train_on_gpu = torch.cuda.is_available()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "published 2628, disabled 5348\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>message</th>\n",
       "      <th>image_concept</th>\n",
       "      <th>published</th>\n",
       "      <th>disabled</th>\n",
       "      <th>available</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5e5836fee917e8d9a8a7b277</td>\n",
       "      <td>endless blues greatbarrierreef australia whits...</td>\n",
       "      <td>seascape water shoal sea turquoise sun tropica...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5e58343ded065ad79e312f3d</td>\n",
       "      <td>hamiltonisland</td>\n",
       "      <td>tree travel vacation seashore water hotel isla...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5e57dc939e88b6be2ac42800</td>\n",
       "      <td>we are going coconuts for hamiltonisland here ...</td>\n",
       "      <td>relaxation beach sea vacation sand recreation ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5e55dca437fa5927dcdf02f3</td>\n",
       "      <td>en route to gbr embrace the elevation in luxur...</td>\n",
       "      <td>nature travel diving water sea underwater ocea...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5e55d69eb9e5b725cd7ba02f</td>\n",
       "      <td>golf course views hamiltonislandgolfcourse whi...</td>\n",
       "      <td>outdoors landscape beach sky nature rural nope...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7971</th>\n",
       "      <td>5e253779f1b8d48ba5de7d32</td>\n",
       "      <td>colours so bright they hurt your eyes tropical...</td>\n",
       "      <td>outdoors nature scenery landscape water land o...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7972</th>\n",
       "      <td>5e252d334610948976f731e5</td>\n",
       "      <td>호 주 학 생 비 자 치 료 마 사 지 과 정 치 료 마 사 지 과 정 은 마 사 ...</td>\n",
       "      <td>human person patient therapy massage heel spa</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7973</th>\n",
       "      <td>5e252d334610948976f731e6</td>\n",
       "      <td>호 주 학 생 비 자 치 료 마 사 지 과 정 치 료 마 사 지 과 정 은 마 사 ...</td>\n",
       "      <td>plant paper text flower blossom</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7974</th>\n",
       "      <td>5e252d3342307c89757703c0</td>\n",
       "      <td>호 주 학 생 비 자 치 료 마 사 지 과 정 치 료 마 사 지 과 정 은 마 사 ...</td>\n",
       "      <td>person human finger hand dating face arm</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7975</th>\n",
       "      <td>5e252d33f7b8d8898b9e841f</td>\n",
       "      <td>호 주 학 생 비 자 치 료 마 사 지 과 정 치 료 마 사 지 과 정 은 마 사 ...</td>\n",
       "      <td>human person patient massage therapy spa</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7976 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           _id  \\\n",
       "0     5e5836fee917e8d9a8a7b277   \n",
       "1     5e58343ded065ad79e312f3d   \n",
       "2     5e57dc939e88b6be2ac42800   \n",
       "3     5e55dca437fa5927dcdf02f3   \n",
       "4     5e55d69eb9e5b725cd7ba02f   \n",
       "...                        ...   \n",
       "7971  5e253779f1b8d48ba5de7d32   \n",
       "7972  5e252d334610948976f731e5   \n",
       "7973  5e252d334610948976f731e6   \n",
       "7974  5e252d3342307c89757703c0   \n",
       "7975  5e252d33f7b8d8898b9e841f   \n",
       "\n",
       "                                                message  \\\n",
       "0     endless blues greatbarrierreef australia whits...   \n",
       "1                                        hamiltonisland   \n",
       "2     we are going coconuts for hamiltonisland here ...   \n",
       "3     en route to gbr embrace the elevation in luxur...   \n",
       "4     golf course views hamiltonislandgolfcourse whi...   \n",
       "...                                                 ...   \n",
       "7971  colours so bright they hurt your eyes tropical...   \n",
       "7972  호 주 학 생 비 자 치 료 마 사 지 과 정 치 료 마 사 지 과 정 은 마 사 ...   \n",
       "7973  호 주 학 생 비 자 치 료 마 사 지 과 정 치 료 마 사 지 과 정 은 마 사 ...   \n",
       "7974  호 주 학 생 비 자 치 료 마 사 지 과 정 치 료 마 사 지 과 정 은 마 사 ...   \n",
       "7975  호 주 학 생 비 자 치 료 마 사 지 과 정 치 료 마 사 지 과 정 은 마 사 ...   \n",
       "\n",
       "                                          image_concept  published  disabled  \\\n",
       "0     seascape water shoal sea turquoise sun tropica...          1         0   \n",
       "1     tree travel vacation seashore water hotel isla...          1         0   \n",
       "2     relaxation beach sea vacation sand recreation ...          1         0   \n",
       "3     nature travel diving water sea underwater ocea...          1         0   \n",
       "4     outdoors landscape beach sky nature rural nope...          1         0   \n",
       "...                                                 ...        ...       ...   \n",
       "7971  outdoors nature scenery landscape water land o...          0         1   \n",
       "7972      human person patient therapy massage heel spa          0         1   \n",
       "7973                    plant paper text flower blossom          0         1   \n",
       "7974           person human finger hand dating face arm          0         1   \n",
       "7975           human person patient massage therapy spa          0         1   \n",
       "\n",
       "      available  \n",
       "0             1  \n",
       "1             1  \n",
       "2             1  \n",
       "3             1  \n",
       "4             1  \n",
       "...         ...  \n",
       "7971          1  \n",
       "7972          1  \n",
       "7973          1  \n",
       "7974          1  \n",
       "7975          1  \n",
       "\n",
       "[7976 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "df = pd.read_csv('flair-vgg16-data.csv', names=['_id', 'message', 'image_concept', 'published', 'disabled'])\n",
    "df['available'] = 0\n",
    "\n",
    "all_images_path = 'data/all_images'\n",
    "for i, row in df.iterrows():\n",
    "    if os.path.isfile(os.path.join(all_images_path, row['_id'] + '.jpg')):\n",
    "        df.at[i, 'available']= 1    \n",
    "\n",
    "df_published = df[(df.published == 1) & (df.available == 1) & df.message.notnull()]\n",
    "\n",
    "df_disabled = df[(df.disabled == 1) & (df.available == 1) & df.message.notnull()]\n",
    "\n",
    "print(f\"published {len(df_published)}, disabled {len(df_disabled)}\")\n",
    "\n",
    "df_all = pd.concat([df_published, df_disabled], ignore_index=True)\n",
    "\n",
    "df_all = df_all.reset_index(drop=True)\n",
    "\n",
    "df_all        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 4785, val 1914, test 1277\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(df_all, test_size=0.4, random_state=42)\n",
    "val_df, test_df = train_test_split(val_df, test_size=0.4, random_state=42)\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "print(f\"train {len(train_df)}, val {len(val_df)}, test {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/efficientnet/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ec2-user/anaconda3/envs/efficientnet/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ec2-user/anaconda3/envs/efficientnet/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ec2-user/anaconda3/envs/efficientnet/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ec2-user/anaconda3/envs/efficientnet/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ec2-user/anaconda3/envs/efficientnet/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "published 3116\n",
      "disabled 3227\n",
      "published 1558\n",
      "disabled 1558\n"
     ]
    }
   ],
   "source": [
    "from flair.embeddings import (\n",
    "    Sentence, \n",
    "    WordEmbeddings, \n",
    "    FlairEmbeddings, \n",
    "    StackedEmbeddings, \n",
    "    DocumentRNNEmbeddings,\n",
    "    BytePairEmbeddings\n",
    ")\n",
    "from flair.training_utils import store_embeddings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def oversample_df(df):\n",
    "    classes = ['published', 'disabled'] \n",
    "    classes_count = []\n",
    "    for c in classes:    \n",
    "        classes_count.append(len(df.loc[df[c] == 1]))\n",
    "    \n",
    "    max_count = max(classes_count)\n",
    "    resample_ratios = [round(max_count/c) for c in classes_count]\n",
    "            \n",
    "    resampled = []\n",
    "    for i in range(len(resample_ratios)):\n",
    "        c = classes[i]\n",
    "        ratio = resample_ratios[i]        \n",
    "        for r in range(ratio):            \n",
    "            resampled.append(df.loc[df[c] == 1])\n",
    "            \n",
    "    resampled_df = pd.concat(resampled, ignore_index=True)\n",
    "    resampled_df = resampled_df.sample(frac=1)\n",
    "    resampled_df = resampled_df.reset_index(drop=True)\n",
    "    \n",
    "    return resampled_df\n",
    "\n",
    "resampled = oversample_df(train_df)\n",
    "print(f\"published {len(resampled.loc[resampled.published == 1])}\")\n",
    "print(f\"disabled {len(resampled.loc[resampled.disabled == 1])}\")\n",
    "\n",
    "\n",
    "def balance_df(df):\n",
    "    classes = ['published', 'disabled'] \n",
    "    classes_count = []\n",
    "    for c in classes:    \n",
    "        classes_count.append(len(df.loc[df[c] == 1]))\n",
    "    \n",
    "    min_count = min(classes_count)\n",
    "    \n",
    "    resampled = []\n",
    "    for c in classes:\n",
    "        resampled.append(df[df[c] == 1][:min_count])\n",
    "        \n",
    "    resampled_df = pd.concat(resampled, ignore_index=True)\n",
    "    resampled_df = resampled_df.sample(frac=1)\n",
    "    resampled_df = resampled_df.reset_index(drop=True)\n",
    "    \n",
    "    return resampled_df\n",
    "    \n",
    "    \n",
    "resampled = balance_df(train_df)\n",
    "print(f\"published {len(resampled.loc[resampled.published == 1])}\")\n",
    "print(f\"disabled {len(resampled.loc[resampled.disabled == 1])}\")\n",
    "\n",
    "def get_batches(df, transformer, oversample=False, batch_size=16):        \n",
    "    if oversample:        \n",
    "        df = oversample_df(df)\n",
    "    else:\n",
    "        df = balance_df(df)\n",
    "        \n",
    "    n_batches = len(df)//batch_size    \n",
    "    for i in range(0, len(df), batch_size):\n",
    "        sentences = [Sentence(txt) for txt in df[i:i+batch_size]['message']]\n",
    "        labels = [1 if label else 0 for label in df[i:i+batch_size]['published']]\n",
    "        \n",
    "        images = []\n",
    "        for _id in df[i:i+batch_size]['_id']:\n",
    "            image = Image.open('data/all_images/'+_id+'.jpg').convert('RGB')\n",
    "            images.append(transformer(image).unsqueeze(0))\n",
    "            \n",
    "        image_tensor = torch.cat(images, 0)\n",
    "        \n",
    "        yield sentences, image_tensor, torch.FloatTensor(labels)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/efficientnet/lib/python3.6/site-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "/home/ec2-user/anaconda3/envs/efficientnet/lib/python3.6/site-packages/ipykernel_launcher.py:55: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "/home/ec2-user/anaconda3/envs/efficientnet/lib/python3.6/site-packages/ipykernel_launcher.py:56: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "/home/ec2-user/anaconda3/envs/efficientnet/lib/python3.6/site-packages/ipykernel_launcher.py:57: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "/home/ec2-user/anaconda3/envs/efficientnet/lib/python3.6/site-packages/ipykernel_launcher.py:58: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "/home/ec2-user/anaconda3/envs/efficientnet/lib/python3.6/site-packages/ipykernel_launcher.py:59: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "/home/ec2-user/anaconda3/envs/efficientnet/lib/python3.6/site-packages/ipykernel_launcher.py:60: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv2d(128, 256, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv5): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (pool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (drop1): Dropout(p=0.1, inplace=False)\n",
      "  (drop2): Dropout(p=0.2, inplace=False)\n",
      "  (fc1): Linear(in_features=18432, out_features=1024, bias=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=1, bias=True)\n",
      ")\n",
      "torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        \n",
    "        # output size: (224 - 5)/1 + 1 = 220\n",
    "        # output tensor: 32 x 220 x 220\n",
    "        self.conv1 = nn.Conv2d(3, 32, 5)             \n",
    "               \n",
    "        # output size: 220/2 = 110\n",
    "        # output matrix: 32 x 110 x 110\n",
    "        self.pool1 = nn.MaxPool2d(2,2)\n",
    "        \n",
    "        # output size: (110 - 4)/1 + 1 = 107\n",
    "        # output tensor: 64 x 107 x 107\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4)\n",
    "        \n",
    "        # output size: 107/2 = 53\n",
    "        # output tensor: 64 x 53 x 53\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "        \n",
    "        # output size: (53 - 3)/1 + 1 = 51\n",
    "        # output tensor: 128 x 51 x 51\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3)\n",
    "        \n",
    "        # output size: 51/2 = 25\n",
    "        # output tensor: 128 x 25 x 25\n",
    "        self.pool3 = nn.MaxPool2d(2,2)\n",
    "        \n",
    "        # output size: (25 - 2)/1 + 1 = 24\n",
    "        # output tensor: 256 x 24 x 24\n",
    "        self.conv4 = nn.Conv2d(128, 256, 2)\n",
    "        \n",
    "        # output size: 24/2 = 12\n",
    "        # output tensor: 256 x 12 x 12\n",
    "        self.pool4 = nn.MaxPool2d(2,2)\n",
    "\n",
    "        \n",
    "        # output size: (12 - 1)/1 + 1 = 12\n",
    "        # output tensor: 512 x 12 x 12\n",
    "        self.conv5 = nn.Conv2d(256, 512, 1)\n",
    "        \n",
    "        # output size: 12/2 = 6\n",
    "        # output tensor: 512 x 6 x 6\n",
    "        self.pool5 = nn.MaxPool2d(2,2)\n",
    "\n",
    "                               \n",
    "        self.drop1 = nn.Dropout(p=0.1)\n",
    "        self.drop2 = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(512*6*6, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1)\n",
    "        \n",
    "        \n",
    "        torch.nn.init.xavier_uniform(self.conv1.weight)\n",
    "        torch.nn.init.xavier_uniform(self.conv2.weight)\n",
    "        torch.nn.init.xavier_uniform(self.conv3.weight)\n",
    "        torch.nn.init.xavier_uniform(self.conv4.weight)\n",
    "        torch.nn.init.xavier_uniform(self.conv5.weight)\n",
    "        torch.nn.init.xavier_uniform(self.fc1.weight)\n",
    "        torch.nn.init.xavier_uniform(self.fc2.weight)\n",
    "        \n",
    "    def forward(self, sentences, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        if train_on_gpu:\n",
    "            x = x.cuda()\n",
    "        \n",
    "        x = self.drop1(self.pool1(F.relu(self.conv1(x))))\n",
    "        \n",
    "        x = self.drop1(self.pool2(F.relu(self.conv2(x))))\n",
    "        \n",
    "        x = self.drop1(self.pool3(F.relu(self.conv3(x))))\n",
    "        \n",
    "        x = self.drop1(self.pool4(F.relu(self.conv4(x))))\n",
    "        \n",
    "        x = self.drop1(self.pool5(F.relu(self.conv5(x))))\n",
    "        \n",
    "        x = x.view(batch_size, -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop2(x)        \n",
    "        x = self.fc2(x)\n",
    "        return x        \n",
    "    \n",
    "    \n",
    "\n",
    "model = MyModel()\n",
    "if train_on_gpu:\n",
    "    model = model.cuda()\n",
    "print(model)\n",
    "\n",
    "\n",
    "print(model(None, torch.zeros((1,3,224,224))).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0, train loss 0.044128403067588806\n",
      "Epoch 0, Batch 10, train loss 0.045231640338897705\n",
      "Epoch 0, Batch 20, train loss 0.04155183956027031\n",
      "Epoch 0, Batch 30, train loss 0.04200037941336632\n",
      "Epoch 0, Batch 40, train loss 0.04160936176776886\n",
      "Epoch 0, Batch 50, train loss 0.03597522899508476\n",
      "Epoch 0, Batch 60, train loss 0.049866754561662674\n",
      "Epoch 0, Batch 70, train loss 0.04419554024934769\n",
      "Epoch 0, Batch 80, train loss 0.041537266224622726\n",
      "Epoch 0, Batch 90, train loss 0.0436345636844635\n",
      "Epoch 0, Batch 100, train loss 0.04237906262278557\n",
      "Epoch 0, Batch 110, train loss 0.04124557226896286\n",
      "Epoch 0, Batch 120, train loss 0.03947645425796509\n",
      "Epoch 0, Batch 130, train loss 0.04014018550515175\n",
      "Epoch 0, Batch 140, train loss 0.04337479919195175\n",
      "Epoch 0, Batch 150, train loss 0.04203856736421585\n",
      "Epoch 0, Batch 160, train loss 0.04278353974223137\n",
      "Epoch 0, Batch 170, train loss 0.039997123181819916\n",
      "Epoch 0, Batch 180, train loss 0.044882938265800476\n",
      "Epoch 0, Batch 190, train loss 0.04224656894803047\n",
      "Epoch 0, Batch 200, train loss 0.04202086478471756\n",
      "Epoch 0, Batch 210, train loss 0.04290235415101051\n",
      "Epoch 0, Batch 220, train loss 0.04155950993299484\n",
      "Epoch 0, Batch 230, train loss 0.04041384533047676\n",
      "Epoch 0, Batch 240, train loss 0.04074418544769287\n",
      "Epoch 0, Batch 250, train loss 0.04017358273267746\n",
      "Epoch 0, Batch 260, train loss 0.042113564908504486\n",
      "Epoch 0, Batch 270, train loss 0.04361999034881592\n",
      "Epoch 0, Batch 280, train loss 0.042476389557123184\n",
      "Epoch 0, Batch 290, train loss 0.04208911955356598\n",
      "Epoch 0, Batch 300, train loss 0.04221935570240021\n",
      "Epoch 0, Batch 310, train loss 0.04179201275110245\n",
      "Epoch 0, Batch 320, train loss 0.043511245399713516\n",
      "Epoch 0, Batch 330, train loss 0.04102431982755661\n",
      "Epoch 0, Batch 340, train loss 0.04997175931930542\n",
      "Epoch 0, Batch 350, train loss 0.04538465663790703\n",
      "Epoch 0, Batch 360, train loss 0.04068485647439957\n",
      "Epoch 0, Batch 370, train loss 0.0423852801322937\n",
      "Epoch 0, Batch 380, train loss 0.03940245509147644\n",
      "Epoch 0, Batch 390, train loss 0.04374369978904724\n",
      "> Epoch 0, train loss 0.044384600818110215\n",
      "> Epoch 0, val loss 0.04305884602457978, accuracy 0.5007751937984496, f1_score 0.33505367530999774\n",
      "Saved model.\n",
      "Epoch 1, Batch 0, train loss 0.04051637649536133\n",
      "Epoch 1, Batch 10, train loss 0.04239568114280701\n",
      "Epoch 1, Batch 20, train loss 0.04306797683238983\n",
      "Epoch 1, Batch 30, train loss 0.03841178119182587\n",
      "Epoch 1, Batch 40, train loss 0.043073173612356186\n",
      "Epoch 1, Batch 50, train loss 0.03640342503786087\n",
      "Epoch 1, Batch 60, train loss 0.040079593658447266\n",
      "Epoch 1, Batch 70, train loss 0.043353721499443054\n",
      "Epoch 1, Batch 80, train loss 0.04439179599285126\n",
      "Epoch 1, Batch 90, train loss 0.04303080216050148\n",
      "Epoch 1, Batch 100, train loss 0.04166525602340698\n",
      "Epoch 1, Batch 110, train loss 0.04333600401878357\n",
      "Epoch 1, Batch 120, train loss 0.04277704283595085\n",
      "Epoch 1, Batch 130, train loss 0.04544275626540184\n",
      "Epoch 1, Batch 140, train loss 0.043631330132484436\n",
      "Epoch 1, Batch 150, train loss 0.03745505213737488\n",
      "Epoch 1, Batch 160, train loss 0.04103755205869675\n",
      "Epoch 1, Batch 170, train loss 0.04344377666711807\n",
      "Epoch 1, Batch 180, train loss 0.04261614382266998\n",
      "Epoch 1, Batch 190, train loss 0.041777823120355606\n",
      "Epoch 1, Batch 200, train loss 0.03814855217933655\n",
      "Epoch 1, Batch 210, train loss 0.04616234451532364\n",
      "Epoch 1, Batch 220, train loss 0.04303310811519623\n",
      "Epoch 1, Batch 230, train loss 0.042015332728624344\n",
      "Epoch 1, Batch 240, train loss 0.03823118284344673\n",
      "Epoch 1, Batch 250, train loss 0.03922990337014198\n",
      "Epoch 1, Batch 260, train loss 0.04115689545869827\n",
      "Epoch 1, Batch 270, train loss 0.043442826718091965\n",
      "Epoch 1, Batch 280, train loss 0.044335562735795975\n",
      "Epoch 1, Batch 290, train loss 0.03944447636604309\n",
      "Epoch 1, Batch 300, train loss 0.04037889093160629\n",
      "Epoch 1, Batch 310, train loss 0.04225408658385277\n",
      "Epoch 1, Batch 320, train loss 0.04174669086933136\n",
      "Epoch 1, Batch 330, train loss 0.042502596974372864\n",
      "Epoch 1, Batch 340, train loss 0.042168859392404556\n",
      "Epoch 1, Batch 350, train loss 0.043828416615724564\n",
      "Epoch 1, Batch 360, train loss 0.04491689056158066\n",
      "Epoch 1, Batch 370, train loss 0.04125743731856346\n",
      "Epoch 1, Batch 380, train loss 0.036876924335956573\n",
      "Epoch 1, Batch 390, train loss 0.052149221301078796\n",
      "> Epoch 1, train loss 0.04268326126737924\n",
      "> Epoch 1, val loss 0.04307436430177023, accuracy 0.5348837209302325, f1_score 0.4742762128325509\n",
      "No improvement.\n",
      "Epoch 2, Batch 0, train loss 0.04034746438264847\n",
      "Epoch 2, Batch 10, train loss 0.042579419910907745\n",
      "Epoch 2, Batch 20, train loss 0.04261019825935364\n",
      "Epoch 2, Batch 30, train loss 0.04385979101061821\n",
      "Epoch 2, Batch 40, train loss 0.03995591774582863\n",
      "Epoch 2, Batch 50, train loss 0.04194583743810654\n",
      "Epoch 2, Batch 60, train loss 0.04414951056241989\n",
      "Epoch 2, Batch 70, train loss 0.038180939853191376\n",
      "Epoch 2, Batch 80, train loss 0.03926530107855797\n",
      "Epoch 2, Batch 90, train loss 0.04090087115764618\n",
      "Epoch 2, Batch 100, train loss 0.041756413877010345\n",
      "Epoch 2, Batch 110, train loss 0.04273379221558571\n",
      "Epoch 2, Batch 120, train loss 0.04018763452768326\n",
      "Epoch 2, Batch 130, train loss 0.03451715037226677\n",
      "Epoch 2, Batch 140, train loss 0.038825903087854385\n",
      "Epoch 2, Batch 150, train loss 0.0411742702126503\n",
      "Epoch 2, Batch 160, train loss 0.03845289349555969\n",
      "Epoch 2, Batch 170, train loss 0.049607858061790466\n",
      "Epoch 2, Batch 180, train loss 0.045041970908641815\n",
      "Epoch 2, Batch 190, train loss 0.043473102152347565\n",
      "Epoch 2, Batch 200, train loss 0.036617908626794815\n",
      "Epoch 2, Batch 210, train loss 0.036973439157009125\n",
      "Epoch 2, Batch 220, train loss 0.041185613721609116\n",
      "Epoch 2, Batch 230, train loss 0.04032142460346222\n",
      "Epoch 2, Batch 240, train loss 0.03896573185920715\n",
      "Epoch 2, Batch 250, train loss 0.04155872017145157\n",
      "Epoch 2, Batch 260, train loss 0.04555077850818634\n",
      "Epoch 2, Batch 270, train loss 0.047282446175813675\n",
      "Epoch 2, Batch 280, train loss 0.043881095945835114\n",
      "Epoch 2, Batch 290, train loss 0.04007154330611229\n",
      "Epoch 2, Batch 300, train loss 0.04548847675323486\n",
      "Epoch 2, Batch 310, train loss 0.055547118186950684\n",
      "Epoch 2, Batch 320, train loss 0.04070081189274788\n",
      "Epoch 2, Batch 330, train loss 0.03854276239871979\n",
      "Epoch 2, Batch 340, train loss 0.048785872757434845\n",
      "Epoch 2, Batch 350, train loss 0.04023318737745285\n",
      "Epoch 2, Batch 360, train loss 0.039287418127059937\n",
      "Epoch 2, Batch 370, train loss 0.03765101730823517\n",
      "Epoch 2, Batch 380, train loss 0.03375495970249176\n",
      "Epoch 2, Batch 390, train loss 0.045263148844242096\n",
      "> Epoch 2, train loss 0.042215020047101655\n",
      "> Epoch 2, val loss 0.04314079053642214, accuracy 0.5596899224806201, f1_score 0.5590274434280212\n",
      "No improvement.\n",
      "Epoch 3, Batch 0, train loss 0.03740924596786499\n",
      "Epoch 3, Batch 10, train loss 0.04305975139141083\n",
      "Epoch 3, Batch 20, train loss 0.041923634707927704\n",
      "Epoch 3, Batch 30, train loss 0.038678236305713654\n",
      "Epoch 3, Batch 40, train loss 0.04067361727356911\n",
      "Epoch 3, Batch 50, train loss 0.0401858389377594\n",
      "Epoch 3, Batch 60, train loss 0.03996998071670532\n",
      "Epoch 3, Batch 70, train loss 0.04309600964188576\n",
      "Epoch 3, Batch 80, train loss 0.03742324188351631\n",
      "Epoch 3, Batch 90, train loss 0.042736418545246124\n",
      "Epoch 3, Batch 100, train loss 0.03919466212391853\n",
      "Epoch 3, Batch 110, train loss 0.04102326184511185\n",
      "Epoch 3, Batch 120, train loss 0.04601902514696121\n",
      "Epoch 3, Batch 130, train loss 0.03928827866911888\n",
      "Epoch 3, Batch 140, train loss 0.03889165073633194\n",
      "Epoch 3, Batch 150, train loss 0.04021339863538742\n",
      "Epoch 3, Batch 160, train loss 0.0394924134016037\n",
      "Epoch 3, Batch 170, train loss 0.040136002004146576\n",
      "Epoch 3, Batch 180, train loss 0.03894556313753128\n",
      "Epoch 3, Batch 190, train loss 0.04788373410701752\n",
      "Epoch 3, Batch 200, train loss 0.04116864502429962\n",
      "Epoch 3, Batch 210, train loss 0.04692738875746727\n",
      "Epoch 3, Batch 220, train loss 0.04232156649231911\n",
      "Epoch 3, Batch 230, train loss 0.04014122486114502\n",
      "Epoch 3, Batch 240, train loss 0.03896530717611313\n",
      "Epoch 3, Batch 250, train loss 0.03514646738767624\n",
      "Epoch 3, Batch 260, train loss 0.04993467777967453\n",
      "Epoch 3, Batch 270, train loss 0.039153844118118286\n",
      "Epoch 3, Batch 280, train loss 0.040138423442840576\n",
      "Epoch 3, Batch 290, train loss 0.03841901198029518\n",
      "Epoch 3, Batch 300, train loss 0.04920559749007225\n",
      "Epoch 3, Batch 310, train loss 0.04355456307530403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Batch 320, train loss 0.03607296198606491\n",
      "Epoch 3, Batch 330, train loss 0.03665335476398468\n",
      "Epoch 3, Batch 340, train loss 0.04982275143265724\n",
      "Epoch 3, Batch 350, train loss 0.04847220331430435\n",
      "Epoch 3, Batch 360, train loss 0.040311574935913086\n",
      "Epoch 3, Batch 370, train loss 0.04558607563376427\n",
      "Epoch 3, Batch 380, train loss 0.04325726628303528\n",
      "Epoch 3, Batch 390, train loss 0.04032911732792854\n",
      "> Epoch 3, train loss 0.041687841551096755\n",
      "> Epoch 3, val loss 0.042583943568458855, accuracy 0.5976744186046512, f1_score 0.5950930485862941\n",
      "Saved model.\n",
      "Epoch 4, Batch 0, train loss 0.04030797630548477\n",
      "Epoch 4, Batch 10, train loss 0.040858082473278046\n",
      "Epoch 4, Batch 20, train loss 0.042447417974472046\n",
      "Epoch 4, Batch 30, train loss 0.04611053690314293\n",
      "Epoch 4, Batch 40, train loss 0.04414750635623932\n",
      "Epoch 4, Batch 50, train loss 0.03949117287993431\n",
      "Epoch 4, Batch 60, train loss 0.044487595558166504\n",
      "Epoch 4, Batch 70, train loss 0.04399013891816139\n",
      "Epoch 4, Batch 80, train loss 0.04091332107782364\n",
      "Epoch 4, Batch 90, train loss 0.03312954679131508\n",
      "Epoch 4, Batch 100, train loss 0.04293014109134674\n",
      "Epoch 4, Batch 110, train loss 0.04237401857972145\n",
      "Epoch 4, Batch 120, train loss 0.03417116776108742\n",
      "Epoch 4, Batch 130, train loss 0.03686736896634102\n",
      "Epoch 4, Batch 140, train loss 0.0443631187081337\n",
      "Epoch 4, Batch 150, train loss 0.04338633641600609\n",
      "Epoch 4, Batch 160, train loss 0.04819057881832123\n",
      "Epoch 4, Batch 170, train loss 0.04238881170749664\n",
      "Epoch 4, Batch 180, train loss 0.04092300683259964\n",
      "Epoch 4, Batch 190, train loss 0.05146382004022598\n",
      "Epoch 4, Batch 200, train loss 0.04197113960981369\n",
      "Epoch 4, Batch 210, train loss 0.03543335571885109\n",
      "Epoch 4, Batch 220, train loss 0.038352131843566895\n",
      "Epoch 4, Batch 230, train loss 0.04308856651186943\n",
      "Epoch 4, Batch 240, train loss 0.04684600979089737\n",
      "Epoch 4, Batch 250, train loss 0.043282218277454376\n",
      "Epoch 4, Batch 260, train loss 0.04516766220331192\n",
      "Epoch 4, Batch 270, train loss 0.03932735323905945\n",
      "Epoch 4, Batch 280, train loss 0.03543022274971008\n",
      "Epoch 4, Batch 290, train loss 0.03870554268360138\n",
      "Epoch 4, Batch 300, train loss 0.04119096323847771\n",
      "Epoch 4, Batch 310, train loss 0.045704036951065063\n",
      "Epoch 4, Batch 320, train loss 0.04328859597444534\n",
      "Epoch 4, Batch 330, train loss 0.041483066976070404\n",
      "Epoch 4, Batch 340, train loss 0.046618372201919556\n",
      "Epoch 4, Batch 350, train loss 0.0444033183157444\n",
      "Epoch 4, Batch 360, train loss 0.042090944945812225\n",
      "Epoch 4, Batch 370, train loss 0.040956564247608185\n",
      "Epoch 4, Batch 380, train loss 0.04204719513654709\n",
      "Epoch 4, Batch 390, train loss 0.03608629107475281\n",
      "> Epoch 4, train loss 0.042032968381517564\n",
      "> Epoch 4, val loss 0.04153946846030479, accuracy 0.6093023255813953, f1_score 0.6055390529442599\n",
      "Saved model.\n",
      "Epoch 5, Batch 0, train loss 0.03734460845589638\n",
      "Epoch 5, Batch 10, train loss 0.043575070798397064\n",
      "Epoch 5, Batch 20, train loss 0.033699311316013336\n",
      "Epoch 5, Batch 30, train loss 0.04323291778564453\n",
      "Epoch 5, Batch 40, train loss 0.04081561043858528\n",
      "Epoch 5, Batch 50, train loss 0.0380847230553627\n",
      "Epoch 5, Batch 60, train loss 0.0361957885324955\n",
      "Epoch 5, Batch 70, train loss 0.04205404967069626\n",
      "Epoch 5, Batch 80, train loss 0.05317332223057747\n",
      "Epoch 5, Batch 90, train loss 0.04468678683042526\n",
      "Epoch 5, Batch 100, train loss 0.044719401746988297\n",
      "Epoch 5, Batch 110, train loss 0.04148772358894348\n",
      "Epoch 5, Batch 120, train loss 0.04411713033914566\n",
      "Epoch 5, Batch 130, train loss 0.0442131906747818\n",
      "Epoch 5, Batch 140, train loss 0.03752289339900017\n",
      "Epoch 5, Batch 150, train loss 0.04653961956501007\n",
      "Epoch 5, Batch 160, train loss 0.04290661960840225\n",
      "Epoch 5, Batch 170, train loss 0.04138040170073509\n",
      "Epoch 5, Batch 180, train loss 0.0414835624396801\n",
      "Epoch 5, Batch 190, train loss 0.03739551827311516\n",
      "Epoch 5, Batch 200, train loss 0.039894282817840576\n",
      "Epoch 5, Batch 210, train loss 0.036581724882125854\n",
      "Epoch 5, Batch 220, train loss 0.03665204346179962\n",
      "Epoch 5, Batch 230, train loss 0.0427948497235775\n",
      "Epoch 5, Batch 240, train loss 0.04011940583586693\n",
      "Epoch 5, Batch 250, train loss 0.038613203912973404\n",
      "Epoch 5, Batch 260, train loss 0.04146668687462807\n",
      "Epoch 5, Batch 270, train loss 0.040756676346063614\n",
      "Epoch 5, Batch 280, train loss 0.0355101078748703\n",
      "Epoch 5, Batch 290, train loss 0.0340082123875618\n",
      "Epoch 5, Batch 300, train loss 0.04463905841112137\n",
      "Epoch 5, Batch 310, train loss 0.046554286032915115\n",
      "Epoch 5, Batch 320, train loss 0.03645850718021393\n",
      "Epoch 5, Batch 330, train loss 0.038745611906051636\n",
      "Epoch 5, Batch 340, train loss 0.04553510248661041\n",
      "Epoch 5, Batch 350, train loss 0.03702276572585106\n",
      "Epoch 5, Batch 360, train loss 0.04353168606758118\n",
      "Epoch 5, Batch 370, train loss 0.045484691858291626\n",
      "Epoch 5, Batch 380, train loss 0.04271094128489494\n",
      "Epoch 5, Batch 390, train loss 0.03788154944777489\n",
      "> Epoch 5, train loss 0.041190792662703084\n",
      "> Epoch 5, val loss 0.043442603645398634, accuracy 0.5666666666666667, f1_score 0.52082498559704\n",
      "No improvement.\n",
      "Epoch 6, Batch 0, train loss 0.038854751735925674\n",
      "Epoch 6, Batch 10, train loss 0.04368139058351517\n",
      "Epoch 6, Batch 20, train loss 0.04040665552020073\n",
      "Epoch 6, Batch 30, train loss 0.03780171275138855\n",
      "Epoch 6, Batch 40, train loss 0.04566199332475662\n",
      "Epoch 6, Batch 50, train loss 0.046610645949840546\n",
      "Epoch 6, Batch 60, train loss 0.03700471669435501\n",
      "Epoch 6, Batch 70, train loss 0.045574501156806946\n",
      "Epoch 6, Batch 80, train loss 0.04388730227947235\n",
      "Epoch 6, Batch 90, train loss 0.03992530331015587\n",
      "Epoch 6, Batch 100, train loss 0.05259701609611511\n",
      "Epoch 6, Batch 110, train loss 0.04060462862253189\n",
      "Epoch 6, Batch 120, train loss 0.04286431893706322\n",
      "Epoch 6, Batch 130, train loss 0.040914811193943024\n",
      "Epoch 6, Batch 140, train loss 0.04910574108362198\n",
      "Epoch 6, Batch 150, train loss 0.0441528856754303\n",
      "Epoch 6, Batch 160, train loss 0.04247530549764633\n",
      "Epoch 6, Batch 170, train loss 0.031668029725551605\n",
      "Epoch 6, Batch 180, train loss 0.04237132892012596\n",
      "Epoch 6, Batch 190, train loss 0.03762645274400711\n",
      "Epoch 6, Batch 200, train loss 0.044714562594890594\n",
      "Epoch 6, Batch 210, train loss 0.04439464956521988\n",
      "Epoch 6, Batch 220, train loss 0.042496711015701294\n",
      "Epoch 6, Batch 230, train loss 0.03608379140496254\n",
      "Epoch 6, Batch 240, train loss 0.04322657734155655\n",
      "Epoch 6, Batch 250, train loss 0.0460849404335022\n",
      "Epoch 6, Batch 260, train loss 0.04254203289747238\n",
      "Epoch 6, Batch 270, train loss 0.03838761895895004\n",
      "Epoch 6, Batch 280, train loss 0.03949768468737602\n",
      "Epoch 6, Batch 290, train loss 0.04472724348306656\n",
      "Epoch 6, Batch 300, train loss 0.04037528112530708\n",
      "Epoch 6, Batch 310, train loss 0.04633667692542076\n",
      "Epoch 6, Batch 320, train loss 0.032900191843509674\n",
      "Epoch 6, Batch 330, train loss 0.04644232243299484\n",
      "Epoch 6, Batch 340, train loss 0.045094847679138184\n",
      "Epoch 6, Batch 350, train loss 0.04246286675333977\n",
      "Epoch 6, Batch 360, train loss 0.04609636962413788\n",
      "Epoch 6, Batch 370, train loss 0.04601484537124634\n",
      "Epoch 6, Batch 380, train loss 0.04024512693285942\n",
      "Epoch 6, Batch 390, train loss 0.04455365240573883\n",
      "> Epoch 6, train loss 0.04134200058317177\n",
      "> Epoch 6, val loss 0.04170410517574281, accuracy 0.6069767441860465, f1_score 0.6057141100248317\n",
      "No improvement.\n",
      "Epoch 7, Batch 0, train loss 0.040904682129621506\n",
      "Epoch 7, Batch 10, train loss 0.04046519100666046\n",
      "Epoch 7, Batch 20, train loss 0.041300591081380844\n",
      "Epoch 7, Batch 30, train loss 0.046540237963199615\n",
      "Epoch 7, Batch 40, train loss 0.03276282921433449\n",
      "Epoch 7, Batch 50, train loss 0.046976055949926376\n",
      "Epoch 7, Batch 60, train loss 0.03540010750293732\n",
      "Epoch 7, Batch 70, train loss 0.04527890682220459\n",
      "Epoch 7, Batch 80, train loss 0.035892926156520844\n",
      "Epoch 7, Batch 90, train loss 0.04133814200758934\n",
      "Epoch 7, Batch 100, train loss 0.03927616402506828\n",
      "Epoch 7, Batch 110, train loss 0.043770767748355865\n",
      "Epoch 7, Batch 120, train loss 0.03597412258386612\n",
      "Epoch 7, Batch 130, train loss 0.04066981375217438\n",
      "Epoch 7, Batch 140, train loss 0.03159399330615997\n",
      "Epoch 7, Batch 150, train loss 0.03251329064369202\n",
      "Epoch 7, Batch 160, train loss 0.040776997804641724\n",
      "Epoch 7, Batch 170, train loss 0.04205331206321716\n",
      "Epoch 7, Batch 180, train loss 0.04064889997243881\n",
      "Epoch 7, Batch 190, train loss 0.03882270306348801\n",
      "Epoch 7, Batch 200, train loss 0.04458334296941757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Batch 210, train loss 0.04546167701482773\n",
      "Epoch 7, Batch 220, train loss 0.03904763236641884\n",
      "Epoch 7, Batch 230, train loss 0.03997502475976944\n",
      "Epoch 7, Batch 240, train loss 0.04048467427492142\n",
      "Epoch 7, Batch 250, train loss 0.03991119936108589\n",
      "Epoch 7, Batch 260, train loss 0.039404552429914474\n",
      "Epoch 7, Batch 270, train loss 0.04532230645418167\n",
      "Epoch 7, Batch 280, train loss 0.04087156429886818\n",
      "Epoch 7, Batch 290, train loss 0.04074954614043236\n",
      "Epoch 7, Batch 300, train loss 0.04314976558089256\n",
      "Epoch 7, Batch 310, train loss 0.040833987295627594\n",
      "Epoch 7, Batch 320, train loss 0.0402429923415184\n",
      "Epoch 7, Batch 330, train loss 0.04056434705853462\n",
      "Epoch 7, Batch 340, train loss 0.033670179545879364\n",
      "Epoch 7, Batch 350, train loss 0.04725867882370949\n",
      "Epoch 7, Batch 360, train loss 0.04031580686569214\n",
      "Epoch 7, Batch 370, train loss 0.045339856296777725\n",
      "Epoch 7, Batch 380, train loss 0.04327091947197914\n",
      "Epoch 7, Batch 390, train loss 0.037514932453632355\n",
      "> Epoch 7, train loss 0.04096585514510484\n",
      "> Epoch 7, val loss 0.04232705494230108, accuracy 0.5728682170542636, f1_score 0.5325161678151138\n",
      "No improvement.\n",
      "Epoch 8, Batch 0, train loss 0.05002991110086441\n",
      "Epoch 8, Batch 10, train loss 0.03973079472780228\n",
      "Epoch 8, Batch 20, train loss 0.041702426970005035\n",
      "Epoch 8, Batch 30, train loss 0.04392482340335846\n",
      "Epoch 8, Batch 40, train loss 0.03985150530934334\n",
      "Epoch 8, Batch 50, train loss 0.04574605077505112\n",
      "Epoch 8, Batch 60, train loss 0.037755854427814484\n",
      "Epoch 8, Batch 70, train loss 0.0386727936565876\n",
      "Epoch 8, Batch 80, train loss 0.0379282683134079\n",
      "Epoch 8, Batch 90, train loss 0.03796488046646118\n",
      "Epoch 8, Batch 100, train loss 0.032195888459682465\n",
      "Epoch 8, Batch 110, train loss 0.03803906589746475\n",
      "Epoch 8, Batch 120, train loss 0.03527856990695\n",
      "Epoch 8, Batch 130, train loss 0.03376103565096855\n",
      "Epoch 8, Batch 140, train loss 0.04070047661662102\n",
      "Epoch 8, Batch 150, train loss 0.039772115647792816\n",
      "Epoch 8, Batch 160, train loss 0.04806758090853691\n",
      "Epoch 8, Batch 170, train loss 0.04597651958465576\n",
      "Epoch 8, Batch 180, train loss 0.03955785185098648\n",
      "Epoch 8, Batch 190, train loss 0.043578967452049255\n",
      "Epoch 8, Batch 200, train loss 0.04485545679926872\n",
      "Epoch 8, Batch 210, train loss 0.04318908974528313\n",
      "Epoch 8, Batch 220, train loss 0.04212992638349533\n",
      "Epoch 8, Batch 230, train loss 0.03971434757113457\n",
      "Epoch 8, Batch 240, train loss 0.037001535296440125\n",
      "Epoch 8, Batch 250, train loss 0.03854646906256676\n",
      "Epoch 8, Batch 260, train loss 0.04254066199064255\n",
      "Epoch 8, Batch 270, train loss 0.041027359664440155\n",
      "Epoch 8, Batch 280, train loss 0.03789190948009491\n",
      "Epoch 8, Batch 290, train loss 0.04916198551654816\n",
      "Epoch 8, Batch 300, train loss 0.03793811798095703\n",
      "Epoch 8, Batch 310, train loss 0.03469732403755188\n",
      "Epoch 8, Batch 320, train loss 0.0341598279774189\n",
      "Epoch 8, Batch 330, train loss 0.04026414453983307\n",
      "Epoch 8, Batch 340, train loss 0.03986772894859314\n",
      "Epoch 8, Batch 350, train loss 0.039239995181560516\n",
      "Epoch 8, Batch 360, train loss 0.03903985396027565\n",
      "Epoch 8, Batch 370, train loss 0.03696516156196594\n",
      "Epoch 8, Batch 380, train loss 0.04182559624314308\n",
      "Epoch 8, Batch 390, train loss 0.04055236279964447\n",
      "> Epoch 8, train loss 0.04061004933396781\n",
      "> Epoch 8, val loss 0.04702429179997407, accuracy 0.5744186046511628, f1_score 0.5083141773890199\n",
      "No improvement.\n",
      "Epoch 9, Batch 0, train loss 0.0292424988001585\n",
      "Epoch 9, Batch 10, train loss 0.040147095918655396\n",
      "Epoch 9, Batch 20, train loss 0.043291643261909485\n",
      "Epoch 9, Batch 30, train loss 0.04023844748735428\n",
      "Epoch 9, Batch 40, train loss 0.04375544190406799\n",
      "Epoch 9, Batch 50, train loss 0.036900389939546585\n",
      "Epoch 9, Batch 60, train loss 0.03739947825670242\n",
      "Epoch 9, Batch 70, train loss 0.044147901237010956\n",
      "Epoch 9, Batch 80, train loss 0.0412132628262043\n",
      "Epoch 9, Batch 90, train loss 0.040138691663742065\n",
      "Epoch 9, Batch 100, train loss 0.038932621479034424\n",
      "Epoch 9, Batch 110, train loss 0.04319930821657181\n",
      "Epoch 9, Batch 120, train loss 0.0448705218732357\n",
      "Epoch 9, Batch 130, train loss 0.04145577922463417\n",
      "Epoch 9, Batch 140, train loss 0.04258979856967926\n",
      "Epoch 9, Batch 150, train loss 0.03181913495063782\n",
      "Epoch 9, Batch 160, train loss 0.033650413155555725\n",
      "Epoch 9, Batch 170, train loss 0.04089412838220596\n",
      "Epoch 9, Batch 180, train loss 0.03475527465343475\n",
      "Epoch 9, Batch 190, train loss 0.0376335084438324\n",
      "Epoch 9, Batch 200, train loss 0.03818018361926079\n",
      "Epoch 9, Batch 210, train loss 0.0473516583442688\n",
      "Epoch 9, Batch 220, train loss 0.03480733186006546\n",
      "Epoch 9, Batch 230, train loss 0.042046964168548584\n",
      "Epoch 9, Batch 240, train loss 0.04266728088259697\n",
      "Epoch 9, Batch 250, train loss 0.044550973922014236\n",
      "Epoch 9, Batch 260, train loss 0.035701360553503036\n",
      "Epoch 9, Batch 270, train loss 0.0363907665014267\n",
      "Epoch 9, Batch 280, train loss 0.03447417542338371\n",
      "Epoch 9, Batch 290, train loss 0.0360417366027832\n",
      "Epoch 9, Batch 300, train loss 0.04274603724479675\n",
      "Epoch 9, Batch 310, train loss 0.040217988193035126\n",
      "Epoch 9, Batch 320, train loss 0.04080952703952789\n",
      "Epoch 9, Batch 330, train loss 0.03860614448785782\n",
      "Epoch 9, Batch 340, train loss 0.04277094453573227\n",
      "Epoch 9, Batch 350, train loss 0.03055492788553238\n",
      "Epoch 9, Batch 360, train loss 0.044280003756284714\n",
      "Epoch 9, Batch 370, train loss 0.04034297168254852\n",
      "Epoch 9, Batch 380, train loss 0.036096274852752686\n",
      "Epoch 9, Batch 390, train loss 0.03844538331031799\n",
      "> Epoch 9, train loss 0.04067600135188289\n",
      "> Epoch 9, val loss 0.040252177937086235, accuracy 0.637984496124031, f1_score 0.6379738361384357\n",
      "Saved model.\n",
      "Epoch 10, Batch 0, train loss 0.04146980121731758\n",
      "Epoch 10, Batch 10, train loss 0.02878720313310623\n",
      "Epoch 10, Batch 20, train loss 0.04369765520095825\n",
      "Epoch 10, Batch 30, train loss 0.04261201620101929\n",
      "Epoch 10, Batch 40, train loss 0.03712702542543411\n",
      "Epoch 10, Batch 50, train loss 0.03742557764053345\n",
      "Epoch 10, Batch 60, train loss 0.04783841222524643\n",
      "Epoch 10, Batch 70, train loss 0.039831582456827164\n",
      "Epoch 10, Batch 80, train loss 0.03633269667625427\n",
      "Epoch 10, Batch 90, train loss 0.04346851631999016\n",
      "Epoch 10, Batch 100, train loss 0.037557125091552734\n",
      "Epoch 10, Batch 110, train loss 0.03657270222902298\n",
      "Epoch 10, Batch 120, train loss 0.05310829356312752\n",
      "Epoch 10, Batch 130, train loss 0.03254135698080063\n",
      "Epoch 10, Batch 140, train loss 0.050611987709999084\n",
      "Epoch 10, Batch 150, train loss 0.031264275312423706\n",
      "Epoch 10, Batch 160, train loss 0.0384223610162735\n",
      "Epoch 10, Batch 170, train loss 0.03116470016539097\n",
      "Epoch 10, Batch 180, train loss 0.04186008498072624\n",
      "Epoch 10, Batch 190, train loss 0.04463740438222885\n",
      "Epoch 10, Batch 200, train loss 0.046789396554231644\n",
      "Epoch 10, Batch 210, train loss 0.045410171151161194\n",
      "Epoch 10, Batch 220, train loss 0.05111469328403473\n",
      "Epoch 10, Batch 230, train loss 0.03326694667339325\n",
      "Epoch 10, Batch 240, train loss 0.03704618290066719\n",
      "Epoch 10, Batch 250, train loss 0.030577100813388824\n",
      "Epoch 10, Batch 260, train loss 0.0367603600025177\n",
      "Epoch 10, Batch 270, train loss 0.04203319549560547\n",
      "Epoch 10, Batch 280, train loss 0.04024659842252731\n",
      "Epoch 10, Batch 290, train loss 0.04078398644924164\n",
      "Epoch 10, Batch 300, train loss 0.03845663741230965\n",
      "Epoch 10, Batch 310, train loss 0.042805641889572144\n",
      "Epoch 10, Batch 320, train loss 0.03592047840356827\n",
      "Epoch 10, Batch 330, train loss 0.03413042053580284\n",
      "Epoch 10, Batch 340, train loss 0.039039187133312225\n",
      "Epoch 10, Batch 350, train loss 0.03447308763861656\n",
      "Epoch 10, Batch 360, train loss 0.04139263927936554\n",
      "Epoch 10, Batch 370, train loss 0.0420229509472847\n",
      "Epoch 10, Batch 380, train loss 0.0444122776389122\n",
      "Epoch 10, Batch 390, train loss 0.04132328927516937\n",
      "> Epoch 10, train loss 0.039922247169033954\n",
      "> Epoch 10, val loss 0.04132292640301608, accuracy 0.624031007751938, f1_score 0.6233463269268396\n",
      "No improvement.\n",
      "Epoch 11, Batch 0, train loss 0.03755045682191849\n",
      "Epoch 11, Batch 10, train loss 0.03857474401593208\n",
      "Epoch 11, Batch 20, train loss 0.037934791296720505\n",
      "Epoch 11, Batch 30, train loss 0.03614708408713341\n",
      "Epoch 11, Batch 40, train loss 0.04095796123147011\n",
      "Epoch 11, Batch 50, train loss 0.03915392607450485\n",
      "Epoch 11, Batch 60, train loss 0.035072192549705505\n",
      "Epoch 11, Batch 70, train loss 0.04409242421388626\n",
      "Epoch 11, Batch 80, train loss 0.0429663360118866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Batch 90, train loss 0.0431549996137619\n",
      "Epoch 11, Batch 100, train loss 0.046392254531383514\n",
      "Epoch 11, Batch 110, train loss 0.03976084291934967\n",
      "Epoch 11, Batch 120, train loss 0.042561084032058716\n",
      "Epoch 11, Batch 130, train loss 0.04505384713411331\n",
      "Epoch 11, Batch 140, train loss 0.03992954641580582\n",
      "Epoch 11, Batch 150, train loss 0.0401659831404686\n",
      "Epoch 11, Batch 160, train loss 0.036695703864097595\n",
      "Epoch 11, Batch 170, train loss 0.043377723544836044\n",
      "Epoch 11, Batch 180, train loss 0.03415998816490173\n",
      "Epoch 11, Batch 190, train loss 0.05077485367655754\n",
      "Epoch 11, Batch 200, train loss 0.04095795005559921\n",
      "Epoch 11, Batch 210, train loss 0.041418932378292084\n",
      "Epoch 11, Batch 220, train loss 0.043778009712696075\n",
      "Epoch 11, Batch 230, train loss 0.03911814093589783\n",
      "Epoch 11, Batch 240, train loss 0.0319441482424736\n",
      "Epoch 11, Batch 250, train loss 0.04329684376716614\n",
      "Epoch 11, Batch 260, train loss 0.030780859291553497\n",
      "Epoch 11, Batch 270, train loss 0.04750007390975952\n",
      "Epoch 11, Batch 280, train loss 0.036919381469488144\n",
      "Epoch 11, Batch 290, train loss 0.041167836636304855\n",
      "Epoch 11, Batch 300, train loss 0.042263373732566833\n",
      "Epoch 11, Batch 310, train loss 0.03653290867805481\n",
      "Epoch 11, Batch 320, train loss 0.04332752898335457\n",
      "Epoch 11, Batch 330, train loss 0.0403275340795517\n",
      "Epoch 11, Batch 340, train loss 0.044042930006980896\n",
      "Epoch 11, Batch 350, train loss 0.04174032807350159\n",
      "Epoch 11, Batch 360, train loss 0.03967663645744324\n",
      "Epoch 11, Batch 370, train loss 0.048992305994033813\n",
      "Epoch 11, Batch 380, train loss 0.049250148236751556\n",
      "Epoch 11, Batch 390, train loss 0.03640284761786461\n",
      "> Epoch 11, train loss 0.039785532654036525\n",
      "> Epoch 11, val loss 0.040215916157692905, accuracy 0.6356589147286822, f1_score 0.6214410589410588\n",
      "Saved model.\n",
      "Epoch 12, Batch 0, train loss 0.036932073533535004\n",
      "Epoch 12, Batch 10, train loss 0.04420171678066254\n",
      "Epoch 12, Batch 20, train loss 0.04261849820613861\n",
      "Epoch 12, Batch 30, train loss 0.04262039065361023\n",
      "Epoch 12, Batch 40, train loss 0.04477858543395996\n",
      "Epoch 12, Batch 50, train loss 0.04132649302482605\n",
      "Epoch 12, Batch 60, train loss 0.044424235820770264\n",
      "Epoch 12, Batch 70, train loss 0.041752204298973083\n",
      "Epoch 12, Batch 80, train loss 0.04102777689695358\n",
      "Epoch 12, Batch 90, train loss 0.03748028725385666\n",
      "Epoch 12, Batch 100, train loss 0.034893572330474854\n",
      "Epoch 12, Batch 110, train loss 0.04615156725049019\n",
      "Epoch 12, Batch 120, train loss 0.031155524775385857\n",
      "Epoch 12, Batch 130, train loss 0.0414135679602623\n",
      "Epoch 12, Batch 140, train loss 0.03274780511856079\n",
      "Epoch 12, Batch 150, train loss 0.041154056787490845\n",
      "Epoch 12, Batch 160, train loss 0.03616047278046608\n",
      "Epoch 12, Batch 170, train loss 0.04727908968925476\n",
      "Epoch 12, Batch 180, train loss 0.03687988966703415\n",
      "Epoch 12, Batch 190, train loss 0.02488104999065399\n",
      "Epoch 12, Batch 200, train loss 0.036618757992982864\n",
      "Epoch 12, Batch 210, train loss 0.04388004168868065\n",
      "Epoch 12, Batch 220, train loss 0.04494862258434296\n",
      "Epoch 12, Batch 230, train loss 0.046545758843421936\n",
      "Epoch 12, Batch 240, train loss 0.04665178805589676\n",
      "Epoch 12, Batch 250, train loss 0.038866519927978516\n",
      "Epoch 12, Batch 260, train loss 0.035961493849754333\n",
      "Epoch 12, Batch 270, train loss 0.03658226504921913\n",
      "Epoch 12, Batch 280, train loss 0.036470167338848114\n",
      "Epoch 12, Batch 290, train loss 0.04456212744116783\n",
      "Epoch 12, Batch 300, train loss 0.030222691595554352\n",
      "Epoch 12, Batch 310, train loss 0.035991787910461426\n",
      "Epoch 12, Batch 320, train loss 0.03208072856068611\n",
      "Epoch 12, Batch 330, train loss 0.037955932319164276\n",
      "Epoch 12, Batch 340, train loss 0.02835359238088131\n",
      "Epoch 12, Batch 350, train loss 0.030837979167699814\n",
      "Epoch 12, Batch 360, train loss 0.037592630833387375\n",
      "Epoch 12, Batch 370, train loss 0.0353565514087677\n",
      "Epoch 12, Batch 380, train loss 0.044518619775772095\n",
      "Epoch 12, Batch 390, train loss 0.02653072029352188\n",
      "> Epoch 12, train loss 0.03922397875549025\n",
      "> Epoch 12, val loss 0.040142932788346165, accuracy 0.6286821705426356, f1_score 0.6274227829783385\n",
      "Saved model.\n",
      "Epoch 13, Batch 0, train loss 0.04111144691705704\n",
      "Epoch 13, Batch 10, train loss 0.05537569895386696\n",
      "Epoch 13, Batch 20, train loss 0.03581290692090988\n",
      "Epoch 13, Batch 30, train loss 0.03173653036355972\n",
      "Epoch 13, Batch 40, train loss 0.03345906734466553\n",
      "Epoch 13, Batch 50, train loss 0.040776364505290985\n",
      "Epoch 13, Batch 60, train loss 0.04022754728794098\n",
      "Epoch 13, Batch 70, train loss 0.03543791174888611\n",
      "Epoch 13, Batch 80, train loss 0.04252611845731735\n",
      "Epoch 13, Batch 90, train loss 0.04306936636567116\n",
      "Epoch 13, Batch 100, train loss 0.034405093640089035\n",
      "Epoch 13, Batch 110, train loss 0.031128698959946632\n",
      "Epoch 13, Batch 120, train loss 0.04442425072193146\n",
      "Epoch 13, Batch 130, train loss 0.05173620581626892\n",
      "Epoch 13, Batch 140, train loss 0.04030275344848633\n",
      "Epoch 13, Batch 150, train loss 0.03879588097333908\n",
      "Epoch 13, Batch 160, train loss 0.030803009867668152\n",
      "Epoch 13, Batch 170, train loss 0.033879801630973816\n",
      "Epoch 13, Batch 180, train loss 0.04492715001106262\n",
      "Epoch 13, Batch 190, train loss 0.045413538813591\n",
      "Epoch 13, Batch 200, train loss 0.042377784848213196\n",
      "Epoch 13, Batch 210, train loss 0.03499019145965576\n",
      "Epoch 13, Batch 220, train loss 0.04956400394439697\n",
      "Epoch 13, Batch 230, train loss 0.02913232520222664\n",
      "Epoch 13, Batch 240, train loss 0.03458847850561142\n",
      "Epoch 13, Batch 250, train loss 0.0372651144862175\n",
      "Epoch 13, Batch 260, train loss 0.03726349025964737\n",
      "Epoch 13, Batch 270, train loss 0.0461510568857193\n",
      "Epoch 13, Batch 280, train loss 0.036952245980501175\n",
      "Epoch 13, Batch 290, train loss 0.03939015418291092\n",
      "Epoch 13, Batch 300, train loss 0.03748680651187897\n",
      "Epoch 13, Batch 310, train loss 0.034246109426021576\n",
      "Epoch 13, Batch 320, train loss 0.03624996915459633\n",
      "Epoch 13, Batch 330, train loss 0.03133009374141693\n",
      "Epoch 13, Batch 340, train loss 0.040508490055799484\n",
      "Epoch 13, Batch 350, train loss 0.050256747752428055\n",
      "Epoch 13, Batch 360, train loss 0.03940162807703018\n",
      "Epoch 13, Batch 370, train loss 0.03437045216560364\n",
      "Epoch 13, Batch 380, train loss 0.04564949870109558\n",
      "Epoch 13, Batch 390, train loss 0.03559635579586029\n",
      "> Epoch 13, train loss 0.03917321352745933\n",
      "> Epoch 13, val loss 0.039927358913791275, accuracy 0.6286821705426356, f1_score 0.6212252634185782\n",
      "Saved model.\n",
      "Epoch 14, Batch 0, train loss 0.04104957729578018\n",
      "Epoch 14, Batch 10, train loss 0.036252059042453766\n",
      "Epoch 14, Batch 20, train loss 0.031095974147319794\n",
      "Epoch 14, Batch 30, train loss 0.0382252037525177\n",
      "Epoch 14, Batch 40, train loss 0.036297768354415894\n",
      "Epoch 14, Batch 50, train loss 0.04177471995353699\n",
      "Epoch 14, Batch 60, train loss 0.05143693834543228\n",
      "Epoch 14, Batch 70, train loss 0.03797955438494682\n",
      "Epoch 14, Batch 80, train loss 0.03029477596282959\n",
      "Epoch 14, Batch 90, train loss 0.03172136843204498\n",
      "Epoch 14, Batch 100, train loss 0.049065813422203064\n",
      "Epoch 14, Batch 110, train loss 0.03159753605723381\n",
      "Epoch 14, Batch 120, train loss 0.04003879427909851\n",
      "Epoch 14, Batch 130, train loss 0.02798699587583542\n",
      "Epoch 14, Batch 140, train loss 0.03416711837053299\n",
      "Epoch 14, Batch 150, train loss 0.03929980844259262\n",
      "Epoch 14, Batch 160, train loss 0.03152856230735779\n",
      "Epoch 14, Batch 170, train loss 0.04141862690448761\n",
      "Epoch 14, Batch 180, train loss 0.03851582109928131\n",
      "Epoch 14, Batch 190, train loss 0.05111156031489372\n",
      "Epoch 14, Batch 200, train loss 0.04403844103217125\n",
      "Epoch 14, Batch 210, train loss 0.04422738775610924\n",
      "Epoch 14, Batch 220, train loss 0.03953700140118599\n",
      "Epoch 14, Batch 230, train loss 0.034175679087638855\n",
      "Epoch 14, Batch 240, train loss 0.03280320018529892\n",
      "Epoch 14, Batch 250, train loss 0.039452046155929565\n",
      "Epoch 14, Batch 260, train loss 0.03217410296201706\n",
      "Epoch 14, Batch 270, train loss 0.03737325966358185\n",
      "Epoch 14, Batch 280, train loss 0.03304663673043251\n",
      "Epoch 14, Batch 290, train loss 0.03972092643380165\n",
      "Epoch 14, Batch 300, train loss 0.034848086535930634\n",
      "Epoch 14, Batch 310, train loss 0.03829822689294815\n",
      "Epoch 14, Batch 320, train loss 0.03976549208164215\n",
      "Epoch 14, Batch 330, train loss 0.033178724348545074\n",
      "Epoch 14, Batch 340, train loss 0.03345588967204094\n",
      "Epoch 14, Batch 350, train loss 0.048498380929231644\n",
      "Epoch 14, Batch 360, train loss 0.03674114868044853\n",
      "Epoch 14, Batch 370, train loss 0.0376957431435585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Batch 380, train loss 0.04482386261224747\n",
      "Epoch 14, Batch 390, train loss 0.03922957554459572\n",
      "> Epoch 14, train loss 0.03877400716697119\n",
      "> Epoch 14, val loss 0.0394467027381409, accuracy 0.6596899224806202, f1_score 0.6584731007839361\n",
      "Saved model.\n",
      "Epoch 15, Batch 0, train loss 0.03839360922574997\n",
      "Epoch 15, Batch 10, train loss 0.04142832010984421\n",
      "Epoch 15, Batch 20, train loss 0.03975613787770271\n",
      "Epoch 15, Batch 30, train loss 0.04663298279047012\n",
      "Epoch 15, Batch 40, train loss 0.03539949655532837\n",
      "Epoch 15, Batch 50, train loss 0.03825460374355316\n",
      "Epoch 15, Batch 60, train loss 0.0406566858291626\n",
      "Epoch 15, Batch 70, train loss 0.04677731543779373\n",
      "Epoch 15, Batch 80, train loss 0.038029931485652924\n",
      "Epoch 15, Batch 90, train loss 0.03449748456478119\n",
      "Epoch 15, Batch 100, train loss 0.040652282536029816\n",
      "Epoch 15, Batch 110, train loss 0.0330142006278038\n",
      "Epoch 15, Batch 120, train loss 0.03748701512813568\n",
      "Epoch 15, Batch 130, train loss 0.041344430297613144\n",
      "Epoch 15, Batch 140, train loss 0.026988806203007698\n",
      "Epoch 15, Batch 150, train loss 0.0402790829539299\n",
      "Epoch 15, Batch 160, train loss 0.040342748165130615\n",
      "Epoch 15, Batch 170, train loss 0.03874281793832779\n",
      "Epoch 15, Batch 180, train loss 0.04114864766597748\n",
      "Epoch 15, Batch 190, train loss 0.037207651883363724\n",
      "Epoch 15, Batch 200, train loss 0.03275479003787041\n",
      "Epoch 15, Batch 210, train loss 0.052094973623752594\n",
      "Epoch 15, Batch 220, train loss 0.040544163435697556\n",
      "Epoch 15, Batch 230, train loss 0.035413406789302826\n",
      "Epoch 15, Batch 240, train loss 0.03507968783378601\n",
      "Epoch 15, Batch 250, train loss 0.035914428532123566\n",
      "Epoch 15, Batch 260, train loss 0.0452774278819561\n",
      "Epoch 15, Batch 270, train loss 0.03771229088306427\n",
      "Epoch 15, Batch 280, train loss 0.03510843217372894\n",
      "Epoch 15, Batch 290, train loss 0.03683500364422798\n",
      "Epoch 15, Batch 300, train loss 0.02993844635784626\n",
      "Epoch 15, Batch 310, train loss 0.03648294508457184\n",
      "Epoch 15, Batch 320, train loss 0.04278017207980156\n",
      "Epoch 15, Batch 330, train loss 0.030906815081834793\n",
      "Epoch 15, Batch 340, train loss 0.03315092995762825\n",
      "Epoch 15, Batch 350, train loss 0.03337014466524124\n",
      "Epoch 15, Batch 360, train loss 0.042520537972450256\n",
      "Epoch 15, Batch 370, train loss 0.042748432606458664\n",
      "Epoch 15, Batch 380, train loss 0.04075684770941734\n",
      "Epoch 15, Batch 390, train loss 0.03579944372177124\n",
      "> Epoch 15, train loss 0.038298812646247904\n",
      "> Epoch 15, val loss 0.03894065533035485, accuracy 0.6449612403100775, f1_score 0.6442909057631592\n",
      "Saved model.\n",
      "Epoch 16, Batch 0, train loss 0.036415763199329376\n",
      "Epoch 16, Batch 10, train loss 0.0348932221531868\n",
      "Epoch 16, Batch 20, train loss 0.04066871106624603\n",
      "Epoch 16, Batch 30, train loss 0.032834991812705994\n",
      "Epoch 16, Batch 40, train loss 0.040804989635944366\n",
      "Epoch 16, Batch 50, train loss 0.04080771654844284\n",
      "Epoch 16, Batch 60, train loss 0.04370734840631485\n",
      "Epoch 16, Batch 70, train loss 0.03243507444858551\n",
      "Epoch 16, Batch 80, train loss 0.04683324694633484\n",
      "Epoch 16, Batch 90, train loss 0.048683051019907\n",
      "Epoch 16, Batch 100, train loss 0.04019208252429962\n",
      "Epoch 16, Batch 110, train loss 0.03466250002384186\n",
      "Epoch 16, Batch 120, train loss 0.028482291847467422\n",
      "Epoch 16, Batch 130, train loss 0.03136751428246498\n",
      "Epoch 16, Batch 140, train loss 0.033284857869148254\n",
      "Epoch 16, Batch 150, train loss 0.043412644416093826\n",
      "Epoch 16, Batch 160, train loss 0.035329341888427734\n",
      "Epoch 16, Batch 170, train loss 0.03253115713596344\n",
      "Epoch 16, Batch 180, train loss 0.05755601450800896\n",
      "Epoch 16, Batch 190, train loss 0.04281477630138397\n",
      "Epoch 16, Batch 200, train loss 0.04139459878206253\n",
      "Epoch 16, Batch 210, train loss 0.035495005548000336\n",
      "Epoch 16, Batch 220, train loss 0.045740049332380295\n",
      "Epoch 16, Batch 230, train loss 0.03722619265317917\n",
      "Epoch 16, Batch 240, train loss 0.03874334692955017\n",
      "Epoch 16, Batch 250, train loss 0.03911720961332321\n",
      "Epoch 16, Batch 260, train loss 0.0413353331387043\n",
      "Epoch 16, Batch 270, train loss 0.03641089051961899\n",
      "Epoch 16, Batch 280, train loss 0.03702196106314659\n",
      "Epoch 16, Batch 290, train loss 0.04162716120481491\n",
      "Epoch 16, Batch 300, train loss 0.03488659858703613\n",
      "Epoch 16, Batch 310, train loss 0.03585478663444519\n",
      "Epoch 16, Batch 320, train loss 0.030168280005455017\n",
      "Epoch 16, Batch 330, train loss 0.039794258773326874\n",
      "Epoch 16, Batch 340, train loss 0.03745345026254654\n",
      "Epoch 16, Batch 350, train loss 0.03788436949253082\n",
      "Epoch 16, Batch 360, train loss 0.03921770304441452\n",
      "Epoch 16, Batch 370, train loss 0.046673811972141266\n",
      "Epoch 16, Batch 380, train loss 0.03996073454618454\n",
      "Epoch 16, Batch 390, train loss 0.03265787288546562\n",
      "> Epoch 16, train loss 0.0378907061461374\n",
      "> Epoch 16, val loss 0.0405966719453649, accuracy 0.6356589147286822, f1_score 0.6202317569683683\n",
      "No improvement.\n",
      "Epoch 17, Batch 0, train loss 0.047564007341861725\n",
      "Epoch 17, Batch 10, train loss 0.0429922454059124\n",
      "Epoch 17, Batch 20, train loss 0.04018666595220566\n",
      "Epoch 17, Batch 30, train loss 0.02490195445716381\n",
      "Epoch 17, Batch 40, train loss 0.04040415585041046\n",
      "Epoch 17, Batch 50, train loss 0.030085548758506775\n",
      "Epoch 17, Batch 60, train loss 0.033316291868686676\n",
      "Epoch 17, Batch 70, train loss 0.028923962265253067\n",
      "Epoch 17, Batch 80, train loss 0.03865595906972885\n",
      "Epoch 17, Batch 90, train loss 0.04118058830499649\n",
      "Epoch 17, Batch 100, train loss 0.04153435304760933\n",
      "Epoch 17, Batch 110, train loss 0.041762761771678925\n",
      "Epoch 17, Batch 120, train loss 0.03986590355634689\n",
      "Epoch 17, Batch 130, train loss 0.03292778134346008\n",
      "Epoch 17, Batch 140, train loss 0.034987084567546844\n",
      "Epoch 17, Batch 150, train loss 0.03837566450238228\n",
      "Epoch 17, Batch 160, train loss 0.03433775156736374\n",
      "Epoch 17, Batch 170, train loss 0.03833916038274765\n",
      "Epoch 17, Batch 180, train loss 0.03706318140029907\n",
      "Epoch 17, Batch 190, train loss 0.04302387312054634\n",
      "Epoch 17, Batch 200, train loss 0.04378752410411835\n",
      "Epoch 17, Batch 210, train loss 0.04483799636363983\n",
      "Epoch 17, Batch 220, train loss 0.03830236941576004\n",
      "Epoch 17, Batch 230, train loss 0.03494179993867874\n",
      "Epoch 17, Batch 240, train loss 0.04287594184279442\n",
      "Epoch 17, Batch 250, train loss 0.03303970396518707\n",
      "Epoch 17, Batch 260, train loss 0.02845243364572525\n",
      "Epoch 17, Batch 270, train loss 0.03508709371089935\n",
      "Epoch 17, Batch 280, train loss 0.04131819307804108\n",
      "Epoch 17, Batch 290, train loss 0.039919037371873856\n",
      "Epoch 17, Batch 300, train loss 0.04564676433801651\n",
      "Epoch 17, Batch 310, train loss 0.045139431953430176\n",
      "Epoch 17, Batch 320, train loss 0.033772893249988556\n",
      "Epoch 17, Batch 330, train loss 0.03962836414575577\n",
      "Epoch 17, Batch 340, train loss 0.035576026886701584\n",
      "Epoch 17, Batch 350, train loss 0.02881002239882946\n",
      "Epoch 17, Batch 360, train loss 0.043044161051511765\n",
      "Epoch 17, Batch 370, train loss 0.036408111453056335\n",
      "Epoch 17, Batch 380, train loss 0.036428436636924744\n",
      "Epoch 17, Batch 390, train loss 0.0488288551568985\n",
      "> Epoch 17, train loss 0.03804236332851123\n",
      "> Epoch 17, val loss 0.039520978026611864, accuracy 0.6410852713178294, f1_score 0.6386915294224399\n",
      "No improvement.\n",
      "Epoch 18, Batch 0, train loss 0.03630959242582321\n",
      "Epoch 18, Batch 10, train loss 0.034054115414619446\n",
      "Epoch 18, Batch 20, train loss 0.03709549456834793\n",
      "Epoch 18, Batch 30, train loss 0.03638434410095215\n",
      "Epoch 18, Batch 40, train loss 0.03418614715337753\n",
      "Epoch 18, Batch 50, train loss 0.04248452186584473\n",
      "Epoch 18, Batch 60, train loss 0.043711595237255096\n",
      "Epoch 18, Batch 70, train loss 0.03397078439593315\n",
      "Epoch 18, Batch 80, train loss 0.037299130111932755\n",
      "Epoch 18, Batch 90, train loss 0.04059024527668953\n",
      "Epoch 18, Batch 100, train loss 0.029664834961295128\n",
      "Epoch 18, Batch 110, train loss 0.0423014760017395\n",
      "Epoch 18, Batch 120, train loss 0.03887879103422165\n",
      "Epoch 18, Batch 130, train loss 0.038870252668857574\n",
      "Epoch 18, Batch 140, train loss 0.04042397439479828\n",
      "Epoch 18, Batch 150, train loss 0.04455604404211044\n",
      "Epoch 18, Batch 160, train loss 0.04294796288013458\n",
      "Epoch 18, Batch 170, train loss 0.03464127331972122\n",
      "Epoch 18, Batch 180, train loss 0.028675295412540436\n",
      "Epoch 18, Batch 190, train loss 0.03482238948345184\n",
      "Epoch 18, Batch 200, train loss 0.04102664440870285\n",
      "Epoch 18, Batch 210, train loss 0.028399232774972916\n",
      "Epoch 18, Batch 220, train loss 0.03638145327568054\n",
      "Epoch 18, Batch 230, train loss 0.03350658342242241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Batch 240, train loss 0.0497392937541008\n",
      "Epoch 18, Batch 250, train loss 0.04026898741722107\n",
      "Epoch 18, Batch 260, train loss 0.03902272507548332\n",
      "Epoch 18, Batch 270, train loss 0.04398314654827118\n",
      "Epoch 18, Batch 280, train loss 0.03978273272514343\n",
      "Epoch 18, Batch 290, train loss 0.036171942949295044\n",
      "Epoch 18, Batch 300, train loss 0.03048797696828842\n",
      "Epoch 18, Batch 310, train loss 0.047592706978321075\n",
      "Epoch 18, Batch 320, train loss 0.04620489478111267\n",
      "Epoch 18, Batch 330, train loss 0.039416976273059845\n",
      "Epoch 18, Batch 340, train loss 0.04587762802839279\n",
      "Epoch 18, Batch 350, train loss 0.03659966588020325\n",
      "Epoch 18, Batch 360, train loss 0.03516196832060814\n",
      "Epoch 18, Batch 370, train loss 0.032169945538043976\n",
      "Epoch 18, Batch 380, train loss 0.03982378542423248\n",
      "Epoch 18, Batch 390, train loss 0.04446859657764435\n",
      "> Epoch 18, train loss 0.037709300678347296\n",
      "> Epoch 18, val loss 0.039197813378748045, accuracy 0.6550387596899225, f1_score 0.6549921117872436\n",
      "No improvement.\n",
      "Epoch 19, Batch 0, train loss 0.039564795792102814\n",
      "Epoch 19, Batch 10, train loss 0.04148385673761368\n",
      "Epoch 19, Batch 20, train loss 0.04122620075941086\n",
      "Epoch 19, Batch 30, train loss 0.03361125662922859\n",
      "Epoch 19, Batch 40, train loss 0.03709524869918823\n",
      "Epoch 19, Batch 50, train loss 0.036425501108169556\n",
      "Epoch 19, Batch 60, train loss 0.03351995348930359\n",
      "Epoch 19, Batch 70, train loss 0.022801542654633522\n",
      "Epoch 19, Batch 80, train loss 0.02607758715748787\n",
      "Epoch 19, Batch 90, train loss 0.030330650508403778\n",
      "Epoch 19, Batch 100, train loss 0.03801996260881424\n",
      "Epoch 19, Batch 110, train loss 0.042483165860176086\n",
      "Epoch 19, Batch 120, train loss 0.03381003811955452\n",
      "Epoch 19, Batch 130, train loss 0.03670399636030197\n",
      "Epoch 19, Batch 140, train loss 0.036901652812957764\n",
      "Epoch 19, Batch 150, train loss 0.03733668476343155\n",
      "Epoch 19, Batch 160, train loss 0.035076647996902466\n",
      "Epoch 19, Batch 170, train loss 0.03997365012764931\n",
      "Epoch 19, Batch 180, train loss 0.04271535202860832\n",
      "Epoch 19, Batch 190, train loss 0.0416782982647419\n",
      "Epoch 19, Batch 200, train loss 0.037488169968128204\n",
      "Epoch 19, Batch 210, train loss 0.03632737696170807\n",
      "Epoch 19, Batch 220, train loss 0.0351983904838562\n",
      "Epoch 19, Batch 230, train loss 0.02790592797100544\n",
      "Epoch 19, Batch 240, train loss 0.042783088982105255\n",
      "Epoch 19, Batch 250, train loss 0.034592121839523315\n",
      "Epoch 19, Batch 260, train loss 0.04068390652537346\n",
      "Epoch 19, Batch 270, train loss 0.040686968713998795\n",
      "Epoch 19, Batch 280, train loss 0.04395125061273575\n",
      "Epoch 19, Batch 290, train loss 0.03161639720201492\n",
      "Epoch 19, Batch 300, train loss 0.035449687391519547\n",
      "Epoch 19, Batch 310, train loss 0.03235216066241264\n",
      "Epoch 19, Batch 320, train loss 0.02719080075621605\n",
      "Epoch 19, Batch 330, train loss 0.026416819542646408\n",
      "Epoch 19, Batch 340, train loss 0.03235986828804016\n",
      "Epoch 19, Batch 350, train loss 0.03937274590134621\n",
      "Epoch 19, Batch 360, train loss 0.034845925867557526\n",
      "Epoch 19, Batch 370, train loss 0.04275732487440109\n",
      "Epoch 19, Batch 380, train loss 0.03606259822845459\n",
      "Epoch 19, Batch 390, train loss 0.04096989333629608\n",
      "> Epoch 19, train loss 0.037643053155582105\n",
      "> Epoch 19, val loss 0.03857629444710044, accuracy 0.6689922480620155, f1_score 0.6689920491509219\n",
      "Saved model.\n",
      "Epoch 20, Batch 0, train loss 0.03042757324874401\n",
      "Epoch 20, Batch 10, train loss 0.023585377261042595\n",
      "Epoch 20, Batch 20, train loss 0.03584202751517296\n",
      "Epoch 20, Batch 30, train loss 0.03364436328411102\n",
      "Epoch 20, Batch 40, train loss 0.046560876071453094\n",
      "Epoch 20, Batch 50, train loss 0.0349406898021698\n",
      "Epoch 20, Batch 60, train loss 0.04711593687534332\n",
      "Epoch 20, Batch 70, train loss 0.03767278417944908\n",
      "Epoch 20, Batch 80, train loss 0.03883863985538483\n",
      "Epoch 20, Batch 90, train loss 0.03331509977579117\n",
      "Epoch 20, Batch 100, train loss 0.03338995948433876\n",
      "Epoch 20, Batch 110, train loss 0.03793823719024658\n",
      "Epoch 20, Batch 120, train loss 0.03645782917737961\n",
      "Epoch 20, Batch 130, train loss 0.029401175677776337\n",
      "Epoch 20, Batch 140, train loss 0.046786993741989136\n",
      "Epoch 20, Batch 150, train loss 0.03778449818491936\n",
      "Epoch 20, Batch 160, train loss 0.027801377698779106\n",
      "Epoch 20, Batch 170, train loss 0.029637685045599937\n",
      "Epoch 20, Batch 180, train loss 0.04490784555673599\n",
      "Epoch 20, Batch 190, train loss 0.03678803890943527\n",
      "Epoch 20, Batch 200, train loss 0.027046259492635727\n",
      "Epoch 20, Batch 210, train loss 0.048311881721019745\n",
      "Epoch 20, Batch 220, train loss 0.03990940377116203\n",
      "Epoch 20, Batch 230, train loss 0.0388234481215477\n",
      "Epoch 20, Batch 240, train loss 0.030410150066018105\n",
      "Epoch 20, Batch 250, train loss 0.0371817909181118\n",
      "Epoch 20, Batch 260, train loss 0.033539801836013794\n",
      "Epoch 20, Batch 270, train loss 0.02896096184849739\n",
      "Epoch 20, Batch 280, train loss 0.04106053709983826\n",
      "Epoch 20, Batch 290, train loss 0.03680034726858139\n",
      "Epoch 20, Batch 300, train loss 0.03986690193414688\n",
      "Epoch 20, Batch 310, train loss 0.04443011432886124\n",
      "Epoch 20, Batch 320, train loss 0.04072578251361847\n",
      "Epoch 20, Batch 330, train loss 0.03893566131591797\n",
      "Epoch 20, Batch 340, train loss 0.03131115436553955\n",
      "Epoch 20, Batch 350, train loss 0.04868031293153763\n",
      "Epoch 20, Batch 360, train loss 0.045783158391714096\n",
      "Epoch 20, Batch 370, train loss 0.035971928387880325\n",
      "Epoch 20, Batch 380, train loss 0.04002777487039566\n",
      "Epoch 20, Batch 390, train loss 0.042005814611911774\n",
      "> Epoch 20, train loss 0.0373703276868613\n",
      "> Epoch 20, val loss 0.03824299322080243, accuracy 0.6705426356589147, f1_score 0.6700268853607495\n",
      "Saved model.\n",
      "Epoch 21, Batch 0, train loss 0.036897335201501846\n",
      "Epoch 21, Batch 10, train loss 0.03040294535458088\n",
      "Epoch 21, Batch 20, train loss 0.028375737369060516\n",
      "Epoch 21, Batch 30, train loss 0.04056183248758316\n",
      "Epoch 21, Batch 40, train loss 0.03345850855112076\n",
      "Epoch 21, Batch 50, train loss 0.0435466393828392\n",
      "Epoch 21, Batch 60, train loss 0.03868219256401062\n",
      "Epoch 21, Batch 70, train loss 0.040581852197647095\n",
      "Epoch 21, Batch 80, train loss 0.03252226859331131\n",
      "Epoch 21, Batch 90, train loss 0.05332338064908981\n",
      "Epoch 21, Batch 100, train loss 0.04349182918667793\n",
      "Epoch 21, Batch 110, train loss 0.06395775079727173\n",
      "Epoch 21, Batch 120, train loss 0.03858724236488342\n",
      "Epoch 21, Batch 130, train loss 0.03477255254983902\n",
      "Epoch 21, Batch 140, train loss 0.03870052099227905\n",
      "Epoch 21, Batch 150, train loss 0.0533197782933712\n",
      "Epoch 21, Batch 160, train loss 0.03482009470462799\n",
      "Epoch 21, Batch 170, train loss 0.043859899044036865\n",
      "Epoch 21, Batch 180, train loss 0.03336666524410248\n",
      "Epoch 21, Batch 190, train loss 0.03967589884996414\n",
      "Epoch 21, Batch 200, train loss 0.0356656014919281\n",
      "Epoch 21, Batch 210, train loss 0.0287094097584486\n",
      "Epoch 21, Batch 220, train loss 0.03895456716418266\n",
      "Epoch 21, Batch 230, train loss 0.03741113841533661\n",
      "Epoch 21, Batch 240, train loss 0.03424283117055893\n",
      "Epoch 21, Batch 250, train loss 0.04436230659484863\n",
      "Epoch 21, Batch 260, train loss 0.04159891605377197\n",
      "Epoch 21, Batch 270, train loss 0.03534223139286041\n",
      "Epoch 21, Batch 280, train loss 0.03239421546459198\n",
      "Epoch 21, Batch 290, train loss 0.032874904572963715\n",
      "Epoch 21, Batch 300, train loss 0.04623762518167496\n",
      "Epoch 21, Batch 310, train loss 0.03727666661143303\n",
      "Epoch 21, Batch 320, train loss 0.032258570194244385\n",
      "Epoch 21, Batch 330, train loss 0.03217589110136032\n",
      "Epoch 21, Batch 340, train loss 0.04451107978820801\n",
      "Epoch 21, Batch 350, train loss 0.033729515969753265\n",
      "Epoch 21, Batch 360, train loss 0.03212299942970276\n",
      "Epoch 21, Batch 370, train loss 0.04685056954622269\n",
      "Epoch 21, Batch 380, train loss 0.04660973697900772\n",
      "Epoch 21, Batch 390, train loss 0.03370663523674011\n",
      "> Epoch 21, train loss 0.037041161712667234\n",
      "> Epoch 21, val loss 0.03880540527576624, accuracy 0.6558139534883721, f1_score 0.654054393598593\n",
      "No improvement.\n",
      "Epoch 22, Batch 0, train loss 0.02932138741016388\n",
      "Epoch 22, Batch 10, train loss 0.041711486876010895\n",
      "Epoch 22, Batch 20, train loss 0.03149077296257019\n",
      "Epoch 22, Batch 30, train loss 0.05023157596588135\n",
      "Epoch 22, Batch 40, train loss 0.041030917316675186\n",
      "Epoch 22, Batch 50, train loss 0.04247488081455231\n",
      "Epoch 22, Batch 60, train loss 0.03683720529079437\n",
      "Epoch 22, Batch 70, train loss 0.037594929337501526\n",
      "Epoch 22, Batch 80, train loss 0.03567811846733093\n",
      "Epoch 22, Batch 90, train loss 0.03543537110090256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Batch 100, train loss 0.03383694589138031\n",
      "Epoch 22, Batch 110, train loss 0.024161234498023987\n",
      "Epoch 22, Batch 120, train loss 0.048098042607307434\n",
      "Epoch 22, Batch 130, train loss 0.03456074744462967\n",
      "Epoch 22, Batch 140, train loss 0.03787953406572342\n",
      "Epoch 22, Batch 150, train loss 0.03960981220006943\n",
      "Epoch 22, Batch 160, train loss 0.03755572438240051\n",
      "Epoch 22, Batch 170, train loss 0.03157321363687515\n",
      "Epoch 22, Batch 180, train loss 0.03594920039176941\n",
      "Epoch 22, Batch 190, train loss 0.03433220833539963\n",
      "Epoch 22, Batch 200, train loss 0.04249384626746178\n",
      "Epoch 22, Batch 210, train loss 0.04871947318315506\n",
      "Epoch 22, Batch 220, train loss 0.027717482298612595\n",
      "Epoch 22, Batch 230, train loss 0.03061191737651825\n",
      "Epoch 22, Batch 240, train loss 0.04531942680478096\n",
      "Epoch 22, Batch 250, train loss 0.03792309761047363\n",
      "Epoch 22, Batch 260, train loss 0.037915635854005814\n",
      "Epoch 22, Batch 270, train loss 0.03322660177946091\n",
      "Epoch 22, Batch 280, train loss 0.033793266862630844\n",
      "Epoch 22, Batch 290, train loss 0.03644764423370361\n",
      "Epoch 22, Batch 300, train loss 0.03200691193342209\n",
      "Epoch 22, Batch 310, train loss 0.03075464256107807\n",
      "Epoch 22, Batch 320, train loss 0.03577931597828865\n",
      "Epoch 22, Batch 330, train loss 0.024913348257541656\n",
      "Epoch 22, Batch 340, train loss 0.03618282824754715\n",
      "Epoch 22, Batch 350, train loss 0.03373463451862335\n",
      "Epoch 22, Batch 360, train loss 0.04181576520204544\n",
      "Epoch 22, Batch 370, train loss 0.037264324724674225\n",
      "Epoch 22, Batch 380, train loss 0.042427949607372284\n",
      "Epoch 22, Batch 390, train loss 0.03581269085407257\n",
      "> Epoch 22, train loss 0.036697061265208414\n",
      "> Epoch 22, val loss 0.03893667695134185, accuracy 0.6503875968992248, f1_score 0.6405888316218076\n",
      "No improvement.\n",
      "Epoch 23, Batch 0, train loss 0.050006456673145294\n",
      "Epoch 23, Batch 10, train loss 0.030778497457504272\n",
      "Epoch 23, Batch 20, train loss 0.027241935953497887\n",
      "Epoch 23, Batch 30, train loss 0.03702010214328766\n",
      "Epoch 23, Batch 40, train loss 0.03749116510152817\n",
      "Epoch 23, Batch 50, train loss 0.03394790366292\n",
      "Epoch 23, Batch 60, train loss 0.033925894647836685\n",
      "Epoch 23, Batch 70, train loss 0.04402898624539375\n",
      "Epoch 23, Batch 80, train loss 0.034639570862054825\n",
      "Epoch 23, Batch 90, train loss 0.03149353712797165\n",
      "Epoch 23, Batch 100, train loss 0.03982522338628769\n",
      "Epoch 23, Batch 110, train loss 0.03943667560815811\n",
      "Epoch 23, Batch 120, train loss 0.04523647576570511\n",
      "Epoch 23, Batch 130, train loss 0.035618122667074203\n",
      "Epoch 23, Batch 140, train loss 0.03516847640275955\n",
      "Epoch 23, Batch 150, train loss 0.03670483082532883\n",
      "Epoch 23, Batch 160, train loss 0.0381365530192852\n",
      "Epoch 23, Batch 170, train loss 0.03247736021876335\n",
      "Epoch 23, Batch 180, train loss 0.03818739578127861\n",
      "Epoch 23, Batch 190, train loss 0.036656562238931656\n",
      "Epoch 23, Batch 200, train loss 0.04569178819656372\n",
      "Epoch 23, Batch 210, train loss 0.030203653499484062\n",
      "Epoch 23, Batch 220, train loss 0.03719085454940796\n",
      "Epoch 23, Batch 230, train loss 0.037048086524009705\n",
      "Epoch 23, Batch 240, train loss 0.03787621110677719\n",
      "Epoch 23, Batch 250, train loss 0.03283360227942467\n",
      "Epoch 23, Batch 260, train loss 0.04044945538043976\n",
      "Epoch 23, Batch 270, train loss 0.036047376692295074\n",
      "Epoch 23, Batch 280, train loss 0.02885347232222557\n",
      "Epoch 23, Batch 290, train loss 0.030055781826376915\n",
      "Epoch 23, Batch 300, train loss 0.02987588942050934\n",
      "Epoch 23, Batch 310, train loss 0.03220456838607788\n",
      "Epoch 23, Batch 320, train loss 0.03267059847712517\n",
      "Epoch 23, Batch 330, train loss 0.03889722377061844\n",
      "Epoch 23, Batch 340, train loss 0.05066763609647751\n",
      "Epoch 23, Batch 350, train loss 0.039983443915843964\n",
      "Epoch 23, Batch 360, train loss 0.04850243777036667\n",
      "Epoch 23, Batch 370, train loss 0.038394492119550705\n",
      "Epoch 23, Batch 380, train loss 0.04019739478826523\n",
      "Epoch 23, Batch 390, train loss 0.040707461535930634\n",
      "> Epoch 23, train loss 0.03665695122959823\n",
      "> Epoch 23, val loss 0.03892300699570382, accuracy 0.6627906976744186, f1_score 0.6588295448811055\n",
      "No improvement.\n",
      "Epoch 24, Batch 0, train loss 0.043053872883319855\n",
      "Epoch 24, Batch 10, train loss 0.03936431556940079\n",
      "Epoch 24, Batch 20, train loss 0.024951253086328506\n",
      "Epoch 24, Batch 30, train loss 0.0324297770857811\n",
      "Epoch 24, Batch 40, train loss 0.03235981613397598\n",
      "Epoch 24, Batch 50, train loss 0.03337102383375168\n",
      "Epoch 24, Batch 60, train loss 0.0404527522623539\n",
      "Epoch 24, Batch 70, train loss 0.02969258464872837\n",
      "Epoch 24, Batch 80, train loss 0.03792048990726471\n",
      "Epoch 24, Batch 90, train loss 0.04739566519856453\n",
      "Epoch 24, Batch 100, train loss 0.03453069180250168\n",
      "Epoch 24, Batch 110, train loss 0.04315806180238724\n",
      "Epoch 24, Batch 120, train loss 0.03626973181962967\n",
      "Epoch 24, Batch 130, train loss 0.028271663933992386\n",
      "Epoch 24, Batch 140, train loss 0.038194768130779266\n",
      "Epoch 24, Batch 150, train loss 0.0467330664396286\n",
      "Epoch 24, Batch 160, train loss 0.035623278468847275\n",
      "Epoch 24, Batch 170, train loss 0.030507419258356094\n",
      "Epoch 24, Batch 180, train loss 0.03798818588256836\n",
      "Epoch 24, Batch 190, train loss 0.03156839683651924\n",
      "Epoch 24, Batch 200, train loss 0.02810826525092125\n",
      "Epoch 24, Batch 210, train loss 0.055339474231004715\n",
      "Epoch 24, Batch 220, train loss 0.033249858766794205\n",
      "Epoch 24, Batch 230, train loss 0.025720667093992233\n",
      "Epoch 24, Batch 240, train loss 0.03185517340898514\n",
      "Epoch 24, Batch 250, train loss 0.04065414518117905\n",
      "Epoch 24, Batch 260, train loss 0.04063250869512558\n",
      "Epoch 24, Batch 270, train loss 0.0315728485584259\n",
      "Epoch 24, Batch 280, train loss 0.028696252033114433\n",
      "Epoch 24, Batch 290, train loss 0.029818017035722733\n",
      "Epoch 24, Batch 300, train loss 0.03228741139173508\n",
      "Epoch 24, Batch 310, train loss 0.039498910307884216\n",
      "Epoch 24, Batch 320, train loss 0.02951597422361374\n",
      "Epoch 24, Batch 330, train loss 0.04534143954515457\n",
      "Epoch 24, Batch 340, train loss 0.03698350116610527\n",
      "Epoch 24, Batch 350, train loss 0.04058028757572174\n",
      "Epoch 24, Batch 360, train loss 0.04011287912726402\n",
      "Epoch 24, Batch 370, train loss 0.041828155517578125\n",
      "Epoch 24, Batch 380, train loss 0.03844943642616272\n",
      "Epoch 24, Batch 390, train loss 0.03314965218305588\n",
      "> Epoch 24, train loss 0.03674142019203291\n",
      "> Epoch 24, val loss 0.03990270502345507, accuracy 0.6364341085271318, f1_score 0.6160838535152495\n",
      "No improvement.\n",
      "Epoch 25, Batch 0, train loss 0.042482759803533554\n",
      "Epoch 25, Batch 10, train loss 0.03230780363082886\n",
      "Epoch 25, Batch 20, train loss 0.028983626514673233\n",
      "Epoch 25, Batch 30, train loss 0.03198341280221939\n",
      "Epoch 25, Batch 40, train loss 0.031772613525390625\n",
      "Epoch 25, Batch 50, train loss 0.02946329303085804\n",
      "Epoch 25, Batch 60, train loss 0.0580817386507988\n",
      "Epoch 25, Batch 70, train loss 0.03447852283716202\n",
      "Epoch 25, Batch 80, train loss 0.03034823387861252\n",
      "Epoch 25, Batch 90, train loss 0.038303881883621216\n",
      "Epoch 25, Batch 100, train loss 0.027804208919405937\n",
      "Epoch 25, Batch 110, train loss 0.0310819074511528\n",
      "Epoch 25, Batch 120, train loss 0.04026951640844345\n",
      "Epoch 25, Batch 130, train loss 0.034734971821308136\n",
      "Epoch 25, Batch 140, train loss 0.03336677327752113\n",
      "Epoch 25, Batch 150, train loss 0.03863169252872467\n",
      "Epoch 25, Batch 160, train loss 0.034750256687402725\n",
      "Epoch 25, Batch 170, train loss 0.04056015983223915\n",
      "Epoch 25, Batch 180, train loss 0.027154896408319473\n",
      "Epoch 25, Batch 190, train loss 0.031058020889759064\n",
      "Epoch 25, Batch 200, train loss 0.03957975283265114\n",
      "Epoch 25, Batch 210, train loss 0.02656446397304535\n",
      "Epoch 25, Batch 220, train loss 0.029972922056913376\n",
      "Epoch 25, Batch 230, train loss 0.032117586582899094\n",
      "Epoch 25, Batch 240, train loss 0.03478587418794632\n",
      "Epoch 25, Batch 250, train loss 0.033195920288562775\n",
      "Epoch 25, Batch 260, train loss 0.03281775861978531\n",
      "Epoch 25, Batch 270, train loss 0.03038817271590233\n",
      "Epoch 25, Batch 280, train loss 0.03306659683585167\n",
      "Epoch 25, Batch 290, train loss 0.044730715453624725\n",
      "Epoch 25, Batch 300, train loss 0.04097405821084976\n",
      "Epoch 25, Batch 310, train loss 0.03780445456504822\n",
      "Epoch 25, Batch 320, train loss 0.035937607288360596\n",
      "Epoch 25, Batch 330, train loss 0.030571142211556435\n",
      "Epoch 25, Batch 340, train loss 0.04002363979816437\n",
      "Epoch 25, Batch 350, train loss 0.02420063689351082\n",
      "Epoch 25, Batch 360, train loss 0.03635431081056595\n",
      "Epoch 25, Batch 370, train loss 0.0398414209485054\n",
      "Epoch 25, Batch 380, train loss 0.03345048427581787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Batch 390, train loss 0.03789776563644409\n",
      "> Epoch 25, train loss 0.03643814356530406\n",
      "> Epoch 25, val loss 0.03943733656129172, accuracy 0.6565891472868217, f1_score 0.6465886750306583\n",
      "No improvement.\n",
      "Epoch 26, Batch 0, train loss 0.044968754053115845\n",
      "Epoch 26, Batch 10, train loss 0.03319312632083893\n",
      "Epoch 26, Batch 20, train loss 0.04738796874880791\n",
      "Epoch 26, Batch 30, train loss 0.03554488345980644\n",
      "Epoch 26, Batch 40, train loss 0.035676341503858566\n",
      "Epoch 26, Batch 50, train loss 0.029145393520593643\n",
      "Epoch 26, Batch 60, train loss 0.02851555496454239\n",
      "Epoch 26, Batch 70, train loss 0.04006858542561531\n",
      "Epoch 26, Batch 80, train loss 0.032019615173339844\n",
      "Epoch 26, Batch 90, train loss 0.03689422458410263\n",
      "Epoch 26, Batch 100, train loss 0.046456821262836456\n",
      "Epoch 26, Batch 110, train loss 0.038001418113708496\n",
      "Epoch 26, Batch 120, train loss 0.03300659358501434\n",
      "Epoch 26, Batch 130, train loss 0.029569555073976517\n",
      "Epoch 26, Batch 140, train loss 0.05561809241771698\n",
      "Epoch 26, Batch 150, train loss 0.040475815534591675\n",
      "Epoch 26, Batch 160, train loss 0.031437892466783524\n",
      "Epoch 26, Batch 170, train loss 0.03657013177871704\n",
      "Epoch 26, Batch 180, train loss 0.024765610694885254\n",
      "Epoch 26, Batch 190, train loss 0.023011330515146255\n",
      "Epoch 26, Batch 200, train loss 0.035688359290361404\n",
      "Epoch 26, Batch 210, train loss 0.04152050241827965\n",
      "Epoch 26, Batch 220, train loss 0.04382675141096115\n",
      "Epoch 26, Batch 230, train loss 0.04756707698106766\n",
      "Epoch 26, Batch 240, train loss 0.03116835281252861\n",
      "Epoch 26, Batch 250, train loss 0.030275259166955948\n",
      "Epoch 26, Batch 260, train loss 0.034395426511764526\n",
      "Epoch 26, Batch 270, train loss 0.03129154443740845\n",
      "Epoch 26, Batch 280, train loss 0.029092881828546524\n",
      "Epoch 26, Batch 290, train loss 0.027462441474199295\n",
      "Epoch 26, Batch 300, train loss 0.05131934955716133\n",
      "Epoch 26, Batch 310, train loss 0.04014404118061066\n",
      "Epoch 26, Batch 320, train loss 0.031181590631604195\n",
      "Epoch 26, Batch 330, train loss 0.02779187448322773\n",
      "Epoch 26, Batch 340, train loss 0.02799522876739502\n",
      "Epoch 26, Batch 350, train loss 0.047459688037633896\n",
      "Epoch 26, Batch 360, train loss 0.030394421890378\n",
      "Epoch 26, Batch 370, train loss 0.041776884347200394\n",
      "Epoch 26, Batch 380, train loss 0.032650575041770935\n",
      "Epoch 26, Batch 390, train loss 0.04879824444651604\n",
      "> Epoch 26, train loss 0.03594725233569199\n",
      "> Epoch 26, val loss 0.03959366921768632, accuracy 0.6550387596899225, f1_score 0.6414650187152701\n",
      "No improvement.\n",
      "Epoch 27, Batch 0, train loss 0.03706303611397743\n",
      "Epoch 27, Batch 10, train loss 0.03544314205646515\n",
      "Epoch 27, Batch 20, train loss 0.03100067749619484\n",
      "Epoch 27, Batch 30, train loss 0.02610444650053978\n",
      "Epoch 27, Batch 40, train loss 0.033791378140449524\n",
      "Epoch 27, Batch 50, train loss 0.03200576826930046\n",
      "Epoch 27, Batch 60, train loss 0.027410391718149185\n",
      "Epoch 27, Batch 70, train loss 0.048354074358940125\n",
      "Epoch 27, Batch 80, train loss 0.030676018446683884\n",
      "Epoch 27, Batch 90, train loss 0.02804863080382347\n",
      "Epoch 27, Batch 100, train loss 0.030825555324554443\n",
      "Epoch 27, Batch 110, train loss 0.03010309487581253\n",
      "Epoch 27, Batch 120, train loss 0.03364568576216698\n",
      "Epoch 27, Batch 130, train loss 0.03251369297504425\n",
      "Epoch 27, Batch 140, train loss 0.04468360170722008\n",
      "Epoch 27, Batch 150, train loss 0.032186321914196014\n",
      "Epoch 27, Batch 160, train loss 0.025783542543649673\n",
      "Epoch 27, Batch 170, train loss 0.031058160588145256\n",
      "Epoch 27, Batch 180, train loss 0.03601081669330597\n",
      "Epoch 27, Batch 190, train loss 0.03547944128513336\n",
      "Epoch 27, Batch 200, train loss 0.03558386489748955\n",
      "Epoch 27, Batch 210, train loss 0.03784644231200218\n",
      "Epoch 27, Batch 220, train loss 0.03472009301185608\n",
      "Epoch 27, Batch 230, train loss 0.0400676354765892\n",
      "Epoch 27, Batch 240, train loss 0.0369887575507164\n",
      "Epoch 27, Batch 250, train loss 0.03254446014761925\n",
      "Epoch 27, Batch 260, train loss 0.038342248648405075\n",
      "Epoch 27, Batch 270, train loss 0.029014211148023605\n",
      "Epoch 27, Batch 280, train loss 0.03540372475981712\n",
      "Epoch 27, Batch 290, train loss 0.03918932378292084\n",
      "Epoch 27, Batch 300, train loss 0.031882766634225845\n",
      "Epoch 27, Batch 310, train loss 0.0335751473903656\n",
      "Epoch 27, Batch 320, train loss 0.04210447520017624\n",
      "Epoch 27, Batch 330, train loss 0.03312291204929352\n",
      "Epoch 27, Batch 340, train loss 0.0466441735625267\n",
      "Epoch 27, Batch 350, train loss 0.036358438432216644\n",
      "Epoch 27, Batch 360, train loss 0.037165574729442596\n",
      "Epoch 27, Batch 370, train loss 0.027494629845023155\n",
      "Epoch 27, Batch 380, train loss 0.03313078731298447\n",
      "Epoch 27, Batch 390, train loss 0.03130580112338066\n",
      "> Epoch 27, train loss 0.035817439076315245\n",
      "> Epoch 27, val loss 0.03808803759342016, accuracy 0.6705426356589147, f1_score 0.6701412391137584\n",
      "Saved model.\n",
      "Epoch 28, Batch 0, train loss 0.036962203681468964\n",
      "Epoch 28, Batch 10, train loss 0.03338736668229103\n",
      "Epoch 28, Batch 20, train loss 0.03451717272400856\n",
      "Epoch 28, Batch 30, train loss 0.03781338781118393\n",
      "Epoch 28, Batch 40, train loss 0.02791561186313629\n",
      "Epoch 28, Batch 50, train loss 0.038036540150642395\n",
      "Epoch 28, Batch 60, train loss 0.033956170082092285\n",
      "Epoch 28, Batch 70, train loss 0.03144390881061554\n",
      "Epoch 28, Batch 80, train loss 0.030208563432097435\n",
      "Epoch 28, Batch 90, train loss 0.03743559867143631\n",
      "Epoch 28, Batch 100, train loss 0.02990889735519886\n",
      "Epoch 28, Batch 110, train loss 0.01916818507015705\n",
      "Epoch 28, Batch 120, train loss 0.04816555976867676\n",
      "Epoch 28, Batch 130, train loss 0.04213618487119675\n",
      "Epoch 28, Batch 140, train loss 0.036673106253147125\n",
      "Epoch 28, Batch 150, train loss 0.03195508196949959\n",
      "Epoch 28, Batch 160, train loss 0.033706068992614746\n",
      "Epoch 28, Batch 170, train loss 0.03328295052051544\n",
      "Epoch 28, Batch 180, train loss 0.04727743938565254\n",
      "Epoch 28, Batch 190, train loss 0.037181079387664795\n",
      "Epoch 28, Batch 200, train loss 0.02899262309074402\n",
      "Epoch 28, Batch 210, train loss 0.033013537526130676\n",
      "Epoch 28, Batch 220, train loss 0.03378178924322128\n",
      "Epoch 28, Batch 230, train loss 0.03541778028011322\n",
      "Epoch 28, Batch 240, train loss 0.049395833164453506\n",
      "Epoch 28, Batch 250, train loss 0.03181426227092743\n",
      "Epoch 28, Batch 260, train loss 0.026336124166846275\n",
      "Epoch 28, Batch 270, train loss 0.028618628159165382\n",
      "Epoch 28, Batch 280, train loss 0.03323095664381981\n",
      "Epoch 28, Batch 290, train loss 0.042749255895614624\n",
      "Epoch 28, Batch 300, train loss 0.03534844517707825\n",
      "Epoch 28, Batch 310, train loss 0.03487227112054825\n",
      "Epoch 28, Batch 320, train loss 0.034105315804481506\n",
      "Epoch 28, Batch 330, train loss 0.03388981893658638\n",
      "Epoch 28, Batch 340, train loss 0.025063054636120796\n",
      "Epoch 28, Batch 350, train loss 0.027002640068531036\n",
      "Epoch 28, Batch 360, train loss 0.04064524546265602\n",
      "Epoch 28, Batch 370, train loss 0.04395124688744545\n",
      "Epoch 28, Batch 380, train loss 0.041377246379852295\n",
      "Epoch 28, Batch 390, train loss 0.05002758279442787\n",
      "> Epoch 28, train loss 0.035847697778772745\n",
      "> Epoch 28, val loss 0.03854619805202928, accuracy 0.665891472868217, f1_score 0.6613738116656384\n",
      "No improvement.\n",
      "Epoch 29, Batch 0, train loss 0.032165706157684326\n",
      "Epoch 29, Batch 10, train loss 0.027494024485349655\n",
      "Epoch 29, Batch 20, train loss 0.04153374582529068\n",
      "Epoch 29, Batch 30, train loss 0.0493110790848732\n",
      "Epoch 29, Batch 40, train loss 0.03490328788757324\n",
      "Epoch 29, Batch 50, train loss 0.041365284472703934\n",
      "Epoch 29, Batch 60, train loss 0.03338174521923065\n",
      "Epoch 29, Batch 70, train loss 0.04329564422369003\n",
      "Epoch 29, Batch 80, train loss 0.027019137516617775\n",
      "Epoch 29, Batch 90, train loss 0.03406839072704315\n",
      "Epoch 29, Batch 100, train loss 0.03022884391248226\n",
      "Epoch 29, Batch 110, train loss 0.028141247108578682\n",
      "Epoch 29, Batch 120, train loss 0.03029114380478859\n",
      "Epoch 29, Batch 130, train loss 0.039823878556489944\n",
      "Epoch 29, Batch 140, train loss 0.040037039667367935\n",
      "Epoch 29, Batch 150, train loss 0.028578883036971092\n",
      "Epoch 29, Batch 160, train loss 0.03735342621803284\n",
      "Epoch 29, Batch 170, train loss 0.02982190251350403\n",
      "Epoch 29, Batch 180, train loss 0.034457288682460785\n",
      "Epoch 29, Batch 190, train loss 0.02917790226638317\n",
      "Epoch 29, Batch 200, train loss 0.03898712247610092\n",
      "Epoch 29, Batch 210, train loss 0.028722703456878662\n",
      "Epoch 29, Batch 220, train loss 0.041057832539081573\n",
      "Epoch 29, Batch 230, train loss 0.028967581689357758\n",
      "Epoch 29, Batch 240, train loss 0.04316840320825577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Batch 250, train loss 0.04096140339970589\n",
      "Epoch 29, Batch 260, train loss 0.020912256091833115\n",
      "Epoch 29, Batch 270, train loss 0.0337306372821331\n",
      "Epoch 29, Batch 280, train loss 0.033207863569259644\n",
      "Epoch 29, Batch 290, train loss 0.028332248330116272\n",
      "Epoch 29, Batch 300, train loss 0.029454713687300682\n",
      "Epoch 29, Batch 310, train loss 0.03014862723648548\n",
      "Epoch 29, Batch 320, train loss 0.03538237139582634\n",
      "Epoch 29, Batch 330, train loss 0.03928515315055847\n",
      "Epoch 29, Batch 340, train loss 0.02776039019227028\n",
      "Epoch 29, Batch 350, train loss 0.044649526476860046\n",
      "Epoch 29, Batch 360, train loss 0.03497619181871414\n",
      "Epoch 29, Batch 370, train loss 0.028214674443006516\n",
      "Epoch 29, Batch 380, train loss 0.026746641844511032\n",
      "Epoch 29, Batch 390, train loss 0.03978221118450165\n",
      "> Epoch 29, train loss 0.03584456285918415\n",
      "> Epoch 29, val loss 0.0379096145084662, accuracy 0.672093023255814, f1_score 0.6720833676371698\n",
      "Saved model.\n",
      "Epoch 30, Batch 0, train loss 0.042283184826374054\n",
      "Epoch 30, Batch 10, train loss 0.027412038296461105\n",
      "Epoch 30, Batch 20, train loss 0.03497614338994026\n",
      "Epoch 30, Batch 30, train loss 0.043173499405384064\n",
      "Epoch 30, Batch 40, train loss 0.032089632004499435\n",
      "Epoch 30, Batch 50, train loss 0.036765772849321365\n",
      "Epoch 30, Batch 60, train loss 0.03404780104756355\n",
      "Epoch 30, Batch 70, train loss 0.045464277267456055\n",
      "Epoch 30, Batch 80, train loss 0.05685097351670265\n",
      "Epoch 30, Batch 90, train loss 0.032452888786792755\n",
      "Epoch 30, Batch 100, train loss 0.041156403720378876\n",
      "Epoch 30, Batch 110, train loss 0.040733084082603455\n",
      "Epoch 30, Batch 120, train loss 0.04662881791591644\n",
      "Epoch 30, Batch 130, train loss 0.031758084893226624\n",
      "Epoch 30, Batch 140, train loss 0.036921270191669464\n",
      "Epoch 30, Batch 150, train loss 0.054142311215400696\n",
      "Epoch 30, Batch 160, train loss 0.033123258501291275\n",
      "Epoch 30, Batch 170, train loss 0.03530003875494003\n",
      "Epoch 30, Batch 180, train loss 0.046082962304353714\n",
      "Epoch 30, Batch 190, train loss 0.023912932723760605\n",
      "Epoch 30, Batch 200, train loss 0.05213321000337601\n",
      "Epoch 30, Batch 210, train loss 0.02961169369518757\n",
      "Epoch 30, Batch 220, train loss 0.031110506504774094\n",
      "Epoch 30, Batch 230, train loss 0.04194110259413719\n",
      "Epoch 30, Batch 240, train loss 0.042946651577949524\n",
      "Epoch 30, Batch 250, train loss 0.06115437299013138\n",
      "Epoch 30, Batch 260, train loss 0.030539948493242264\n",
      "Epoch 30, Batch 270, train loss 0.027762141078710556\n",
      "Epoch 30, Batch 280, train loss 0.04348742961883545\n",
      "Epoch 30, Batch 290, train loss 0.03698928654193878\n",
      "Epoch 30, Batch 300, train loss 0.024486534297466278\n",
      "Epoch 30, Batch 310, train loss 0.04281730577349663\n",
      "Epoch 30, Batch 320, train loss 0.029328931123018265\n",
      "Epoch 30, Batch 330, train loss 0.03140699118375778\n",
      "Epoch 30, Batch 340, train loss 0.0399409681558609\n",
      "Epoch 30, Batch 350, train loss 0.03504592180252075\n",
      "Epoch 30, Batch 360, train loss 0.027051812037825584\n",
      "Epoch 30, Batch 370, train loss 0.04923991858959198\n",
      "Epoch 30, Batch 380, train loss 0.02975771576166153\n",
      "Epoch 30, Batch 390, train loss 0.04324548691511154\n",
      "> Epoch 30, train loss 0.03511347435502887\n",
      "> Epoch 30, val loss 0.03745000893755476, accuracy 0.6751937984496124, f1_score 0.6740921629810519\n",
      "Saved model.\n",
      "Epoch 31, Batch 0, train loss 0.02886226773262024\n",
      "Epoch 31, Batch 10, train loss 0.03551965579390526\n",
      "Epoch 31, Batch 20, train loss 0.042373139411211014\n",
      "Epoch 31, Batch 30, train loss 0.041069306433200836\n",
      "Epoch 31, Batch 40, train loss 0.03253524750471115\n",
      "Epoch 31, Batch 50, train loss 0.040930796414613724\n",
      "Epoch 31, Batch 60, train loss 0.030519692227244377\n",
      "Epoch 31, Batch 70, train loss 0.028488878160715103\n",
      "Epoch 31, Batch 80, train loss 0.02872489206492901\n",
      "Epoch 31, Batch 90, train loss 0.02600535750389099\n",
      "Epoch 31, Batch 100, train loss 0.03214088827371597\n",
      "Epoch 31, Batch 110, train loss 0.03258117660880089\n",
      "Epoch 31, Batch 120, train loss 0.03497843071818352\n",
      "Epoch 31, Batch 130, train loss 0.03746927157044411\n",
      "Epoch 31, Batch 140, train loss 0.032462552189826965\n",
      "Epoch 31, Batch 150, train loss 0.03865426033735275\n",
      "Epoch 31, Batch 160, train loss 0.036618463695049286\n",
      "Epoch 31, Batch 170, train loss 0.02934328094124794\n",
      "Epoch 31, Batch 180, train loss 0.025193633511662483\n",
      "Epoch 31, Batch 190, train loss 0.045877475291490555\n",
      "Epoch 31, Batch 200, train loss 0.039590731263160706\n",
      "Epoch 31, Batch 210, train loss 0.02428627945482731\n",
      "Epoch 31, Batch 220, train loss 0.028045587241649628\n",
      "Epoch 31, Batch 230, train loss 0.0404207669198513\n",
      "Epoch 31, Batch 240, train loss 0.03781483322381973\n",
      "Epoch 31, Batch 250, train loss 0.029449688270688057\n",
      "Epoch 31, Batch 260, train loss 0.029759595170617104\n",
      "Epoch 31, Batch 270, train loss 0.04151390492916107\n",
      "Epoch 31, Batch 280, train loss 0.03285251557826996\n",
      "Epoch 31, Batch 290, train loss 0.03310655802488327\n",
      "Epoch 31, Batch 300, train loss 0.04378793388605118\n",
      "Epoch 31, Batch 310, train loss 0.032709188759326935\n",
      "Epoch 31, Batch 320, train loss 0.045851871371269226\n",
      "Epoch 31, Batch 330, train loss 0.030140643939375877\n",
      "Epoch 31, Batch 340, train loss 0.03873290866613388\n",
      "Epoch 31, Batch 350, train loss 0.035535119473934174\n",
      "Epoch 31, Batch 360, train loss 0.028001368045806885\n",
      "Epoch 31, Batch 370, train loss 0.030338019132614136\n",
      "Epoch 31, Batch 380, train loss 0.03876965492963791\n",
      "Epoch 31, Batch 390, train loss 0.034422144293785095\n",
      "> Epoch 31, train loss 0.03512267671829704\n",
      "> Epoch 31, val loss 0.03750132629575655, accuracy 0.686046511627907, f1_score 0.6860146244044976\n",
      "No improvement.\n",
      "Epoch 32, Batch 0, train loss 0.035718947649002075\n",
      "Epoch 32, Batch 10, train loss 0.04082451015710831\n",
      "Epoch 32, Batch 20, train loss 0.025772567838430405\n",
      "Epoch 32, Batch 30, train loss 0.03435855731368065\n",
      "Epoch 32, Batch 40, train loss 0.03202921152114868\n",
      "Epoch 32, Batch 50, train loss 0.029962848871946335\n",
      "Epoch 32, Batch 60, train loss 0.029159676283597946\n",
      "Epoch 32, Batch 70, train loss 0.039823018014431\n",
      "Epoch 32, Batch 80, train loss 0.03076782450079918\n",
      "Epoch 32, Batch 90, train loss 0.03751944750547409\n",
      "Epoch 32, Batch 100, train loss 0.02734321355819702\n",
      "Epoch 32, Batch 110, train loss 0.03367549926042557\n",
      "Epoch 32, Batch 120, train loss 0.023882992565631866\n",
      "Epoch 32, Batch 130, train loss 0.028768090531229973\n",
      "Epoch 32, Batch 140, train loss 0.030575770884752274\n",
      "Epoch 32, Batch 150, train loss 0.04117964953184128\n",
      "Epoch 32, Batch 160, train loss 0.02856021746993065\n",
      "Epoch 32, Batch 170, train loss 0.04495245963335037\n",
      "Epoch 32, Batch 180, train loss 0.049327779561281204\n",
      "Epoch 32, Batch 190, train loss 0.03774845600128174\n",
      "Epoch 32, Batch 200, train loss 0.0328918993473053\n",
      "Epoch 32, Batch 210, train loss 0.037489812821149826\n",
      "Epoch 32, Batch 220, train loss 0.04228111356496811\n",
      "Epoch 32, Batch 230, train loss 0.032914213836193085\n",
      "Epoch 32, Batch 240, train loss 0.0328991673886776\n",
      "Epoch 32, Batch 250, train loss 0.042370885610580444\n",
      "Epoch 32, Batch 260, train loss 0.029301630333065987\n",
      "Epoch 32, Batch 270, train loss 0.02934020385146141\n",
      "Epoch 32, Batch 280, train loss 0.02777327038347721\n",
      "Epoch 32, Batch 290, train loss 0.0324806310236454\n",
      "Epoch 32, Batch 300, train loss 0.025958577170968056\n",
      "Epoch 32, Batch 310, train loss 0.0513334721326828\n",
      "Epoch 32, Batch 320, train loss 0.02468099817633629\n",
      "Epoch 32, Batch 330, train loss 0.030301548540592194\n",
      "Epoch 32, Batch 340, train loss 0.022608738392591476\n",
      "Epoch 32, Batch 350, train loss 0.037842798978090286\n",
      "Epoch 32, Batch 360, train loss 0.026187488809227943\n",
      "Epoch 32, Batch 370, train loss 0.03196663036942482\n",
      "Epoch 32, Batch 380, train loss 0.02713342010974884\n",
      "Epoch 32, Batch 390, train loss 0.02935069054365158\n",
      "> Epoch 32, train loss 0.03481167055409206\n",
      "> Epoch 32, val loss 0.03780176089715588, accuracy 0.686046511627907, f1_score 0.685037898540546\n",
      "No improvement.\n",
      "Epoch 33, Batch 0, train loss 0.03241860121488571\n",
      "Epoch 33, Batch 10, train loss 0.039529480040073395\n",
      "Epoch 33, Batch 20, train loss 0.02195918560028076\n",
      "Epoch 33, Batch 30, train loss 0.030767597258090973\n",
      "Epoch 33, Batch 40, train loss 0.04699856787919998\n",
      "Epoch 33, Batch 50, train loss 0.039135269820690155\n",
      "Epoch 33, Batch 60, train loss 0.031847160309553146\n",
      "Epoch 33, Batch 70, train loss 0.034256722778081894\n",
      "Epoch 33, Batch 80, train loss 0.034002356231212616\n",
      "Epoch 33, Batch 90, train loss 0.01953992433845997\n",
      "Epoch 33, Batch 100, train loss 0.04445851221680641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33, Batch 110, train loss 0.02177116647362709\n",
      "Epoch 33, Batch 120, train loss 0.026389770209789276\n",
      "Epoch 33, Batch 130, train loss 0.044450677931308746\n",
      "Epoch 33, Batch 140, train loss 0.032670099288225174\n",
      "Epoch 33, Batch 150, train loss 0.030066397041082382\n",
      "Epoch 33, Batch 160, train loss 0.02912496216595173\n",
      "Epoch 33, Batch 170, train loss 0.032931625843048096\n",
      "Epoch 33, Batch 180, train loss 0.03567907214164734\n",
      "Epoch 33, Batch 190, train loss 0.04458506405353546\n",
      "Epoch 33, Batch 200, train loss 0.04976477473974228\n",
      "Epoch 33, Batch 210, train loss 0.03296174854040146\n",
      "Epoch 33, Batch 220, train loss 0.030750341713428497\n",
      "Epoch 33, Batch 230, train loss 0.031294096261262894\n",
      "Epoch 33, Batch 240, train loss 0.04964863136410713\n",
      "Epoch 33, Batch 250, train loss 0.04246348887681961\n",
      "Epoch 33, Batch 260, train loss 0.044054239988327026\n",
      "Epoch 33, Batch 270, train loss 0.03057599999010563\n",
      "Epoch 33, Batch 280, train loss 0.026494566351175308\n",
      "Epoch 33, Batch 290, train loss 0.01528957486152649\n",
      "Epoch 33, Batch 300, train loss 0.019237026572227478\n",
      "Epoch 33, Batch 310, train loss 0.03629232197999954\n",
      "Epoch 33, Batch 320, train loss 0.03492221236228943\n",
      "Epoch 33, Batch 330, train loss 0.04353275150060654\n",
      "Epoch 33, Batch 340, train loss 0.025266211479902267\n",
      "Epoch 33, Batch 350, train loss 0.03797324001789093\n",
      "Epoch 33, Batch 360, train loss 0.04506422579288483\n",
      "Epoch 33, Batch 370, train loss 0.03600450977683067\n",
      "Epoch 33, Batch 380, train loss 0.03496050462126732\n",
      "Epoch 33, Batch 390, train loss 0.04385954886674881\n",
      "> Epoch 33, train loss 0.03463007304150844\n",
      "> Epoch 33, val loss 0.03859738534273103, accuracy 0.6689922480620155, f1_score 0.6586064240226839\n",
      "No improvement.\n",
      "Epoch 34, Batch 0, train loss 0.041461266577243805\n",
      "Epoch 34, Batch 10, train loss 0.035380955785512924\n",
      "Epoch 34, Batch 20, train loss 0.03997427225112915\n",
      "Epoch 34, Batch 30, train loss 0.030820585787296295\n",
      "Epoch 34, Batch 40, train loss 0.03492993116378784\n",
      "Epoch 34, Batch 50, train loss 0.0438571497797966\n",
      "Epoch 34, Batch 60, train loss 0.02107110619544983\n",
      "Epoch 34, Batch 70, train loss 0.03692653030157089\n",
      "Epoch 34, Batch 80, train loss 0.03313707560300827\n",
      "Epoch 34, Batch 90, train loss 0.029911156743764877\n",
      "Epoch 34, Batch 100, train loss 0.02324073389172554\n",
      "Epoch 34, Batch 110, train loss 0.04118669033050537\n",
      "Epoch 34, Batch 120, train loss 0.054445959627628326\n",
      "Epoch 34, Batch 130, train loss 0.0380195751786232\n",
      "Epoch 34, Batch 140, train loss 0.03011023998260498\n",
      "Epoch 34, Batch 150, train loss 0.04505929350852966\n",
      "Epoch 34, Batch 160, train loss 0.027262575924396515\n",
      "Epoch 34, Batch 170, train loss 0.03863708674907684\n",
      "Epoch 34, Batch 180, train loss 0.027932416647672653\n",
      "Epoch 34, Batch 190, train loss 0.030092723667621613\n",
      "Epoch 34, Batch 200, train loss 0.04302234202623367\n",
      "Epoch 34, Batch 210, train loss 0.03854920715093613\n",
      "Epoch 34, Batch 220, train loss 0.04422476887702942\n",
      "Epoch 34, Batch 230, train loss 0.03228543698787689\n",
      "Epoch 34, Batch 240, train loss 0.02671976387500763\n",
      "Epoch 34, Batch 250, train loss 0.02969924733042717\n",
      "Epoch 34, Batch 260, train loss 0.025896582752466202\n",
      "Epoch 34, Batch 270, train loss 0.03600972890853882\n",
      "Epoch 34, Batch 280, train loss 0.04230240359902382\n",
      "Epoch 34, Batch 290, train loss 0.0481683574616909\n",
      "Epoch 34, Batch 300, train loss 0.03166639804840088\n",
      "Epoch 34, Batch 310, train loss 0.047101542353630066\n",
      "Epoch 34, Batch 320, train loss 0.022649617865681648\n",
      "Epoch 34, Batch 330, train loss 0.0375266969203949\n",
      "Epoch 34, Batch 340, train loss 0.046915456652641296\n",
      "Epoch 34, Batch 350, train loss 0.03528233617544174\n",
      "Epoch 34, Batch 360, train loss 0.04637390747666359\n",
      "Epoch 34, Batch 370, train loss 0.025030776858329773\n",
      "Epoch 34, Batch 380, train loss 0.03749765083193779\n",
      "Epoch 34, Batch 390, train loss 0.035689253360033035\n",
      "> Epoch 34, train loss 0.03431297968600275\n",
      "> Epoch 34, val loss 0.03809260409469752, accuracy 0.6697674418604651, f1_score 0.6691439429150783\n",
      "No improvement.\n",
      "Epoch 35, Batch 0, train loss 0.030796824023127556\n",
      "Epoch 35, Batch 10, train loss 0.036829520016908646\n",
      "Epoch 35, Batch 20, train loss 0.04156382381916046\n",
      "Epoch 35, Batch 30, train loss 0.02513076364994049\n",
      "Epoch 35, Batch 40, train loss 0.040507905185222626\n",
      "Epoch 35, Batch 50, train loss 0.03483587130904198\n",
      "Epoch 35, Batch 60, train loss 0.04367411136627197\n",
      "Epoch 35, Batch 70, train loss 0.03768504410982132\n",
      "Epoch 35, Batch 80, train loss 0.03937632218003273\n",
      "Epoch 35, Batch 90, train loss 0.02626178041100502\n",
      "Epoch 35, Batch 100, train loss 0.017433565109968185\n",
      "Epoch 35, Batch 110, train loss 0.05626456066966057\n",
      "Epoch 35, Batch 120, train loss 0.03169094771146774\n",
      "Epoch 35, Batch 130, train loss 0.035388171672821045\n",
      "Epoch 35, Batch 140, train loss 0.03394845128059387\n",
      "Epoch 35, Batch 150, train loss 0.024986043572425842\n",
      "Epoch 35, Batch 160, train loss 0.029986092820763588\n",
      "Epoch 35, Batch 170, train loss 0.041995819658041\n",
      "Epoch 35, Batch 180, train loss 0.03979840874671936\n",
      "Epoch 35, Batch 190, train loss 0.03337901085615158\n",
      "Epoch 35, Batch 200, train loss 0.030839653685688972\n",
      "Epoch 35, Batch 210, train loss 0.039535220712423325\n",
      "Epoch 35, Batch 220, train loss 0.03854183107614517\n",
      "Epoch 35, Batch 230, train loss 0.02909100614488125\n",
      "Epoch 35, Batch 240, train loss 0.029045848175883293\n",
      "Epoch 35, Batch 250, train loss 0.029043516144156456\n",
      "Epoch 35, Batch 260, train loss 0.029613148421049118\n",
      "Epoch 35, Batch 270, train loss 0.039121247828006744\n",
      "Epoch 35, Batch 280, train loss 0.03954527527093887\n",
      "Epoch 35, Batch 290, train loss 0.026015017181634903\n",
      "Epoch 35, Batch 300, train loss 0.018249254673719406\n",
      "Epoch 35, Batch 310, train loss 0.044356659054756165\n",
      "Epoch 35, Batch 320, train loss 0.03949901461601257\n",
      "Epoch 35, Batch 330, train loss 0.03643636405467987\n",
      "Epoch 35, Batch 340, train loss 0.035578396171331406\n",
      "Epoch 35, Batch 350, train loss 0.03094266727566719\n",
      "Epoch 35, Batch 360, train loss 0.0438845269382\n",
      "Epoch 35, Batch 370, train loss 0.0337376594543457\n",
      "Epoch 35, Batch 380, train loss 0.02460293471813202\n",
      "Epoch 35, Batch 390, train loss 0.040628716349601746\n",
      "> Epoch 35, train loss 0.03404144226950312\n",
      "> Epoch 35, val loss 0.0387133037166078, accuracy 0.6813953488372093, f1_score 0.6767281984055606\n",
      "No improvement.\n",
      "Epoch 36, Batch 0, train loss 0.036705099046230316\n",
      "Epoch 36, Batch 10, train loss 0.03908005356788635\n",
      "Epoch 36, Batch 20, train loss 0.029667900875210762\n",
      "Epoch 36, Batch 30, train loss 0.03278558328747749\n",
      "Epoch 36, Batch 40, train loss 0.038595959544181824\n",
      "Epoch 36, Batch 50, train loss 0.041446175426244736\n",
      "Epoch 36, Batch 60, train loss 0.0352347195148468\n",
      "Epoch 36, Batch 70, train loss 0.03643406555056572\n",
      "Epoch 36, Batch 80, train loss 0.04025177285075188\n",
      "Epoch 36, Batch 90, train loss 0.0353790819644928\n",
      "Epoch 36, Batch 100, train loss 0.0312834158539772\n",
      "Epoch 36, Batch 110, train loss 0.029705777764320374\n",
      "Epoch 36, Batch 120, train loss 0.040086206048727036\n",
      "Epoch 36, Batch 130, train loss 0.030852124094963074\n",
      "Epoch 36, Batch 140, train loss 0.04699159413576126\n",
      "Epoch 36, Batch 150, train loss 0.02397400513291359\n",
      "Epoch 36, Batch 160, train loss 0.04021695256233215\n",
      "Epoch 36, Batch 170, train loss 0.028873758390545845\n",
      "Epoch 36, Batch 180, train loss 0.032337822020053864\n",
      "Epoch 36, Batch 190, train loss 0.026986747980117798\n",
      "Epoch 36, Batch 200, train loss 0.037470944225788116\n",
      "Epoch 36, Batch 210, train loss 0.029833726584911346\n",
      "Epoch 36, Batch 220, train loss 0.03597726672887802\n",
      "Epoch 36, Batch 230, train loss 0.01790858805179596\n",
      "Epoch 36, Batch 240, train loss 0.024320457130670547\n",
      "Epoch 36, Batch 250, train loss 0.031822312623262405\n",
      "Epoch 36, Batch 260, train loss 0.026431169360876083\n",
      "Epoch 36, Batch 270, train loss 0.025855284184217453\n",
      "Epoch 36, Batch 280, train loss 0.02822146564722061\n",
      "Epoch 36, Batch 290, train loss 0.02989080548286438\n",
      "Epoch 36, Batch 300, train loss 0.044703055173158646\n",
      "Epoch 36, Batch 310, train loss 0.03809412196278572\n",
      "Epoch 36, Batch 320, train loss 0.02797115221619606\n",
      "Epoch 36, Batch 330, train loss 0.03025047853589058\n",
      "Epoch 36, Batch 340, train loss 0.03359869867563248\n",
      "Epoch 36, Batch 350, train loss 0.03436853736639023\n",
      "Epoch 36, Batch 360, train loss 0.027776239439845085\n",
      "Epoch 36, Batch 370, train loss 0.03160368278622627\n",
      "Epoch 36, Batch 380, train loss 0.027284611016511917\n",
      "Epoch 36, Batch 390, train loss 0.023624010384082794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Epoch 36, train loss 0.034077902284120325\n",
      "> Epoch 36, val loss 0.03762450356816137, accuracy 0.6775193798449612, f1_score 0.6762110709941499\n",
      "No improvement.\n",
      "Epoch 37, Batch 0, train loss 0.03291543200612068\n",
      "Epoch 37, Batch 10, train loss 0.02767348475754261\n",
      "Epoch 37, Batch 20, train loss 0.041218049824237823\n",
      "Epoch 37, Batch 30, train loss 0.05423139035701752\n",
      "Epoch 37, Batch 40, train loss 0.03221574425697327\n",
      "Epoch 37, Batch 50, train loss 0.02579713985323906\n",
      "Epoch 37, Batch 60, train loss 0.03440096229314804\n",
      "Epoch 37, Batch 70, train loss 0.02793767675757408\n",
      "Epoch 37, Batch 80, train loss 0.03201704099774361\n",
      "Epoch 37, Batch 90, train loss 0.03541402518749237\n",
      "Epoch 37, Batch 100, train loss 0.04714319109916687\n",
      "Epoch 37, Batch 110, train loss 0.025888849049806595\n",
      "Epoch 37, Batch 120, train loss 0.03369162976741791\n",
      "Epoch 37, Batch 130, train loss 0.03283318877220154\n",
      "Epoch 37, Batch 140, train loss 0.039214592427015305\n",
      "Epoch 37, Batch 150, train loss 0.037331823259592056\n",
      "Epoch 37, Batch 160, train loss 0.040628038346767426\n",
      "Epoch 37, Batch 170, train loss 0.024190261960029602\n",
      "Epoch 37, Batch 180, train loss 0.028013795614242554\n",
      "Epoch 37, Batch 190, train loss 0.040590059012174606\n",
      "Epoch 37, Batch 200, train loss 0.042210791260004044\n",
      "Epoch 37, Batch 210, train loss 0.022157175466418266\n",
      "Epoch 37, Batch 220, train loss 0.02726961299777031\n",
      "Epoch 37, Batch 230, train loss 0.037558794021606445\n",
      "Epoch 37, Batch 240, train loss 0.03502330929040909\n",
      "Epoch 37, Batch 250, train loss 0.04303358495235443\n",
      "Epoch 37, Batch 260, train loss 0.03238317370414734\n",
      "Epoch 37, Batch 270, train loss 0.027347110211849213\n",
      "Epoch 37, Batch 280, train loss 0.02173282578587532\n",
      "Epoch 37, Batch 290, train loss 0.03225330635905266\n",
      "Epoch 37, Batch 300, train loss 0.024021532386541367\n",
      "Epoch 37, Batch 310, train loss 0.02678922936320305\n",
      "Epoch 37, Batch 320, train loss 0.0315832793712616\n",
      "Epoch 37, Batch 330, train loss 0.03339759260416031\n",
      "Epoch 37, Batch 340, train loss 0.03362010419368744\n",
      "Epoch 37, Batch 350, train loss 0.03668920695781708\n",
      "Epoch 37, Batch 360, train loss 0.02591242641210556\n",
      "Epoch 37, Batch 370, train loss 0.03288688138127327\n",
      "Epoch 37, Batch 380, train loss 0.028564052656292915\n",
      "Epoch 37, Batch 390, train loss 0.03455038741230965\n",
      "> Epoch 37, train loss 0.03393429433173718\n",
      "> Epoch 37, val loss 0.03800303155137587, accuracy 0.6759689922480621, f1_score 0.6751185725027474\n",
      "No improvement.\n",
      "Epoch 38, Batch 0, train loss 0.03779677674174309\n",
      "Epoch 38, Batch 10, train loss 0.03309677913784981\n",
      "Epoch 38, Batch 20, train loss 0.04312548413872719\n",
      "Epoch 38, Batch 30, train loss 0.04206671938300133\n",
      "Epoch 38, Batch 40, train loss 0.03711026534438133\n",
      "Epoch 38, Batch 50, train loss 0.021968375891447067\n",
      "Epoch 38, Batch 60, train loss 0.046020910143852234\n",
      "Epoch 38, Batch 70, train loss 0.04997285082936287\n",
      "Epoch 38, Batch 80, train loss 0.033737555146217346\n",
      "Epoch 38, Batch 90, train loss 0.029859891161322594\n",
      "Epoch 38, Batch 100, train loss 0.03775012865662575\n",
      "Epoch 38, Batch 110, train loss 0.04141706973314285\n",
      "Epoch 38, Batch 120, train loss 0.024897217750549316\n",
      "Epoch 38, Batch 130, train loss 0.029779579490423203\n",
      "Epoch 38, Batch 140, train loss 0.04387420415878296\n",
      "Epoch 38, Batch 150, train loss 0.022410525009036064\n",
      "Epoch 38, Batch 160, train loss 0.041359566152095795\n",
      "Epoch 38, Batch 170, train loss 0.02948061376810074\n",
      "Epoch 38, Batch 180, train loss 0.02527381107211113\n",
      "Epoch 38, Batch 190, train loss 0.04287556931376457\n",
      "Epoch 38, Batch 200, train loss 0.02490321919322014\n",
      "Epoch 38, Batch 210, train loss 0.02890828251838684\n",
      "Epoch 38, Batch 220, train loss 0.04071511700749397\n",
      "Epoch 38, Batch 230, train loss 0.024846404790878296\n",
      "Epoch 38, Batch 240, train loss 0.03621407598257065\n",
      "Epoch 38, Batch 250, train loss 0.02452121488749981\n",
      "Epoch 38, Batch 260, train loss 0.025152582675218582\n",
      "Epoch 38, Batch 270, train loss 0.024463534355163574\n",
      "Epoch 38, Batch 280, train loss 0.03303314000368118\n",
      "Epoch 38, Batch 290, train loss 0.021223273128271103\n",
      "Epoch 38, Batch 300, train loss 0.03426419571042061\n",
      "Epoch 38, Batch 310, train loss 0.036566171795129776\n",
      "Epoch 38, Batch 320, train loss 0.04367561638355255\n",
      "Epoch 38, Batch 330, train loss 0.02435818873345852\n",
      "Epoch 38, Batch 340, train loss 0.028996044769883156\n",
      "Epoch 38, Batch 350, train loss 0.03933461755514145\n",
      "Epoch 38, Batch 360, train loss 0.02962789498269558\n",
      "Epoch 38, Batch 370, train loss 0.04441346973180771\n",
      "Epoch 38, Batch 380, train loss 0.03806496411561966\n",
      "Epoch 38, Batch 390, train loss 0.02848198264837265\n",
      "> Epoch 38, train loss 0.03349951122454734\n",
      "> Epoch 38, val loss 0.03983429584854333, accuracy 0.6705426356589147, f1_score 0.6564804424118362\n",
      "No improvement.\n",
      "Epoch 39, Batch 0, train loss 0.02742672525346279\n",
      "Epoch 39, Batch 10, train loss 0.04619459807872772\n",
      "Epoch 39, Batch 20, train loss 0.031055599451065063\n",
      "Epoch 39, Batch 30, train loss 0.035001348704099655\n",
      "Epoch 39, Batch 40, train loss 0.030340204015374184\n",
      "Epoch 39, Batch 50, train loss 0.0373745933175087\n",
      "Epoch 39, Batch 60, train loss 0.03802153468132019\n",
      "Epoch 39, Batch 70, train loss 0.024732865393161774\n",
      "Epoch 39, Batch 80, train loss 0.032113611698150635\n",
      "Epoch 39, Batch 90, train loss 0.040653571486473083\n",
      "Epoch 39, Batch 100, train loss 0.030457645654678345\n",
      "Epoch 39, Batch 110, train loss 0.03680814802646637\n",
      "Epoch 39, Batch 120, train loss 0.039895329624414444\n",
      "Epoch 39, Batch 130, train loss 0.029445486143231392\n",
      "Epoch 39, Batch 140, train loss 0.031235739588737488\n",
      "Epoch 39, Batch 150, train loss 0.029145579785108566\n",
      "Epoch 39, Batch 160, train loss 0.03593917563557625\n",
      "Epoch 39, Batch 170, train loss 0.026051264256238937\n",
      "Epoch 39, Batch 180, train loss 0.05270674079656601\n",
      "Epoch 39, Batch 190, train loss 0.023434774950146675\n",
      "Epoch 39, Batch 200, train loss 0.036086827516555786\n",
      "Epoch 39, Batch 210, train loss 0.03285188600420952\n",
      "Epoch 39, Batch 220, train loss 0.02952747419476509\n",
      "Epoch 39, Batch 230, train loss 0.039822056889534\n",
      "Epoch 39, Batch 240, train loss 0.04276800900697708\n",
      "Epoch 39, Batch 250, train loss 0.025348467752337456\n",
      "Epoch 39, Batch 260, train loss 0.031137721613049507\n",
      "Epoch 39, Batch 270, train loss 0.04571419209241867\n",
      "Epoch 39, Batch 280, train loss 0.048315297812223434\n",
      "Epoch 39, Batch 290, train loss 0.029540034011006355\n",
      "Epoch 39, Batch 300, train loss 0.03880995139479637\n",
      "Epoch 39, Batch 310, train loss 0.032205745577812195\n",
      "Epoch 39, Batch 320, train loss 0.03195541352033615\n",
      "Epoch 39, Batch 330, train loss 0.0446506142616272\n",
      "Epoch 39, Batch 340, train loss 0.03291608393192291\n",
      "Epoch 39, Batch 350, train loss 0.018562160432338715\n",
      "Epoch 39, Batch 360, train loss 0.03173345327377319\n",
      "Epoch 39, Batch 370, train loss 0.031198162585496902\n",
      "Epoch 39, Batch 380, train loss 0.031862061470746994\n",
      "Epoch 39, Batch 390, train loss 0.04033491760492325\n",
      "> Epoch 39, train loss 0.03323601312898939\n",
      "> Epoch 39, val loss 0.03864085334678029, accuracy 0.6767441860465117, f1_score 0.6702524735815181\n",
      "No improvement.\n",
      "Epoch 40, Batch 0, train loss 0.03600339964032173\n",
      "Epoch 40, Batch 10, train loss 0.023723773658275604\n",
      "Epoch 40, Batch 20, train loss 0.02860899828374386\n",
      "Epoch 40, Batch 30, train loss 0.01984790340065956\n",
      "Epoch 40, Batch 40, train loss 0.03742409870028496\n",
      "Epoch 40, Batch 50, train loss 0.03752142935991287\n",
      "Epoch 40, Batch 60, train loss 0.03541829437017441\n",
      "Epoch 40, Batch 70, train loss 0.031080838292837143\n",
      "Epoch 40, Batch 80, train loss 0.03193970397114754\n",
      "Epoch 40, Batch 90, train loss 0.03270970284938812\n",
      "Epoch 40, Batch 100, train loss 0.03214242681860924\n",
      "Epoch 40, Batch 110, train loss 0.037630192935466766\n",
      "Epoch 40, Batch 120, train loss 0.029012590646743774\n",
      "Epoch 40, Batch 130, train loss 0.031114578247070312\n",
      "Epoch 40, Batch 140, train loss 0.02899344079196453\n",
      "Epoch 40, Batch 150, train loss 0.04176173731684685\n",
      "Epoch 40, Batch 160, train loss 0.032420720905065536\n",
      "Epoch 40, Batch 170, train loss 0.0536930188536644\n",
      "Epoch 40, Batch 180, train loss 0.031676121056079865\n",
      "Epoch 40, Batch 190, train loss 0.02545217052102089\n",
      "Epoch 40, Batch 200, train loss 0.0342378243803978\n",
      "Epoch 40, Batch 210, train loss 0.02520441636443138\n",
      "Epoch 40, Batch 220, train loss 0.01948746293783188\n",
      "Epoch 40, Batch 230, train loss 0.029424089938402176\n",
      "Epoch 40, Batch 240, train loss 0.03214823454618454\n",
      "Epoch 40, Batch 250, train loss 0.033734992146492004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40, Batch 260, train loss 0.023086046800017357\n",
      "Epoch 40, Batch 270, train loss 0.022053591907024384\n",
      "Epoch 40, Batch 280, train loss 0.01770361140370369\n",
      "Epoch 40, Batch 290, train loss 0.03303427994251251\n",
      "Epoch 40, Batch 300, train loss 0.030379768460989\n",
      "Epoch 40, Batch 310, train loss 0.031134292483329773\n",
      "Epoch 40, Batch 320, train loss 0.028637312352657318\n",
      "Epoch 40, Batch 330, train loss 0.046447426080703735\n",
      "Epoch 40, Batch 340, train loss 0.027898458763957024\n",
      "Epoch 40, Batch 350, train loss 0.012829294428229332\n",
      "Epoch 40, Batch 360, train loss 0.02809789963066578\n",
      "Epoch 40, Batch 370, train loss 0.03349566459655762\n",
      "Epoch 40, Batch 380, train loss 0.02170201949775219\n",
      "Epoch 40, Batch 390, train loss 0.039883650839328766\n",
      "> Epoch 40, train loss 0.032971125114289214\n",
      "> Epoch 40, val loss 0.037667813758517424, accuracy 0.6906976744186046, f1_score 0.6884735013941857\n",
      "No improvement.\n",
      "Early Stopping\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "inv_normalize = transforms.Normalize(\n",
    "   mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "   std=[1/0.229, 1/0.224, 1/0.225]\n",
    ")\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "\n",
    "image_dimension = 224\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(image_dimension),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])        \n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((image_dimension, image_dimension)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize    \n",
    "])\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "def train_model(model, epochs, lr, train_df, val_df, checkpoint_file, early_stopping=10):        \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.8)\n",
    "    best_loss = np.inf\n",
    "    no_improvement = 0\n",
    "\n",
    "    if train_on_gpu:\n",
    "        model = model.cuda()\n",
    "        \n",
    "    for epoch in range(epochs):        \n",
    "        total_train_loss = 0\n",
    "        total_val_loss = 0\n",
    "        total_train = 0\n",
    "        total_val = 0\n",
    "        train_loss = 0\n",
    "        val_loss = 0\n",
    "        \n",
    "        # Train\n",
    "        model.train()        \n",
    "        for i, (sentences, images, labels) in enumerate(get_batches(train_df, train_transform, True)):         \n",
    "            if train_on_gpu:\n",
    "                labels = labels.cuda()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            out = model(sentences, images)\n",
    "            loss = criterion(out.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            #nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            total_train += labels.size(0)\n",
    "            \n",
    "            store_embeddings(sentences, 'cpu')\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Batch {i}, train loss {loss.item()/labels.size(0)}\")\n",
    "            \n",
    "            \n",
    "        train_loss = total_train_loss/total_train\n",
    "        print(f\"> Epoch {epoch}, train loss {train_loss}\")\n",
    "        \n",
    "        \n",
    "        # Eval\n",
    "        model.eval()\n",
    "        all_labels = np.array([])\n",
    "        all_pred = np.array([])        \n",
    "        for sentences, images, labels in get_batches(val_df, test_transform):\n",
    "            if train_on_gpu:\n",
    "                labels = labels.cuda()\n",
    "            \n",
    "            out = model(sentences, images)\n",
    "            loss = criterion(out.squeeze(), labels)\n",
    "            total_val_loss += loss.item()\n",
    "            total_val += labels.size(0)\n",
    "                                \n",
    "            pred = torch.round(torch.sigmoid(out.squeeze()))\n",
    "            \n",
    "            # for matrix\n",
    "            pred_np = pred.data.cpu().numpy() if train_on_gpu else pred.data.numpy()\n",
    "            labels_np = labels.data.cpu().numpy() if train_on_gpu else labels.data.numpy()                    \n",
    "            all_pred = np.concatenate([all_pred, pred_np])\n",
    "            all_labels = np.concatenate([all_labels, labels_np])\n",
    "                        \n",
    "            # clear memory\n",
    "            store_embeddings(sentences, 'cpu')\n",
    "            \n",
    "            \n",
    "        val_loss = total_val_loss / total_val\n",
    "        f1 = f1_score(all_labels, all_pred, average='weighted')\n",
    "        acc = accuracy_score(all_labels, all_pred)\n",
    "        \n",
    "        print(f\"> Epoch {epoch}, val loss {val_loss}, accuracy {acc}, f1_score {f1}\")\n",
    "        \n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            no_improvement = 0\n",
    "            torch.save(model.state_dict(), checkpoint_file)\n",
    "            print(\"Saved model.\")\n",
    "        else:\n",
    "            no_improvement += 1\n",
    "            print(\"No improvement.\")\n",
    "            if no_improvement >= early_stopping:\n",
    "                print(f\"Early Stopping\")\n",
    "                break\n",
    "                \n",
    "                \n",
    "        # reduce learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "            \n",
    "                                              \n",
    "checkpoint_file = 'custom_cnn_model.pt'      \n",
    "lr = 0.001\n",
    "epochs = 50         \n",
    "\n",
    "train_model(model, epochs, lr, train_df, val_df, checkpoint_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/efficientnet/lib/python3.6/site-packages/ipykernel_launcher.py:54: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "/home/ec2-user/anaconda3/envs/efficientnet/lib/python3.6/site-packages/ipykernel_launcher.py:55: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "/home/ec2-user/anaconda3/envs/efficientnet/lib/python3.6/site-packages/ipykernel_launcher.py:56: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "/home/ec2-user/anaconda3/envs/efficientnet/lib/python3.6/site-packages/ipykernel_launcher.py:57: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "/home/ec2-user/anaconda3/envs/efficientnet/lib/python3.6/site-packages/ipykernel_launcher.py:58: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "/home/ec2-user/anaconda3/envs/efficientnet/lib/python3.6/site-packages/ipykernel_launcher.py:59: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "/home/ec2-user/anaconda3/envs/efficientnet/lib/python3.6/site-packages/ipykernel_launcher.py:60: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "578/850 correct. Accuracy: 68.0 %, acc 0.68, f1 0.6797003147581681\n"
     ]
    }
   ],
   "source": [
    "def eval_model(model, test_df):\n",
    "    if train_on_gpu:\n",
    "        model = model.cuda()\n",
    "        \n",
    "    model.eval()\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "    all_pred = np.array([])\n",
    "    all_labels = np.array([])\n",
    "    for i, (sentences, images, labels) in enumerate(get_batches(test_df, test_transform)):\n",
    "        if train_on_gpu:\n",
    "            labels = labels.cuda()\n",
    "            \n",
    "        out = model(sentences, images)\n",
    "        pred = torch.round(torch.sigmoid(out.squeeze()))\n",
    "\n",
    "        correct = (pred == labels)\n",
    "        correct = correct.cpu().numpy() if train_on_gpu else correct.numpy()        \n",
    "        num_correct += np.sum(correct)\n",
    "        num_total += labels.size(0)\n",
    "\n",
    "\n",
    "        \n",
    "        pred_np = pred.data.cpu().numpy() if train_on_gpu else pred.data.numpy()\n",
    "        labels_np = labels.data.cpu().numpy() if train_on_gpu else labels.data.numpy()                    \n",
    "        all_pred = np.concatenate([all_pred, pred_np])\n",
    "        all_labels = np.concatenate([all_labels, labels_np])\n",
    "        \n",
    "        \n",
    "        store_embeddings(sentences, 'cpu')\n",
    "\n",
    "    \n",
    "    f1 = f1_score(all_labels, all_pred, average='weighted')\n",
    "    acc = accuracy_score(all_labels, all_pred)\n",
    "    \n",
    "    print(f\"{num_correct}/{num_total} correct. Accuracy: {num_correct*100/num_total} %, acc {acc}, f1 {f1}\")\n",
    "    \n",
    "    \n",
    "best_model = MyModel()\n",
    "best_model.load_state_dict(torch.load(checkpoint_file))\n",
    "eval_model(best_model, test_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:efficientnet]",
   "language": "python",
   "name": "conda-env-efficientnet-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
